<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Trabajo Final de Máster - 1&nbsp; Capítulo I: Consideraciones teóricas</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chapter1a.html" rel="next">
<link href="./intro.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "Sin resultados",
    "search-matching-documents-text": "documentos encontrados",
    "search-copy-link-title": "Copiar el enlace en la búsqueda",
    "search-hide-matches-text": "Ocultar resultados adicionales",
    "search-more-match-text": "resultado adicional en este documento",
    "search-more-matches-text": "resultados adicionales en este documento",
    "search-clear-button-title": "Borrar",
    "search-detached-cancel-button-title": "Cancelar",
    "search-submit-button-title": "Enviar"
  }
}</script>
<style>
    .quarto-title-block .quarto-title-banner {
      background: #D60D8C;
    }
    </style>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="my_style.css">
</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Capítulo I: Consideraciones teóricas</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title d-none d-lg-block"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Capítulo I: Consideraciones teóricas</span></h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Trabajo Final de Máster</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/davidrsch/TFM" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
    <a href="" title="Download" id="sidebar-tool-dropdown-0" class="sidebar-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi bi-download"></i></a>
    <ul class="dropdown-menu" aria-labelledby="sidebar-tool-dropdown-0">
        <li>
          <a class="dropdown-item sidebar-tools-main-item" href="./Trabajo-Final-de-Máster.pdf">
            <i class="bi bi-bi-file-pdf pe-1"></i>
          Download PDF
          </a>
        </li>
        <li>
          <a class="dropdown-item sidebar-tools-main-item" href="./Trabajo-Final-de-Máster.epub">
            <i class="bi bi-bi-journal pe-1"></i>
          Download ePub
          </a>
        </li>
    </ul>
    <a href="https://twitter.com/intent/tweet?url=|url|" title="Twitter" class="sidebar-tool px-1"><i class="bi bi-twitter"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Agradecimientos</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">Resumen</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">Introducción</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter1.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Capítulo I: Consideraciones teóricas</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter1a.html" class="sidebar-item-text sidebar-link">Apéndices Capítulo 1</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter2.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Capítulo 2: Procedimiento</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chapter2a.html" class="sidebar-item-text sidebar-link">Apéndices Capítulo 2</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">Appendices</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Annex1.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Anexo 1</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Tabla de contenidos</h2>
   
  <ul>
  <li><a href="#caracterización-de-las-series-temporales-financieras" id="toc-caracterización-de-las-series-temporales-financieras" class="nav-link active" data-scroll-target="#caracterización-de-las-series-temporales-financieras"><span class="toc-section-number">1.1</span>  Caracterización de las series temporales financieras</a>
  <ul class="collapse">
  <li><a href="#series-de-tiempo-y-sus-características" id="toc-series-de-tiempo-y-sus-características" class="nav-link" data-scroll-target="#series-de-tiempo-y-sus-características"><span class="toc-section-number">1.1.1</span>  Series de tiempo y sus características</a></li>
  <li><a href="#características-de-los-precios" id="toc-características-de-los-precios" class="nav-link" data-scroll-target="#características-de-los-precios"><span class="toc-section-number">1.1.2</span>  Características de los precios</a></li>
  </ul></li>
  <li><a href="#redes-neuronales-artificiales-en-la-previsión-de-las-series-de-tiempo" id="toc-redes-neuronales-artificiales-en-la-previsión-de-las-series-de-tiempo" class="nav-link" data-scroll-target="#redes-neuronales-artificiales-en-la-previsión-de-las-series-de-tiempo"><span class="toc-section-number">1.2</span>  Redes neuronales artificiales en la previsión de las series de tiempo</a>
  <ul class="collapse">
  <li><a href="#antecedentes-del-uso-de-redes-neuronales-artificiales-en-la-previsión-de-series-de-tiempo" id="toc-antecedentes-del-uso-de-redes-neuronales-artificiales-en-la-previsión-de-series-de-tiempo" class="nav-link" data-scroll-target="#antecedentes-del-uso-de-redes-neuronales-artificiales-en-la-previsión-de-series-de-tiempo"><span class="toc-section-number">1.2.1</span>  Antecedentes del uso de redes neuronales artificiales en la previsión de series de tiempo</a></li>
  <li><a href="#redes-neuronales-convolucionales" id="toc-redes-neuronales-convolucionales" class="nav-link" data-scroll-target="#redes-neuronales-convolucionales"><span class="toc-section-number">1.2.2</span>  Redes neuronales convolucionales</a></li>
  <li><a href="#long-short-term-memory" id="toc-long-short-term-memory" class="nav-link" data-scroll-target="#long-short-term-memory"><span class="toc-section-number">1.2.3</span>  Long short-term memory</a></li>
  </ul></li>
  <li><a href="#composición-de-carteras" id="toc-composición-de-carteras" class="nav-link" data-scroll-target="#composición-de-carteras"><span class="toc-section-number">1.3</span>  Composición de carteras</a>
  <ul class="collapse">
  <li><a href="#problema-y-técnicas" id="toc-problema-y-técnicas" class="nav-link" data-scroll-target="#problema-y-técnicas"><span class="toc-section-number">1.3.1</span>  Problema y técnicas</a></li>
  <li><a href="#programación-cuadrática" id="toc-programación-cuadrática" class="nav-link" data-scroll-target="#programación-cuadrática"><span class="toc-section-number">1.3.2</span>  Programación cuadrática</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/davidrsch/TFM/edit/main/chapter1.qmd" class="toc-action">Editar esta página</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>El objetivo de este capítulo es brindar una comprensión teórica exhaustiva sobre aspectos fundamentales relacionados con las series temporales financieras y su caracterización, así como el uso de redes neuronales artificiales y la programación cuadrática en la previsión y composición de carteras. En primer lugar, se analizará en detalle la caracterización de las series temporales financieras, explorando su naturaleza y características clave para comprender su comportamiento y aplicaciones en el ámbito financiero. Además, se examinará el uso de redes neuronales artificiales en la previsión de series de tiempo, incluyendo los antecedentes de su aplicación y centrándose específicamente en las Redes Neuronales Convolucionales y las denominadas Long Short-Term Memory. A continuación, se abordará la composición de carteras mediante programación cuadrática, presentando el problema y las técnicas asociadas, y se explorará en detalle el método dual en la optimización de carteras. A través de este capítulo, se sentarán las bases teóricas necesarias para comprender y aplicar de manera efectiva estas técnicas en capítulos posteriores, permitiendo una comprensión profunda y la capacidad de utilizar estos conceptos de manera práctica en el contexto financiero.</p>
<section id="caracterización-de-las-series-temporales-financieras" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="caracterización-de-las-series-temporales-financieras"><span class="header-section-number">1.1</span> Caracterización de las series temporales financieras</h2>
<p>Este epígrafe se divide en dos sub-epígrafes. En el primero se presentan los conceptos fundamentales relacionados con las series de tiempo, incluyendo sus características, componentes y clasificaciones, y se establece su relación con el análisis de las series cronológicas y la previsión, que es el objetivo de este informe. En el segundo sub-epígrafe se profundiza en las características de los precios de las acciones, comenzando por una descripción general y avanzando hacia aspectos más específicos, así como en la importancia de la estructura en la que se encuentran los datos relacionados con estos.</p>
<section id="series-de-tiempo-y-sus-características" class="level3" data-number="1.1.1">
<h3 data-number="1.1.1" class="anchored" data-anchor-id="series-de-tiempo-y-sus-características"><span class="header-section-number">1.1.1</span> Series de tiempo y sus características</h3>
<p>En este sub-epígrafe se analizan varios textos, a partir de los cuales se puede concluir que, una serie de tiempo o serie cronológica es como el registro de los valores de una o más variables en intervalos regulares de tiempo (por ejemplo, diario, semanal, semestral, anual, etc.). El análisis de las series de tiempo puede tener distintos fines, como describir el comportamiento de las variables o predecir o pronosticar sus valores futuros, lo que es especialmente relevante para las series financieras.</p>
<p>El análisis de las series de tiempo es una herramienta estadística que permite estudiar el comportamiento de una variable a lo largo del tiempo. Sin embargo, no existe un consenso único sobre los componentes que se deben considerar en este tipo de análisis. Algunos autores, como <span class="citation" data-cites="kocenda2017">Kocenda y Cerný (<a href="#ref-kocenda2017" role="doc-biblioref">2017</a>)</span> y <span class="citation" data-cites="anderson2017">Anderson et&nbsp;al. (<a href="#ref-anderson2017" role="doc-biblioref">2017</a>)</span>, proponen que las series de tiempo se pueden descomponer en tres componentes: tendencia, estacionalidad y ruido. Otros autores, como <span class="citation" data-cites="dodge2008">Dodge (<a href="#ref-dodge2008" role="doc-biblioref">2008</a>)</span> y <span class="citation" data-cites="espallargas2012">Espallargas y Solís (<a href="#ref-espallargas2012" role="doc-biblioref">2012</a>)</span>, sugieren que se debe añadir un cuarto componente: el ciclo. Finalmente, hay autores que plantean que las series de tiempo pueden tener hasta cinco componentes estos son los casos de <span class="citation" data-cites="IBM2021">IBM (<a href="#ref-IBM2021" role="doc-biblioref">2021</a>)</span> y <span class="citation" data-cites="chirinos2018">Chirinos (<a href="#ref-chirinos2018" role="doc-biblioref">2018</a>)</span>.</p>
<p>Tendencia: La tendencia es el patrón de cambio a largo plazo que se observa en una serie de datos. Se puede definir como la dirección general y persistente de las variaciones de la serie a lo largo del tiempo. La tendencia puede ser positiva (<a href="#fig-tend_pos">Figura&nbsp;<span class="quarto-unresolved-ref">fig-tend_pos</span></a>), negativa (<a href="#fig-tend_neg">Figura&nbsp;<span class="quarto-unresolved-ref">fig-tend_neg</span></a>) o nula (<a href="#fig-tend_nul">Figura&nbsp;<span class="quarto-unresolved-ref">fig-tend_nul</span></a>), dependiendo de si la serie aumenta, disminuye o se mantiene constante en el largo plazo. La tendencia se puede identificar mediante el análisis gráfico o mediante métodos estadísticos. La tendencia es importante para entender el comportamiento histórico y proyectar el futuro de una serie de datos. Este componente es común en los distintos criterios mencionados.</p>
<p>Estacionalidad: Llamada también variación cíclica regular: Se refiere a la variación correspondiente a los movimientos de la serie que ocurren cada cierto periodo de tiempo, <a href="#fig-est_err">Figura&nbsp;<span class="quarto-unresolved-ref">fig-est_err</span></a>. Este componente es, al igual que la tendencia, común en los criterios mencionados. Diferenciándose en que aquellos autores que exponen cuatro y cinco componentes llaman estacionalidad a las variaciones periódicas correspondientes a periodos menores o iguales a un año (como son periodicidad diaria, semanal, mensual, o anual), mientras que las variaciones periódicas correspondientes a periodos mayores las contemplan en un componente llamado, variaciones cíclicas. Por lo que para determinar la estacionalidad de una serie de tiempo es necesario analizarlas en un periodo no menor de dos años.</p>
<p>Un componente que no se puede explicar por los otros elementos de la serie de tiempo es la variación irregular o error. Este componente también se conoce como variación aleatoria, ruido o residuo, y se muestra en la <a href="#fig-est_err">Figura&nbsp;<span class="quarto-unresolved-ref">fig-est_err</span></a>. La variación irregular es común en los tres criterios mencionados anteriormente. Algunos autores distinguen entre la variación irregular, que es ocasional y aleatoria, y la variación atípica, que es causada por eventos aislados que alteran el comportamiento de la serie. La variación atípica se puede clasificar en varios tipos: aditiva, de innovación, de cambio de nivel, transiente, de estacionalidad aditiva y de tendencia local.</p>
<p>Una forma de categorizar las series de tiempo es según el grado de variabilidad que presentan a lo largo del tiempo, según lo expuesto en <span class="citation" data-cites="villagarcia2006">Villagarcía (<a href="#ref-villagarcia2006" role="doc-biblioref">2006</a>)</span> se puede distinguir entre series homocedásticas y heterocedásticas. Las series homocedásticas son aquellas que mantienen un rango constante de variación, como se muestra en la <a href="#fig-tend_nul">Figura&nbsp;<span class="quarto-unresolved-ref">fig-tend_nul</span></a>. Por el contrario, las series heterocedásticas son aquellas que cambian el rango de variación, aumentando o disminuyendo su amplitud, como se ilustra en las <a href="#fig-tend_pos">Figura&nbsp;<span class="quarto-unresolved-ref">fig-tend_pos</span></a> y <a href="#fig-tend_neg">Figura&nbsp;<span class="quarto-unresolved-ref">fig-tend_neg</span></a>.</p>
<p>Las series cronológicas son un tipo de proceso estocástico que se caracteriza por ordenar las variables aleatorias según el tiempo. Esto significa que cada momento tiene asociado un valor de la variable que depende del azar y que puede cambiar a lo largo del tiempo. Según <span class="citation" data-cites="ruiz2011">Ruiz (<a href="#ref-ruiz2011" role="doc-biblioref">2011</a>)</span>, un proceso estocástico es “una colección o familia de variables aleatorias, ordenadas según un subíndice que en general suele ser el tiempo” (p.01).</p>
<p>Un proceso estocástico es una colección de variables aleatorias que representan la evolución de un fenómeno aleatorio en el tiempo. Según <span class="citation" data-cites="ruiz2011">Ruiz (<a href="#ref-ruiz2011" role="doc-biblioref">2011</a>)</span>, los estados son los posibles valores que puede tomar la variable aleatoria, y pueden ser discretos o continuos, dependiendo de la naturaleza de la variable. Asimismo, el tiempo puede ser discreto o continuo, según los cambios de estado se produzcan en intervalos fijos o variables. <span class="citation" data-cites="ruiz2011">Ruiz (<a href="#ref-ruiz2011" role="doc-biblioref">2011</a>)</span> clasifica los procesos estocásticos en cuatro tipos, según el espacio de estados y el tiempo, como se muestra en la <a href="#tbl-pro_esto">Tabla&nbsp;<span class="quarto-unresolved-ref">tbl-pro_esto</span></a>.</p>
<p>Según la definición de serie temporal ofrecida anteriormente, “una serie de tiempo o serie cronológica es como el registro de los valores de una o más variables en intervalos regulares de tiempo”, y a lo expuesto con respecto a los procesos estocásticos en <span class="citation" data-cites="castillo_varela2010">Castillo y Varela (<a href="#ref-castillo_varela2010" role="doc-biblioref">2010</a>)</span>, <span class="citation" data-cites="villavicencio2010">Villavicencio (<a href="#ref-villavicencio2010" role="doc-biblioref">2010</a>)</span> y el propio <span class="citation" data-cites="ruiz2011">Ruiz (<a href="#ref-ruiz2011" role="doc-biblioref">2011</a>)</span> se coincide con lo afirmado por este último: “una serie temporal es una realización parcial de un proceso estocástico de parámetro tiempo discreto” (p.09). Debido a que, en el análisis de series temporales, aunque el tiempo es continuo se adoptan intervalos discretos (días, semanas, meses, años), los cuales se establecen por convenio por los analistas o por las fuentes que suministran los datos.</p>
<p>Un concepto clave en el análisis de series de tiempo es el de estacionariedad. Una serie de tiempo es estacionaria cuando sus propiedades estadísticas, como la media, la varianza y la covarianza, no cambian con el tiempo. Esto implica que la serie no presenta tendencia, ciclos ni estacionalidad. Como señalan <span class="citation" data-cites="castillo_varela2010">Castillo y Varela (<a href="#ref-castillo_varela2010" role="doc-biblioref">2010</a>)</span>, <span class="citation" data-cites="villavicencio2010">Villavicencio (<a href="#ref-villavicencio2010" role="doc-biblioref">2010</a>)</span> y <span class="citation" data-cites="ruiz2011">Ruiz (<a href="#ref-ruiz2011" role="doc-biblioref">2011</a>)</span>, la estacionariedad es una condición necesaria para poder predecir el comportamiento futuro de una serie de tiempo usando técnicas estadísticas. En la <a href="#fig-tend_nul">Figura&nbsp;<span class="quarto-unresolved-ref">fig-tend_nul</span></a> se muestra un ejemplo de una serie de tiempo estacionaria.</p>
<p>Las series de tiempo financieras presentan heterocedasticidad, es decir, varianzas que cambian en el tiempo. Esto implica que no son estacionarias y que su comportamiento depende de factores externos. Para verificar la estacionariedad de una serie de tiempo, se pueden utilizar diferentes métodos, como el correlograma, que muestra las funciones de autocorrelación y autocorrelación parcial de la serie, o las pruebas de raíz unitaria, como la de Dickey Fuller o la de Phillips Perron, que contrastan la hipótesis nula de que la serie tiene una raíz unitaria. Estos métodos se explican con más detalle en <span class="citation" data-cites="castillo_varela2010">Castillo y Varela (<a href="#ref-castillo_varela2010" role="doc-biblioref">2010</a>)</span>, <span class="citation" data-cites="villavicencio2010">Villavicencio (<a href="#ref-villavicencio2010" role="doc-biblioref">2010</a>)</span> y <span class="citation" data-cites="ruiz2011">Ruiz (<a href="#ref-ruiz2011" role="doc-biblioref">2011</a>)</span>. La <a href="#fig-acf_pacf">Figura&nbsp;<span class="quarto-unresolved-ref">fig-acf_pacf</span></a> ilustra un ejemplo de correlograma para una serie de tiempo financiera.</p>
</section>
<section id="características-de-los-precios" class="level3" data-number="1.1.2">
<h3 data-number="1.1.2" class="anchored" data-anchor-id="características-de-los-precios"><span class="header-section-number">1.1.2</span> Características de los precios</h3>
<p>Invertir en acciones o cualquier otro bien que cotice en el mercado de valores es una tarea compleja y desafiante, que requiere una comprensión profunda de las tendencias y fluctuaciones del mercado. En el centro de esta comprensión se encuentra la capacidad de analizar e interpretar los datos de precios del mercado de valores, lo que brinda información clave sobre el comportamiento de los participantes del mercado y los factores que impulsan los movimientos del mercado. El propósito de este sub-epígrafe es proporcionar una descripción general completa del entorno de los precios de las acciones y como son representados comunmente los mismos, señalando los aspectos más importantes para la aplicación de las técnicas que se exploraran en los siguientes epígrafes.</p>
<p>Como se explica en la <span class="citation" data-cites="cnmvbv">CNMV (<a href="#ref-cnmvbv" role="doc-biblioref">s.&nbsp;f.b</a>)</span> las bolsas de valores son mercados organizados donde se negocian acciones y otros valores, como renta fija, warrants, certificados y fondos cotizados. En <span class="citation" data-cites="bmeqe">BME (<a href="#ref-bmeqe" role="doc-biblioref">s.&nbsp;f.</a>)</span> se expone que, en España, existen cuatro bolsas tradicionales (Madrid, Barcelona, Bilbao y Valencia) que forman parte del holding BME (Bolsas y Mercados Españoles), que también integra otros segmentos y sistemas de negociación, compensación y liquidación de valores. Siendo, como se explica en <span class="citation" data-cites="cnmvsibe">CNMV (<a href="#ref-cnmvsibe" role="doc-biblioref">s.&nbsp;f.c</a>)</span>, el Sistema de Interconexión Bursátil Español (SIBE) la plataforma que permite la contratación continua y electrónica de todos los valores admitidos a cotización en las cuatro bolsas españolas.</p>
<p>Como expone la <span class="citation" data-cites="cnmvacci">CNMV (<a href="#ref-cnmvacci" role="doc-biblioref">s.&nbsp;f.a</a>)</span> las acciones son valores mobiliarios que representan una parte proporcional del capital social de una sociedad anónima, y sus tenedores son socios propietarios de la misma. Las acciones pueden negociarse en bolsas de valores o en otros mercados secundarios autorizados.</p>
<p>A partir de lo expuesto en <span class="citation" data-cites="mitchell20">Mitchell (<a href="#ref-mitchell20" role="doc-biblioref">2020</a>)</span>, <span class="citation" data-cites="pinset21">Pinset (<a href="#ref-pinset21" role="doc-biblioref">2021</a>)</span> y <span class="citation" data-cites="cfi23">C. Team (<a href="#ref-cfi23" role="doc-biblioref">2023</a>)</span> se puede concluir que, para explicar el precio de las acciones de una empresa, se pueden considerar los siguientes factores:</p>
<ul>
<li><p>La oferta y la demanda de las acciones en el mercado: si hay más compradores que vendedores, el precio subirá y viceversa. Esto depende de las expectativas y la confianza de los inversores en el futuro de la empresa.</p></li>
<li><p>Los cambios en la gestión o la producción de la empresa: si la empresa mejora su eficiencia, su rentabilidad o su innovación, el precio de sus acciones puede aumentar. Por el contrario, si la empresa tiene problemas internos, pierde competitividad o se ve afectada por crisis externas, el precio puede bajar.</p></li>
<li><p>La reputación de la empresa: si la empresa tiene una buena imagen pública, se asocia con éxitos o logros, o recibe buenas valoraciones de los analistas, el precio de sus acciones puede subir. Por el contrario, si la empresa se ve involucrada en escándalos, demandas o controversias, o recibe malas valoraciones de los analistas, el precio puede bajar.</p></li>
</ul>
<p>En los textos <span class="citation" data-cites="pinset21">Pinset (<a href="#ref-pinset21" role="doc-biblioref">2021</a>)</span>, <span class="citation" data-cites="tinvt22">T. I. Team (<a href="#ref-tinvt22" role="doc-biblioref">2022</a>)</span> y <span class="citation" data-cites="cfi23">C. Team (<a href="#ref-cfi23" role="doc-biblioref">2023</a>)</span> también señalan la importancia de diferenciar el precio de una empresa o acción de esta del valor intrínseco de esta. Pudiéndose resumir teniendo en cuenta lo señalado en esos textos y lo expuesto con anterioridad que el precio de una empresa o acción es lo que los compradores y vendedores están dispuestos a pagar por ella en un momento determinado, mientras que el valor intrínseco de una empresa o acción depende en gran medida de la metodología utilizada para valorar las empresas y los objetivos del evaluador.</p>
<p>Una vez contextualizado de manera general el entorno en el que se encuentran los precios de las acciones y explicado algunos de los factores que pueden afectar a los mismos se explica a continuación como se encuentran estos generalmente estructurados en las distintas fuentes de las que se pueden obtener. Generalmente los precios de las acciones se encuentran registrados de forma periódica (diariamente, semanalmente, mensualmente, anualmente, etc). registrándose para cada periodo el precio de apertura, el precio más alto, el más bajo, el de cierre, el volumen y el de cierre ajustado, ver <a href="#tbl-pre_estruc">Tabla&nbsp;<span class="quarto-unresolved-ref">tbl-pre_estruc</span></a>.</p>
<p>A partir de lo expuesto en <span class="citation" data-cites="barone22">Barone (<a href="#ref-barone22" role="doc-biblioref">2022</a>)</span>, <span class="citation" data-cites="chen22">Chen (<a href="#ref-chen22" role="doc-biblioref">2022</a>)</span>, <span class="citation" data-cites="downey22">Downey (<a href="#ref-downey22" role="doc-biblioref">2022</a>)</span>, <span class="citation" data-cites="hayes21">Hayes (<a href="#ref-hayes21" role="doc-biblioref">2021</a>)</span> y <span class="citation" data-cites="ganti20">Ganti (<a href="#ref-ganti20" role="doc-biblioref">2020</a>)</span> se puede entender que:</p>
<ul>
<li><p>El precio de apertura es el primer precio al que se negocia un activo financiero en una sesión bursátil. Este precio puede ser diferente al precio de cierre de la sesión anterior, ya que puede haber cambios en la oferta y la demanda durante el periodo en que el mercado está cerrado. El precio de apertura suele indicar el tono o la tendencia del mercado para ese día.</p></li>
<li><p>El precio más alto es el mayor precio al que se negocia un activo financiero en una sesión bursátil. Este precio refleja el máximo nivel de interés de los compradores por ese activo en ese día. El precio más alto puede ser un indicador de la fortaleza o la debilidad de un activo, así como de su volatilidad.</p></li>
<li><p>El precio más bajo es el menor precio al que se negocia un activo financiero en una sesión bursátil. Este precio refleja el mínimo nivel de interés de los vendedores por ese activo en ese día. El precio más bajo puede ser un indicador de la presión o la resistencia de un activo, así como de su volatilidad.</p></li>
<li><p>El precio de cierre es el último precio al que se negocia un activo financiero en una sesión bursátil. Este precio es el que se utiliza para calcular el valor de mercado de ese activo al final del día. El precio de cierre suele ser el más importante para los inversores, ya que resume el resultado de las operaciones del día y muestra la dirección del mercado.</p></li>
<li><p>El volumen es la cantidad de unidades de un activo financiero que se negocian en una sesión bursátil. El volumen muestra el nivel de actividad o de liquidez de un mercado o de un activo. El volumen suele acompañar a los movimientos de los precios, ya que indica el grado de consenso o de divergencia entre los participantes del mercado.</p></li>
<li><p>El precio de cierre ajustado es el precio de cierre de un activo financiero que se modifica para tener en cuenta eventos como dividendos, splits, fusiones o adquisiciones que afectan al valor del activo. El precio de cierre ajustado permite comparar el rendimiento histórico de un activo con mayor precisión y consistencia.</p></li>
</ul>
<p>A partir de lo expuesto en <span class="citation" data-cites="hayes21">Hayes (<a href="#ref-hayes21" role="doc-biblioref">2021</a>)</span> y <span class="citation" data-cites="ganti20">Ganti (<a href="#ref-ganti20" role="doc-biblioref">2020</a>)</span> se entiende que la diferencia entre el precio de cierre y el precio de cierre ajustado es de gran importancia, ya que el primero puede dar una imagen distorsionada del rendimiento de una acción a lo largo del tiempo, mientras que el segundo refleja el valor real de la acción después de ajustar los factores que lo alteran.</p>
<blockquote class="blockquote">
<p>Por ejemplo, la junta directiva de una empresa puede decidir dividir las acciones de la empresa 3 por 1. Por lo tanto, las acciones en circulación de la empresa aumentan en un múltiplo de tres, mientras que el precio de sus acciones se divide por tres. Supongamos que una acción cerró a $300 el día anterior a su división de acciones. En este caso, el precio de cierre se ajusta a $100 ($300 divididos por 3) por acción para mantener un estándar de comparación consistente. De manera similar, todos los demás precios de cierre anteriores para esa empresa se dividirían por tres para obtener los precios de cierre ajustados. <span class="citation" data-cites="ganti20">Ganti (<a href="#ref-ganti20" role="doc-biblioref">2020</a>)</span></p>
</blockquote>
<p>Debido a ello el precio de cierre ajustado es mejor para la aplicación de técnicas de análisis de series de tiempo, ya que permite comparar el comportamiento de una acción a lo largo del tiempo sin las distorsiones causadas por los eventos corporativos. Siendo la serie de tiempo más comunmente utilizada en los estudios de los análisis de los precios de mercado la conformado por las rentabilidades calculadas a partir del precio de cierre ajustado.</p>
</section>
</section>
<section id="redes-neuronales-artificiales-en-la-previsión-de-las-series-de-tiempo" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="redes-neuronales-artificiales-en-la-previsión-de-las-series-de-tiempo"><span class="header-section-number">1.2</span> Redes neuronales artificiales en la previsión de las series de tiempo</h2>
<p>Este epígrafe esta dividido en tres sub-epígrafes. En el primero se abordan los antecedentes del uso de redes neuronales artificiales para el trabajo con series de tiempo, más concretamente en la previsión. En el segundo y tercer sub-epígrafes se exponen el funcionamiento de dos de las estructuras de capas de RNA usadas en el presente trabajo, siendo estas las CNN y las LSTM.</p>
<section id="antecedentes-del-uso-de-redes-neuronales-artificiales-en-la-previsión-de-series-de-tiempo" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="antecedentes-del-uso-de-redes-neuronales-artificiales-en-la-previsión-de-series-de-tiempo"><span class="header-section-number">1.2.1</span> Antecedentes del uso de redes neuronales artificiales en la previsión de series de tiempo</h3>
<p>En este sub-epígrafe se abordan las redes neuronales artificiales (en lo adelante RNA) desde lo general a lo particular explicándose desde el entorno a las características básicas de las mismas. Además, se exponen los antecedentes del uso de estas en la solución de problemas de previsión de series de tiempo.</p>
<p>En <span class="citation" data-cites="chollet2018deep">Chollet y Allaire (<a href="#ref-chollet2018deep" role="doc-biblioref">2018</a>)</span> se plantea que el entorno de las RNA está conformado por la inteligencia artificial (en lo adelante IA), machine learning o aprendizaje automatizado (en lo adelante ML) y deep learning o aprendizaje profundo (en lo adelante DL), <a href="#fig-DLenv">Figura&nbsp;<span class="quarto-unresolved-ref">fig-DLenv</span></a>. Por lo que es de vital importancia conocer los aspectos de estos campos que se encuentran íntimamente relacionados con las RNA y que se exponen brevemente a continuación.</p>
<p>“Hacer que una máquina se comporte de tal manera que si un humano lo hiciera se le llamaría inteligente” (<span class="citation" data-cites="McCarthy_Minsky_Rochester_Shannon_2006">McCarthy et&nbsp;al. (<a href="#ref-McCarthy_Minsky_Rochester_Shannon_2006" role="doc-biblioref">2006</a>)</span>, p.11) es la primera definición que se le dio al problema de IA. Con el objetivo de dar solución a este problema surgieron las primeras IA, las llamadas IA simbólicas.</p>
<p>Como se explica en <span class="citation" data-cites="haykin1998neural">Haykin (<a href="#ref-haykin1998neural" role="doc-biblioref">1998</a>)</span>, <span class="citation" data-cites="banda2014">Banda (<a href="#ref-banda2014" role="doc-biblioref">2014</a>)</span> y <span class="citation" data-cites="chollet2018deep">Chollet y Allaire (<a href="#ref-chollet2018deep" role="doc-biblioref">2018</a>)</span>, estas primeras IA, involucraban reglas codificadas creadas por los programadores. Con el objetivo de lograr que estas reglas fueran aprendidas automáticamente por las máquinas al observar los datos surgió una nueva etapa dentro del desarrollo de las IA, la denominada ML. En esta nueva etapa se da pie al surgimiento de una nueva forma de programación, diferenciándose de la clásica, en que, en esta, los programadores introducen los datos y las respuestas esperadas a los mismos, y las computadoras son capaces de generar las reglas, <a href="#fig-MLprog">Figura&nbsp;<span class="quarto-unresolved-ref">fig-MLprog</span></a>.</p>
<p>Por lo que se entiende que los modelos de ML tratan de encontrar representaciones apropiadas para sus datos de entrada: transformaciones de los datos que hacen que sea más susceptible a la tarea en cuestión. En DL, que es un sub-campo específico de ML, estas representaciones de datos son modeladas a través de arquitecturas compuestas de capas sucesivas, las que son llamadas RNA <span class="citation" data-cites="chollet2018deep">Chollet y Allaire (<a href="#ref-chollet2018deep" role="doc-biblioref">2018</a>)</span>.</p>
<p>Tras el estudio de lo expuesto en <span class="citation" data-cites="haykin1998neural">Haykin (<a href="#ref-haykin1998neural" role="doc-biblioref">1998</a>)</span>, <span class="citation" data-cites="Larranaga07">Larrañaga (<a href="#ref-Larranaga07" role="doc-biblioref">2007</a>)</span>, <span class="citation" data-cites="banda2014">Banda (<a href="#ref-banda2014" role="doc-biblioref">2014</a>)</span> y <span class="citation" data-cites="chollet2018deep">Chollet y Allaire (<a href="#ref-chollet2018deep" role="doc-biblioref">2018</a>)</span> sobre las RNA se puede afirmar que están inspiradas en el funcionamiento del cerebro humano, dichos textos confirman y concuerdan en que en una RNA se pueden diferenciar tres tipos de capas: de entrada, de salida y ocultas. Una capa de entrada está compuesta por neuronas que reciben los vectores de entradas. Una capa de salida se compone de neuronas que, durante el entrenamiento reciben los vectores de salidas y que luego generan la respuesta. Una capa oculta se encuentra conectada al entorno a través de las capas de entrada y salida, este tipo de capa oculta procesa la entrada recibida para obtener la salida correspondiente, <a href="#fig-RNAstruct">Figura&nbsp;<span class="quarto-unresolved-ref">fig-RNAstruct</span></a>.</p>
<p>Una de las aplicaciones de las RNA es la previsión de series temporales. cuyo objetivo es predecir los valores futuros de las variables en función de sus observaciones pasadas. Como se expuso con anterioridad las series de tiempo financieras a menudo son no lineales, ruidosas, caóticas y no estacionarias, lo que las hace difíciles de modelar y pronosticar. Las RNA tienen la ventaja de poder capturar relaciones no lineales complejas y adaptarse a condiciones cambiantes sin requerir suposiciones previas sobre la distribución o estructura de datos.</p>
<p>La historia de las RNA en la previsión de series temporales financieras se remonta a finales de la década de 1980 y principios de la de 1990, cuando los investigadores comenzaron a explorar el potencial de las RNA como una alternativa a los métodos estadísticos tradicionales, como el modelo autorregresivo integrado de media móviles, más conocido como ARIMA (por sus siglas en inglés Autoregressive Integrated Moving Average) y los modelos autorregresivos generalizados con heterocedasticidad condicional, más conocido como GARCH (por sus siglas en inglés Generalized Autoregressive Conditional Heteroskedasticity). Se demostró que las RNA tienen varias ventajas sobre estos métodos, como la capacidad de capturar relaciones no lineales y dinámicas, manejar datos ruidosos e incompletos y adaptarse a las condiciones cambiantes del mercado (<span class="citation" data-cites="ZHANG199835">B. Eddy Patuwo &amp; Michael Y. Hu (<a href="#ref-ZHANG199835" role="doc-biblioref">1998</a>)</span>).</p>
<p>Sin embargo, las RNA también enfrentan algunas limitaciones y desafíos en el pronóstico de series temporales financieras, como la dificultad de elegir una arquitectura de red adecuada, un algoritmo de entrenamiento, una función de activación y variables de entrada; el riesgo de sobreajuste y problemas de generalización; la falta de interpretabilidad y transparencia; y el alto costo computacional y tiempo (<span class="citation" data-cites="TEALAB2018334">Tealab (<a href="#ref-TEALAB2018334" role="doc-biblioref">2018</a>)</span>).</p>
<p>Para superar estas limitaciones y desafíos, los investigadores han propuesto varias mejoras y extensiones de RNA para el pronóstico de series temporales financieras en las últimas décadas. Algunos de los principales desarrollos incluyen:</p>
<ul>
<li><p>El uso de modelos híbridos que combinan RNA con otras técnicas, como lógica difusa, algoritmos genéticos, análisis de ondículas, máquinas de vectores de soporte y aprendizaje profundo para mejorar el rendimiento y la solidez de las RNA (<span class="citation" data-cites="wongguo2010">Wong y Guo (<a href="#ref-wongguo2010" role="doc-biblioref">2010</a>)</span>).</p></li>
<li><p>El uso de redes neuronales recurrentes (en lo adelante RNR) o bidireccional, que son un tipo especial de RNA que pueden procesar datos secuenciales y capturar dependencias temporales. Se ha demostrado que las RNR superan a las redes neuronales unidireccionales en series temporales complejas y no lineales (<span class="citation" data-cites="GURESEN201110389">Guresen, Kayakutlu, y Daim (<a href="#ref-GURESEN201110389" role="doc-biblioref">2011</a>)</span>).</p></li>
<li><p>El uso de modelos de RNA más complejas mediante la combinación de distintas capas, como son las redes neuronales convolucionales (en lo adelante, CNN), las long short-term memory (en lo adelante, LSTM), las gated recurrent units (en lo adelante, GRU) se han aplicado a la previsión de series temporales financieras con resultados prometedores (<span class="citation" data-cites="SEZER2020106181">Sezer, Gudelek, y Ozbayoglu (<a href="#ref-SEZER2020106181" role="doc-biblioref">2020</a>)</span>).</p></li>
</ul>
<p>La historia de las RNA en el pronóstico de series temporales financieras muestra que las mismas han ido evolucionando y mejorando con el tiempo para hacer frente a la complejidad y la incertidumbre de los mercados financieros. Sin embargo, todavía persisten algunos de los desafíos y limitaciones señalados con anterioridad como como el sobreajuste, la generalización, la interpretabilidad, la robustez y el costo computacional.</p>
</section>
<section id="redes-neuronales-convolucionales" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="redes-neuronales-convolucionales"><span class="header-section-number">1.2.2</span> Redes neuronales convolucionales</h3>
<p>El modelo de RNA que se usó en este trabajo está compuesto por varias capas siendo las más importantes la capa Conv1D, un tipo especifico de CNN, y la capa LSTM, ambas mencionadas en el sub-epígrafe anterior cuando se listaron las estructuras de ANN que más se utilicen en la actualidad. Este sub-epígrafe se centra en la Capa Conv1D, por lo que a se exploran los conceptos fundamentales para comprender el funcionamiento de esta, explicándose la convolución, las redes neuronales convolucionales y Conv1D y su uso para el análisis de series temporales. Se brinda una descripción general de la convolución y cómo se puede aplicar a los datos de series temporales. Luego, se analizan las CNN y su arquitectura, que les permite aprender características automáticamente a partir de datos de series temporales. Finalmente, se explica Conv1D, un tipo específico de capa de red neuronal convolucional que es particularmente eficaz para procesar datos de series temporales.</p>
<p>Como se expone en <span class="citation" data-cites="rafid23">Siddiqui (<a href="#ref-rafid23" role="doc-biblioref">2023</a>)</span> la convolución es una operación matemática que se usa comúnmente en el procesamiento de señales y el análisis de imágenes. Implica tomar dos funciones y producir una tercera función que representa cómo una de las funciones originales modifica a la otra. En el contexto de los datos de series temporales, la convolución se puede utilizar para extraer características de los datos aplicando un filtro a la serie temporal.</p>
<p>Además de extraer características de los datos de series temporales, la convolución también se puede utilizar para otras tareas, como la reducción de ruido, la detección de anomalías y la predicción. Por ejemplo, se puede entrenar una CNN para predecir valores futuros de una serie temporal aprendiendo los patrones subyacentes en los datos. En general, la convolución es una herramienta poderosa para analizar datos de series temporales y sus aplicaciones son numerosas <span class="citation" data-cites="rafid23">Siddiqui (<a href="#ref-rafid23" role="doc-biblioref">2023</a>)</span>.</p>
<p>Las CNN fueron por primera vez introducidas en <span class="citation" data-cites="cnn">Lecun et&nbsp;al. (<a href="#ref-cnn" role="doc-biblioref">1998</a>)</span> son un tipo de modelo de aprendizaje profundo que se usa comúnmente para el análisis de imágenes. Sin embargo, como se ha mencionado con anterioridad también se pueden utilizar para el análisis de series temporales, ya que son muy adecuados para aprender características a partir de datos que tienen una estructura espacial o temporal.</p>
<p>La arquitectura de una CNN consta de una o más capas convolucionales, que aplican filtros a los datos de entrada para extraer características. Cada filtro es un conjunto de pesos que se aprenden durante el proceso de entrenamiento. Al deslizar el filtro sobre los datos de entrada, la capa convolucional calcula un producto escalar en cada posición, produciendo un nuevo mapa de características <span class="citation" data-cites="cnn">Lecun et&nbsp;al. (<a href="#ref-cnn" role="doc-biblioref">1998</a>)</span>.</p>
<p>En un contexto de series de tiempo, una CNN puede aprender a extraer automáticamente características de los datos en diferentes escalas e intervalos de tiempo, lo que la convierte en una herramienta poderosa para el análisis de series de tiempo. Una ventaja clave de usar una CNN para el análisis de series de tiempo es que reduce la necesidad de ingeniería de características manual. En lugar de diseñar filtros a mano, CNN aprende a extraer automáticamente características de los datos, haciéndolo más flexible y adaptable a diferentes tipos de datos de series temporales.</p>
<p>En general, la arquitectura de una CNN le permite aprender características automáticamente a partir de datos de series temporales, lo que la convierte en una herramienta poderosa para el análisis de series temporales, siendo las Conv1D una de las estructuras de CNN más usadas para esta tarea.</p>
<p>Como se explica en <span class="citation" data-cites="hongj20">Jing (<a href="#ref-hongj20" role="doc-biblioref">2020</a>)</span> Conv1D es un tipo específico de capa de CNN que está diseñado para procesar datos unidimensionales, como datos de series temporales. Mientras que las CNN tradicionales están diseñadas para procesar datos bidimensionales, Conv1D está optimizado específicamente para datos unidimensionales, lo que lo hace más eficiente y eficaz para el análisis de series temporales.</p>
<p>La arquitectura de una capa Conv1D es similar a la de una CNN tradicional, pero con algunas diferencias clave. En lugar de usar filtros bidimensionales, Conv1D usa filtros unidimensionales, que se aplican a la serie temporal de entrada para extraer características. Las características que se extraen de la serie dependerán de las distintas configuraciones usadas para la configuración del filtro y la cantidad de filtros utilizados, siendo la fórmula para calcular la cantidad de característica que extrae cada filtro la siguiente <a href="#eq-cnn-lout">Ecuación&nbsp;<span class="quarto-unresolved-ref">eq-cnn-lout</span></a> (<span class="citation" data-cites="hongj20">Jing (<a href="#ref-hongj20" role="doc-biblioref">2020</a>)</span>):</p>
<p><span id="eq-cnn-lout"><span class="math display">\[
\begin{aligned}
L_{out} &amp;= \frac{L_{in} + 2*padding - dilation*(kerenel\_size - 1)-1}{stride} + 1 \\
\end{aligned}
\tag{1.1}\]</span></span></p>
<p>Donde:</p>
<div class="margin">
<p><em>Lout</em>: es la longitud de la salida del proceso de filtrado o la cantidad de características.</p>
</div>
<div class="margin">
<p><em>Lin</em>: la longitud del vector de entrada, correspondiendo en el análisis de series de tiempo a la cantidad de observaciones que contienen las muestras de la serie de tiempo que se pasan al filtro.</p>
</div>
<div class="margin">
<p><em>kernel_size</em>: es el tamaño del filtro, lo que define cuantas observaciones del vector de entradas se pasan al filtro cada vez. <a href="#fig-HJks">Figura&nbsp;<span class="quarto-unresolved-ref">fig-HJks</span></a> representa como el tamaño del filtro puede afectar la longitud del vector de salida.</p>
</div>
<div class="margin">
<p><em>stride</em>: representa la cantidad de pasos u observaciones en las que se mueve la selección de observaciones que se pasa al filtro. <a href="#fig-HJstride">Figura&nbsp;<span class="quarto-unresolved-ref">fig-HJstride</span></a> representa como el parámetro stride puede afectar la longitud del vector de salida.</p>
</div>
<div class="margin">
<p><em>dilation</em>: es la distancia de las observaciones que pasan al filtro. <a href="#fig-HJdilation">Figura&nbsp;<span class="quarto-unresolved-ref">fig-HJdilation</span></a> representa como el parámetro dilation puede afectar la longitud del vector de salida.</p>
</div>
<div class="margin">
<p><em>padding</em>: representa la cantidad de zeros que se añade a cada extremo del vector. <a href="#fig-HJpadding">Figura&nbsp;<span class="quarto-unresolved-ref">fig-HJpadding</span></a> representa como el parámetro padding puede afectar la longitud del vector de salida.</p>
</div>
<p>En general, Conv1D es una herramienta poderosa para procesar datos de series temporales y sus ventajas incluyen la eficiencia computacional y la capacidad de capturar dependencias temporales en los datos. Sus casos de uso son numerosos y abarcan diferentes campos, lo que lo convierte en una herramienta valiosa para el análisis de series temporales.</p>
</section>
<section id="long-short-term-memory" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="long-short-term-memory"><span class="header-section-number">1.2.3</span> Long short-term memory</h3>
<p>En el presente subepigrafe se explica por qué las LSTM son una de las estructuras más usadas de RNA en la previsión de series de tiempo, partiendo de una breve explicación de las RNR y porque estas son de utilidad en la solución de problemas de previsión de series de tiempo, profundizando en el porque las LSTM se diferencian del resto de las RNN, y el funcionamiento de cada una de las capas que componen la estructura de una capa LSTM.</p>
<p>En <span class="citation" data-cites="COlah15">Olah (<a href="#ref-COlah15" role="doc-biblioref">2015</a>)</span> se explica que una RNN puede considerarse como copias múltiples de la misma red, <a href="#fig-CORNRstruct">Figura&nbsp;<span class="quarto-unresolved-ref">fig-CORNRstruct</span></a>, expone que este aspecto revela que las RNR están íntimamente relacionadas con secuencias y listas, lo que hace que este tipo de RNA sea el que se use naturalmente para el trabajo con series de tiempo.</p>
<p>Las RNR convencionales presentan un problema en lo relacionado con la capacidad de retener la información, como se explica en <span class="citation" data-cites="COlah15">Olah (<a href="#ref-COlah15" role="doc-biblioref">2015</a>)</span>, las RNN estándar se desempeñan con gran capacidad solo si, la información relevante para la situación actual es reciente, es decir donde la brecha entre la información relevante y el lugar en que se necesita es pequeña, <a href="#fig-CORInclose">Figura&nbsp;<span class="quarto-unresolved-ref">fig-CORInclose</span></a>; expone además que a medida que crece la brecha, las RNN estándar son incapaces de acceder a la información relevante, <a href="#fig-CORInaway">Figura&nbsp;<span class="quarto-unresolved-ref">fig-CORInaway</span></a>.</p>
<p>Como se ha mencionado con anterioridad las LSTM son un tipo de RNR que puede aprender dependencias a largo plazo en datos secuenciales. Estas fueron propuestas en <span class="citation" data-cites="SeppJur97">Hochreiter (<a href="#ref-SeppJur97" role="doc-biblioref">1997</a>)</span> y ha sido ampliamente utilizado para diversas tareas como el modelado del lenguaje, el reconocimiento de voz, la traducción automática, la descripción de imágenes y la previsión de series de tiempo.</p>
<p>La idea principal de LSTM es introducir una celda de memoria que pueda almacenar y actualizar información durante largos pasos de tiempo. La celda de memoria está controlada por tres puertas: una puerta de entrada, una puerta de olvido y una puerta de salida. Estas puertas son redes neuronales que aprenden a regular el flujo de información dentro y fuera de la célula <a href="#fig-CODrnrlstm">Figura&nbsp;<span class="quarto-unresolved-ref">fig-CODrnrlstm</span></a>.</p>
<p>La puerta de entrada decide qué cantidad de la nueva entrada agregar al estado de la celda. La puerta de olvido decide qué parte del estado de celda anterior mantener o borrar. La puerta de salida decide qué parte del estado de celda actual se va a enviar a la siguiente capa. <span class="citation" data-cites="COlah15">Olah (<a href="#ref-COlah15" role="doc-biblioref">2015</a>)</span> basado en lo expuesto en <span class="citation" data-cites="SeppJur97">Hochreiter (<a href="#ref-SeppJur97" role="doc-biblioref">1997</a>)</span>, describe la operativa de las puertas en cuatro pasos:</p>
<ol type="1">
<li>Decidir qué información se olvida del estado de la celda a través de la puerta, forget gate layer <span class="math inline">\(f_t\)</span>. Esta puerta ve a <span class="math inline">\(h_{t-1}\)</span>, estado oculto del período de tiempo anterior, y <span class="math inline">\(x_{t}\)</span>, entrada del instante de tiempo actual, y genera un número entre 0 (deshacerse) y 1 (mantener) para cada número en el estado de la celda <span class="math inline">\(C_{t-1}\)</span>, <a href="#fig-COLSTMstep1">Figura&nbsp;<span class="quarto-unresolved-ref">fig-COLSTMstep1</span></a>, <a href="#eq-lstm-fstep">Ecuación&nbsp;<span class="quarto-unresolved-ref">eq-lstm-fstep</span></a>.</li>
</ol>
<p><span id="eq-lstm-fstep"><span class="math display">\[
\begin{aligned}
f_t &amp;= \sigma(W_f [h_{t-1}, x_t] + b_f) \\
\end{aligned}
\tag{1.2}\]</span></span></p>
<ol start="2" type="1">
<li>Decidir qué nueva información se almacena en el estado de la celda. Para esto primero la puerta llamada input gate layer decide qué valores actualizar y luego, una capa tanh (tangente hiperbólica) crea un vector de nuevos valores candidatos (<span class="math inline">\(\tilde{C}_t\)</span>) que podrían agregarse al estado, <a href="#fig-COLSTMstep2">Figura&nbsp;<span class="quarto-unresolved-ref">fig-COLSTMstep2</span></a>, <a href="#eq-lstm-sstepf">Ecuación&nbsp;<span class="quarto-unresolved-ref">eq-lstm-sstepf</span></a> y <a href="#eq-lstm-ssteps">Ecuación&nbsp;<span class="quarto-unresolved-ref">eq-lstm-ssteps</span></a>.</li>
</ol>
<p><span id="eq-lstm-sstepf"><span class="math display">\[
\begin{aligned}
i_t &amp;= \sigma(W_i [h_{t-1}, x_t] + b_i) \\
\end{aligned}
\tag{1.3}\]</span></span></p>
<p><span id="eq-lstm-ssteps"><span class="math display">\[
\begin{aligned}
\tilde{C}_t &amp;= tanh(W_c [h_{t-1}, x_t] + b_c) \\
\end{aligned}
\tag{1.4}\]</span></span></p>
<ol start="3" type="1">
<li>Se actualiza el estado de la celda anterior, <span class="math inline">\(C_{t-1}\)</span> en el nuevo estado de la celda <span class="math inline">\(C_{t}\)</span>. Se multiplica el estado anterior por <span class="math inline">\(f_{t}\)</span>, olvidando lo necesario, luego se agrega $i_t* _t $. Estos son los nuevos valores candidatos, escalados según cuánto se necesita actualizar cada valor de estado, <a href="#fig-COLSTMstep3">Figura&nbsp;<span class="quarto-unresolved-ref">fig-COLSTMstep3</span></a>, ecuación 1.09.</li>
</ol>
<p><span id="eq-lstm-tstep"><span class="math display">\[
\begin{aligned}
C_t &amp;= f_t * C_{t-1} + i_t * \tilde{C}_t  \\
\end{aligned}
\tag{1.5}\]</span></span></p>
<ol start="4" type="1">
<li>Se genera una salida basándose en el estado de celda. Ejecutándose primero una capa sigmoidea que decide qué partes del estado de la celda es la salida; luego el estado de la celda pasa a través de una función tanh (escalando los valores entre −1 y 1) y se multiplican por la salida de la puerta, output gate, <a href="#fig-COLSTMstep4">Figura&nbsp;<span class="quarto-unresolved-ref">fig-COLSTMstep4</span></a>, <a href="#eq-lstm-fstepf">Ecuación&nbsp;<span class="quarto-unresolved-ref">eq-lstm-fstepf</span></a> y <a href="#eq-lstm-fsteps">Ecuación&nbsp;<span class="quarto-unresolved-ref">eq-lstm-fsteps</span></a>.</li>
</ol>
<p><span id="eq-lstm-fstepf"><span class="math display">\[
\begin{aligned}
o_t &amp;= \sigma(W_o [h_{t-1}, x_t] + b_o) \\
\end{aligned}
\tag{1.6}\]</span></span> <span id="eq-lstm-fsteps"><span class="math display">\[
\begin{aligned}
h_t &amp;= o_t * tanh(C_t) \\
\end{aligned}
\tag{1.7}\]</span></span></p>
<p>Las LSTM pueden aprender a capturar dependencias a largo plazo ajustando los valores de la puerta a través de la propagación inversa. Por ejemplo, si una determinada entrada es relevante para una salida posterior, la puerta de entrada aprenderá a dejarla entrar, y la puerta olvidada aprenderá a conservarla en el estado de celda hasta que sea necesaria. Por el contrario, si una entrada es irrelevante u obsoleta, la puerta de entrada aprenderá a ignorarla, y la puerta olvidada aprenderá a borrarla del estado de la celda.</p>
</section>
</section>
<section id="composición-de-carteras" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="composición-de-carteras"><span class="header-section-number">1.3</span> Composición de carteras</h2>
<p>En este epígrafe, se expone el problema de encontrar la mejor composición de cartera posible y explicar de manera general la teoría detrás de los objetivos para solucionarlo. Además, se enumerarán las técnicas más utilizadas, diferenciando entre enfoques clásicos y enfoques inteligentes. Posteriormente, se explicará qué es la programación cuadrática y se mencionarán algunas técnicas dentro de esta disciplina de optimización matemática. Se mostrará cómo el problema de optimización de carteras se puede describir como un problema de programación cuadrática. Además, se proporcionará una breve explicación del Dual Active Set Method, una técnica ampliamente utilizada en esta disciplina y que será utilizada en los capítulos siguientes.</p>
<section id="problema-y-técnicas" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="problema-y-técnicas"><span class="header-section-number">1.3.1</span> Problema y técnicas</h3>
<p>En este sub-epígrafe se presenta el problema de hallar la mejor composición de cartera posible, explicandose de manera general la teoría detras de los objetivos que se deben seguir para darle solución al mismo. Además, se listan las técnicas más utilizadas para darle solución a este problema, diferenciandolas entre enfoques clásicos y enfoques inteligentes.</p>
<p>Como se explica en <span class="citation" data-cites="Gunjan2023">Gunjan (<a href="#ref-Gunjan2023" role="doc-biblioref">2023</a>)</span> la optimización de cartera es el proceso de seleccionar la mejor combinación de activos para mantener en una cartera en función de objetivos predefinidos. Los objetivos pueden ser la maximización del rendimiento o la minimización del riesgo, o ambos. La optimización de la cartera implica encontrar las ponderaciones óptimas para cada activo de la cartera de manera que la cartera general cumpla con los objetivos deseados. Esto puede ser un problema desafiante debido a la gran cantidad de activos para elegir y las complejas relaciones entre ellos.</p>
<p>La optimización de la cartera es un proceso importante para los inversores, ya que les ayuda a minimizar el riesgo y maximizar el rendimiento de sus inversiones. Al seleccionar cuidadosamente los activos que mantendrán en su cartera, los inversores pueden lograr el nivel deseado de riesgo y rendimiento mientras diversifican sus inversiones para reducir el riesgo general. La optimización de la cartera es un mecanismo crucial que se utiliza para reducir el riesgo de la inversión.</p>
<p>Existen diversas técnicas que se pueden utilizar para resolver el problema de optimización de cartera. En <span class="citation" data-cites="Gunjan2023">Gunjan (<a href="#ref-Gunjan2023" role="doc-biblioref">2023</a>)</span> estas técnicas se encuentran clasificadas en en dos categorías: enfoques clásicos y enfoques inteligentes. A continuación, se explica de manera general algunas de las técnicas pertenecientes a cada enfoque.</p>
<p>Enfoques clásicos:</p>
<ul>
<li><p>Media-varianza: Esta técnica, propuesta en <span class="citation" data-cites="markowitz1967">Markowitz y Markowitz (<a href="#ref-markowitz1967" role="doc-biblioref">1967</a>)</span>, se basa en la idea de minimizar la varianza para una determinada rentabilidad esperada o maximizar la rentabilidad esperada para una determinada varianza. Es una técnica de programación cuadrática paramétrica (en lo adelante, PQP) que se puede utilizar para resolver problemas de optimización cuadrática que surgen en la optimización de carteras (<span class="citation" data-cites="Zhi08">Aijun Zhang &amp; Chun-hung Li &amp; Agus Sudjianto (<a href="#ref-Zhi08" role="doc-biblioref">2008</a>)</span>). El enfoque de la varianza media supone que los inversores tienen aversión al riesgo y prefieren carteras con una varianza más baja. La técnica consiste en construir una frontera de cartera que representa el conjunto de carteras que ofrecen el rendimiento esperado más alto para un nivel de riesgo dado. A continuación, se selecciona la cartera óptima de esta frontera en función de las preferencias de riesgo del inversor.</p></li>
<li><p>Varianza con asimetría: esta técnica amplía el enfoque de media-varianza teniendo en cuenta la asimetría de la distribución. Fue propuesta en <span class="citation" data-cites="samuelson1970">Samuelson (<a href="#ref-samuelson1970" role="doc-biblioref">1970</a>)</span> y se puede utilizar cuando la función de distribución no es de naturaleza cuadrática. La asimetría mide la asimetría de una distribución y puede proporcionar información adicional sobre los riesgos y rendimientos potenciales de una cartera. Al incorporar la asimetría en el proceso de optimización de la cartera, los inversores pueden comprender mejor los posibles riesgos a la baja y tomar decisiones más informadas.</p></li>
<li><p>Valor en riesgo (VaR): este enfoque estadístico mide la pérdida potencial de valor de una cartera durante un período definido para un intervalo de confianza dado. Fue introducido en la primera edición de <span class="citation" data-cites="jorion2007">Jorion (<a href="#ref-jorion2007" role="doc-biblioref">2007</a>)</span> en 1997 y requiere la determinación de tres parámetros: período de tiempo, nivel de confianza y unidad de valor en riesgo. El VaR proporciona una medida de la pérdida potencial máxima que podría ocurrir con una probabilidad dada en un horizonte de tiempo específico. Las instituciones financieras lo utilizan comúnmente para administrar su exposición al riesgo y cumplir con los requisitos reglamentarios.</p></li>
<li><p>Valor en riesgo condicional (CVaR): este enfoque amplía el VaR teniendo en cuenta la pérdida esperada que excede el VaR. Fue introducido en <span class="citation" data-cites="rockafellar2002">Rockafellar y Uryasev (<a href="#ref-rockafellar2002" role="doc-biblioref">2002</a>)</span> y puede manejar pérdidas extremas mediante el uso de pesos dinámicos derivados de datos históricos. CVaR proporciona una medida de la pérdida esperada que podría ocurrir más allá del umbral de VaR. También se conoce como Expected Shortfall (ES) o Tail Value-at-Risk (TVaR) y se considera una medida de riesgo más coherente que el VaR.</p></li>
<li><p>Desviación media-absoluta (MAD): esta técnica se puede emplear para problemas de selección de carteras de gran escala y muy diversificados. Fue introducido en <span class="citation" data-cites="konno1991">Konno y Yamazaki (<a href="#ref-konno1991" role="doc-biblioref">1991</a>)</span> y penaliza tanto las desviaciones positivas como las negativas. MAD proporciona una medida de la desviación absoluta promedio de los rendimientos de la cartera de su valor medio. Se considera más sólida que las medidas basadas en la varianza, ya que es menos sensible a los valores atípicos.</p></li>
<li><p>Minimax: Esta técnica utiliza la rentabilidad mínima como medida de riesgo. Fue introducido en <span class="citation" data-cites="cai2004minimax">Cai et&nbsp;al. (<a href="#ref-cai2004minimax" role="doc-biblioref">2004</a>)</span> y tiene ciertas ventajas cuando los rendimientos no se distribuyen normalmente. Minimax proporciona una medida del peor de los casos para una cartera al minimizar la pérdida potencial máxima que podría ocurrir. Puede ser útil para los inversores que están particularmente preocupados por los riesgos a la baja.</p></li>
</ul>
<p>Enfoques inteligentes:</p>
<ul>
<li><p>Redes bayesianas: estos modelos gráficos probabilísticos se pueden utilizar para modelar el riesgo y la rentabilidad. Fueron presentados en <span class="citation" data-cites="shenoy2000bayesian">Shenoy y Shenoy (<a href="#ref-shenoy2000bayesian" role="doc-biblioref">2000</a>)</span> y se pueden utilizar para visualizar la relación entre diferentes variables en un modelo. Las redes bayesianas proporcionan una forma de representar dependencias complejas entre variables utilizando gráficos acíclicos dirigidos (DAG). Se pueden usar para modelar relaciones inciertas entre variables y para hacer predicciones probabilísticas sobre eventos futuros. En el contexto de la gestión de carteras, las redes bayesianas se pueden utilizar para modelar las relaciones entre diferentes activos y hacer predicciones sobre sus rendimientos futuros en función de datos históricos y otra información relevante.</p></li>
<li><p>Regresión de vectores de soporte (SVR): esta técnica de aprendizaje automático se puede utilizar para determinar la cantidad a comprar y vender. Fue introducido por <span class="citation" data-cites="drucker1996linear">Drucker et&nbsp;al. (<a href="#ref-drucker1996linear" role="doc-biblioref">1996</a>)</span> y tiene ciertas ventajas sobre las técnicas basadas en estadísticas, como su capacidad para aprender de datos históricos. SVR implica construir un hiperplano que separa puntos de datos con diferentes etiquetas mientras maximiza el margen entre ellos. Puede usarse para tareas de regresión donde el objetivo es predecir valores continuos en lugar de etiquetas discretas. En el contexto de la gestión de carteras, SVR se puede utilizar para predecir precios de activos futuros en función de datos históricos y otra información relevante.</p></li>
<li><p>Redes neuronales artificiales: como se explico con anterioridad estos modelos computacionales se pueden utilizar para resolver problemas computacionales y de aprendizaje complejos. En el contexto de la gestión de carteras, las redes neuronales se pueden utilizar para predecir futuros precios o rendimientos de activos en función de datos históricos y otra información relevante, que es para lo que se usan en el presente trabajo.</p></li>
<li><p>Aprendizaje por refuerzo: este tipo de aprendizaje automático involucra a un agente o modelo que interactúa con su entorno para aprender de sus acciones. Fue presentado en <span class="citation" data-cites="sutton2018reinforcement">Sutton y Barto (<a href="#ref-sutton2018reinforcement" role="doc-biblioref">2018</a>)</span> y funciona para maximizar la recompensa al agente. El aprendizaje por refuerzo implica aprender a través de interacciones de prueba y error con un entorno. El agente realiza acciones en función de su estado actual y recibe recompensas o sanciones en función de los resultados de esas acciones. Con el tiempo, el agente aprende a realizar acciones que maximicen su recompensa acumulada. En el contexto de la gestión de carteras, el aprendizaje por refuerzo se puede utilizar para desarrollar estrategias comerciales que maximicen los rendimientos mientras se gestiona el riesgo.</p></li>
</ul>
</section>
<section id="programación-cuadrática" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="programación-cuadrática"><span class="header-section-number">1.3.2</span> Programación cuadrática</h3>
<p>En este sub-epígrafe se explica que es la programación cuadrática. Cuáles son algunas de las técnicas que existen dentro de esta disciplina de la optimización matemática. Se expone además como el problema de optimización de carteras se puede describir como un problema de programación cuadratica y se expondra de manera breve como funciona una de las técnicas más usadas en esta disciplina, concretamente la denominada Dual Active Set Method, la cuál es usada en los capítulos posteriores.</p>
<p>La programación cuadrática se puede elegir entre las técnicas enumeradas en el sub-epígrafe anterior por varias razones. En primer lugar, es una técnica bien establecida que se ha utilizado ampliamente en la optimización de carteras. Puede manejar problemas de optimización complejos con múltiples restricciones y puede proporcionar una forma eficiente y efectiva de resolver el problema de optimización de cartera. Esto lo convierte en una herramienta útil para los inversores que buscan minimizar el riesgo mientras logran el nivel de rendimiento deseado. Finalmente, la programación cuadrática tiene una sólida base teórica y ha sido ampliamente estudiada en la literatura. Esto la convierte en una técnica confiable y bien entendida que se puede utilizar con confianza en la optimización de la cartera.</p>
<p>Existen diversas tecnicas de programación cuadrática, entre las más utilizadas se ecuentran:</p>
<ul>
<li><p>Interior Point: Este es un método de programación lineal o no lineal que logra la optimización al pasar por el centro del sólido definido por el problema en lugar de alrededor de su superficie. Un algoritmo de programación lineal de tiempo polinómico utilizando un método de punto interior fue encontrado por <span class="citation" data-cites="karmarkar1984">Karmarkar (<a href="#ref-karmarkar1984" role="doc-biblioref">1984</a>)</span>.</p></li>
<li><p>Active Set: Este es un algoritmo utilizado para identificar las restricciones activas en un conjunto de restricciones de desigualdad. Las restricciones activas se expresan entonces como restricciones de igualdad, transformando así un problema restringido por la desigualdad en un subproblema más simple restringido por la igualdad. El método de conjunto activo fue introducido por primera vez en un artículo de <span class="citation" data-cites="beale1959">Beale (<a href="#ref-beale1959" role="doc-biblioref">1959</a>)</span> y desarrollado por <span class="citation" data-cites="fletcher1971">Fletcher (<a href="#ref-fletcher1971" role="doc-biblioref">1971</a>)</span> y <span class="citation" data-cites="bunch1977">Bunch y Kaufman (<a href="#ref-bunch1977" role="doc-biblioref">1977</a>)</span>.</p></li>
<li><p>Dual Active Set: El método, como se expone en <span class="citation" data-cites="Goldfarb1982ANS">Goldfarb y Idnani (<a href="#ref-Goldfarb1982ANS" role="doc-biblioref">1982</a>)</span> y <span class="citation" data-cites="Goldfarb1983ANS">Goldfarb y Idnani (<a href="#ref-Goldfarb1983ANS" role="doc-biblioref">1983</a>)</span>, es un algoritmo dual eficiente y numéricamente estable para la programación cuadrática definida positiva que aprovecha el hecho de que el mínimo sin restricciones de la función objetivo se puede usar como punto de partida.</p></li>
<li><p>Augmented Lagrangian: Fue introducido independientemente en <span class="citation" data-cites="hestenes1969">Magnus R. Hestenes (<a href="#ref-hestenes1969" role="doc-biblioref">1969</a>)</span> y <span class="citation" data-cites="powell1969">Powell (<a href="#ref-powell1969" role="doc-biblioref">1969</a>)</span>. Se utiliza para resolver problemas de optimización restringidos agregando un término de penalización a la función objetivo que penaliza cualquier violación de las restricciones. El término de penalización suele ser un múltiplo de una medida de infracción de restricción, como la suma de infracciones de restricción al cuadrado.</p></li>
<li><p>Conjugate Gradient: Este es un método iterativo para resolver sistemas de ecuaciones lineales con una matriz definida positiva simétrica. También se puede utilizar para resolver problemas de optimización sin restricciones al encontrar el mínimo de una función cuadrática. El método genera una secuencia de direcciones de búsqueda que se conjugan con respecto a la matriz que define el sistema de ecuaciones o función cuadrática. El método de gradiente conjugado fue introducido originalmente en un artículo de <span class="citation" data-cites="Hestenes1952">Magnus R. Hestenes y Stiefel (<a href="#ref-Hestenes1952" role="doc-biblioref">1952</a>)</span>.</p></li>
<li><p>Gradient Projection: El método de proyección de gradiente fue introducido en <span class="citation" data-cites="rosen1960">J. B. Rosen (<a href="#ref-rosen1960" role="doc-biblioref">1960</a>)</span> y <span class="citation" data-cites="rosen1961">J. Rosen (<a href="#ref-rosen1961" role="doc-biblioref">1961</a>)</span>. Este es un método iterativo para resolver problemas de optimización restringidos proyectando el gradiente en la región factible en cada iteración. El gradiente proyectado se utiliza entonces como dirección de búsqueda, y se realiza una búsqueda de línea a lo largo de esta dirección para encontrar una nueva iteración que satisfaga las restricciones y reduzca la función objetivo.</p></li>
</ul>
<p>De las técnicas señaladas con anterioridad se seleccionó el algoritmo Dual Active Set Method (en lo adelante, DASM) que como se mencionó con anterioridad fue introducido en <span class="citation" data-cites="Goldfarb1982ANS">Goldfarb y Idnani (<a href="#ref-Goldfarb1982ANS" role="doc-biblioref">1982</a>)</span> y <span class="citation" data-cites="Goldfarb1983ANS">Goldfarb y Idnani (<a href="#ref-Goldfarb1983ANS" role="doc-biblioref">1983</a>)</span>, es un algoritmo de optimización para resolver problemas de programación cuadrática. El algoritmo predice el conjunto activo de restricciones que se satisfacen con igualdad en la solución del problema. Calcula una secuencia de soluciones óptimas de problemas QP que involucran algunas de las restricciones del problema original, denominada secuencia de puntos factibles duales.</p>
<p>A continuación, se presenta un ejemplo general de cómo podría funcionar el algoritmo DASM usando valores hipotéticos para un problema de optimización de cartera con 2 activos, el ejemplo fue construido a partir de lo expuesto en <span class="citation" data-cites="GOSWAMI2012620">Goswami, Mondal, y Paruya (<a href="#ref-GOSWAMI2012620" role="doc-biblioref">2012</a>)</span> y <span class="citation" data-cites="rwalk14">Walker (<a href="#ref-rwalk14" role="doc-biblioref">2014</a>)</span>:</p>
<p>Bajo la suposición de que se trata de encontrar la mejor composición de una cartera en la que, por simplicidad, tenemos 2 activos, Se plantearia el problema cuadrático de la siguiente manera <a href="#eq-qpop">Ecuación&nbsp;<span class="quarto-unresolved-ref">eq-qpop</span></a>:</p>
<p><span id="eq-qpop"><span class="math display">\[
\begin{aligned}
min~~Q(\vec{w}) &amp;= \vec{w}^TC\vec{w}\\
sujeto~a:\\
w_{1}+w_{2}=1\\
0\leq{w_{i}}\leq{1}\\
w_{1}\mathbb{E} + w_{2}\mathbb{E} \geq{0.005}
\end{aligned}
\tag{1.8}\]</span></span></p>
<p>Suponiendo que los cuales tienen unos rendimientos mensuales medios <span class="math inline">\(r=\begin{bmatrix} 0.02 &amp; 0.03 \end{bmatrix}\)</span> y matriz de covarianza <span class="math inline">\(C=\begin{bmatrix} 0.001 &amp; 0.0008 \\ 0.0008 &amp; 0.002 \end{bmatrix}\)</span> . Se pueden construir los vectores y matrices necesarios para el algoritmo DASM de la siguiente manera:</p>
<ul>
<li><p>El vector de rentabilidad media mensual sería <span class="math inline">\(r=\begin{bmatrix} 0.02 &amp; 0.03 \end{bmatrix}\)</span>.</p></li>
<li><p>La matriz de covarianza C se usaría como la matriz D en DASM.</p></li>
<li><p>La restricción <span class="math inline">\(w_{1}+w_{2}=1\)</span> se puede escribir en forma de matriz como <span class="math inline">\(\begin{bmatrix} 1 &amp; 1 \end{bmatrix} \begin{bmatrix} w_1 \\ w_2 \end{bmatrix} = 1\)</span>. Esta sería la primera fila de la matriz <span class="math inline">\(A\)</span> en DASM.</p></li>
<li><p>El requisito de rentabilidad mínima <span class="math inline">\(w_{1}\mathbb{E} + w_{2}\mathbb{E} \geq{0.005}\)</span> puede escribirse en forma de matriz como <span class="math inline">\(\begin{bmatrix} 0.02 &amp; 0.03 \end{bmatrix} \begin{bmatrix} w_1 \\ w_2 \end{bmatrix} \geq{0.005}\)</span>. Esta sería otra fila de la matriz <span class="math inline">\(A\)</span> en DASM.</p></li>
<li><p>Las restricciones <span class="math inline">\(0\leq{w_i}\leq{1}\)</span> se pueden escribir en forma de matriz como <span class="math inline">\(\begin{bmatrix} 1 &amp; 0 \end{bmatrix} \begin{bmatrix} w_1 \\ w_2 \end{bmatrix} \geq{0}\)</span> y <span class="math inline">\(\begin{bmatrix} 0 &amp; 1 \end{bmatrix} \begin{bmatrix} w_1 \\ w_2 \end{bmatrix} \geq{0}\)</span> para límites inferiores y <span class="math inline">\(\begin{bmatrix} -1 &amp; 0 \end{bmatrix} \begin{bmatrix} w_1 \\ w_2 \end{bmatrix} \geq{-1}\)</span> y <span class="math inline">\(\begin{bmatrix} 0 &amp; -1 \end{bmatrix} \begin{bmatrix} w_1 \\ w_2 \end{bmatrix} \geq{-1}\)</span> para límites superiores.</p></li>
<li><p>La matriz <span class="math inline">\(A\)</span> luciría así: <span class="math inline">\(A=\begin{bmatrix} 1 &amp; 1 \\ 0.02 &amp; 0.03 \\ 1 &amp; 0 \\ 0 &amp; 1 \\ -1 &amp; 0 \\ 0 &amp; -1 \end{bmatrix}\)</span></p></li>
</ul>
<p>El vector <span class="math inline">\(b\)</span> correspondiente sería <span class="math inline">\(\begin{bmatrix} 1 &amp; 0.005 &amp; 0 &amp; 0 &amp; -1 &amp; -1\end{bmatrix}\)</span>. Luego podemos usar el algoritmo DASM para resolver este problema de programación cuadrática y determinar la asignación óptima de activos en nuestra cartera.</p>
<p>Paso 0: Encuentre el mínimo sin restricciones resolviendo el problema de programación cuadrática sin restricciones. Establecer el número de elementos del conjunto activo A (conjunto vacío) a cero.</p>
<p>Paso 1: Elija una restricción violada, si la hay. En este caso, supongamos que se viola la restricción <span class="math inline">\(w_{1}+w_{2}=1\)</span>.</p>
<p>Paso 2: Calcule las direcciones del paso primario y dual y la longitud del paso <span class="math inline">\(t=min(t_{1},t_{2})\)</span>. Supongamos que <span class="math inline">\(t=t_{2}\)</span>.</p>
<p>Paso 3: Da un paso y actualiza el conjunto activo A y la solución (<span class="math inline">\(S\)</span>) par (x, A). Como <span class="math inline">\(t=t_{2}\)</span> , agregamos la p-ésima restricción (en este caso <span class="math inline">\(w_1+w_2=1\)</span>) a <span class="math inline">\(\bar{N}\)</span> y actualizamos <span class="math inline">\(H\)</span> y <span class="math inline">\(N^{*}\)</span> en <a href="#eq-consup">Ecuación&nbsp;<span class="quarto-unresolved-ref">eq-consup</span></a>.</p>
<p><span id="eq-consup"><span class="math display">\[
\begin{aligned}
N^{*}=(\bar{N}^{T}Q^{-1}\bar{N})\bar{N}^{T}Q^{-1}\\
H=Q^{-1}(I-\bar{N}N^{*})
\end{aligned}
\tag{1.9}\]</span></span></p>
<p>Donde <span class="math inline">\(N^{*}\)</span> es la pseudo-inversa o la inversa generalizada Moore-Penrose de <span class="math inline">\(\bar{N}\)</span>, la matriz de los vectores normales de las restricciones en el conjunto activo <span class="math inline">\(A\)</span>. <span class="math inline">\(H\)</span> es el operador hessiano inverso reducido de <span class="math inline">\(Q\)</span>.</p>
<p>Se repetin estos pasos de manera iterativa hasta que se satisfagan todas las restricciones y se haya determinado la asignación óptima de activos.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Zhi08" class="csl-entry" role="doc-biblioentry">
Aijun Zhang &amp; Chun-hung Li &amp; Agus Sudjianto, Zhi-li Wu &amp;. 2008. <span>«Trace solution paths for SVMs via parametric quadratic programming»</span>. Researchgate. 2008. <a href="https://www.researchgate.net/publication/228577955_Trace_solution_paths_for_SVMs_via_parametric_quadratic_programming">https://www.researchgate.net/publication/228577955_Trace_solution_paths_for_SVMs_via_parametric_quadratic_programming</a>.
</div>
<div id="ref-anderson2017" class="csl-entry" role="doc-biblioentry">
Anderson, D. R., D. J. Sweeney, T. A. Williams, D. J. Camm, y J. J Cochran. 2017. <em>Statistics for business &amp; economics</em>. Boston: Cengage Learning.
</div>
<div id="ref-ZHANG199835" class="csl-entry" role="doc-biblioentry">
B. Eddy Patuwo &amp; Michael Y. Hu, Guoqiang Zhang &amp;. 1998. <span>«Forecasting with artificial neural networks:: The state of the art»</span>. <em>International Journal of Forecasting</em> 14 (1): 35-62. https://doi.org/<a href="https://doi.org/10.1016/S0169-2070(97)00044-7">https://doi.org/10.1016/S0169-2070(97)00044-7</a>.
</div>
<div id="ref-banda2014" class="csl-entry" role="doc-biblioentry">
Banda, Hugo. 2014. <em>Inteligencia Artificial: Principios y Aplicaciones</em>. Quito, Ecuador: Escuela Politécnica Nacional.
</div>
<div id="ref-barone22" class="csl-entry" role="doc-biblioentry">
Barone, A. 2022. <span>«Opening Price: Definition, Example, Trading Strategies»</span>. 2022. <a href="https://www.investopedia.com/terms/o/openingprice.asp">https://www.investopedia.com/terms/o/openingprice.asp</a>.
</div>
<div id="ref-beale1959" class="csl-entry" role="doc-biblioentry">
Beale, EML. 1959. <span>«On quadratic proramming»</span>. <em>Naval Research Logistics Quarterly</em> 6 (3): 227-43.
</div>
<div id="ref-bmeqe" class="csl-entry" role="doc-biblioentry">
BME. s.&nbsp;f. <span>«¿Qué es BME?»</span> Accedido 24 de abril de 2023. <a href="https://www.bolsasymercados.es/esp/Sobre-BME/Que-es">https://www.bolsasymercados.es/esp/Sobre-BME/Que-es</a>.
</div>
<div id="ref-bunch1977" class="csl-entry" role="doc-biblioentry">
Bunch, James R, y Linda Kaufman. 1977. <span>«Some stable methods for calculating inertia and solving symmetric linear systems»</span>. <em>Mathematics of computation</em> 31 (137): 163-79.
</div>
<div id="ref-cai2004minimax" class="csl-entry" role="doc-biblioentry">
Cai, Xiaoqiang, Kok Lay Teo, XQ Yang, y Xun Yu Zhou. 2004. <span>«Minimax portfolio optimization: empirical numerical study»</span>. <em>Journal of the Operational Research Society</em> 55 (1): 65-72.
</div>
<div id="ref-castillo_varela2010" class="csl-entry" role="doc-biblioentry">
Castillo, R. A., y R. Varela. 2010. <em>ECONOMETRÍA PRÁCTICA: Fundamentos de Series de Tiempo</em>. México: Universidad Autónoma de Baja California.
</div>
<div id="ref-chen22" class="csl-entry" role="doc-biblioentry">
Chen, J. 2022. <span>«Today’s High»</span>. 2022. <a href="https://www.investopedia.com/terms/t/todayshigh.asp">https://www.investopedia.com/terms/t/todayshigh.asp</a>.
</div>
<div id="ref-chirinos2018" class="csl-entry" role="doc-biblioentry">
Chirinos, S. 2018. <span>«Series cronológicas»</span>. <a href="https://www.slideshare.net/SuedimarChirinos/series-cronologicas-119058959" class="uri">https://www.slideshare.net/SuedimarChirinos/series-cronologicas-119058959</a>. 2018.
</div>
<div id="ref-chollet2018deep" class="csl-entry" role="doc-biblioentry">
Chollet, F., y J. J. Allaire. 2018. <em>Deep Learning with R</em>. Manning Publications. <a href="https://books.google.es/books?id=xnIRtAEACAAJ">https://books.google.es/books?id=xnIRtAEACAAJ</a>.
</div>
<div id="ref-cnmvacci" class="csl-entry" role="doc-biblioentry">
CNMV. s.&nbsp;f.a. <span>«Glosario Financiero: Acción»</span>. Accedido 24 de abril de 2023. <a href="https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&amp;letra=A&amp;idlng=1">https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&amp;letra=A&amp;idlng=1</a>.
</div>
<div id="ref-cnmvbv" class="csl-entry" role="doc-biblioentry">
———. s.&nbsp;f.b. <span>«Glosario Financiero: Bolsa de valores»</span>. Accedido 24 de abril de 2023. <a href="https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&amp;letra=B&amp;idlng=1">https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&amp;letra=B&amp;idlng=1</a>.
</div>
<div id="ref-cnmvsibe" class="csl-entry" role="doc-biblioentry">
———. s.&nbsp;f.c. <span>«Glosario Financiero: Servicio de Interconexión Bursátil Español, SIBE»</span>. Accedido 24 de abril de 2023. <a href="https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&amp;letra=S&amp;idlng=1">https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&amp;letra=S&amp;idlng=1</a>.
</div>
<div id="ref-dodge2008" class="csl-entry" role="doc-biblioentry">
Dodge, Y. 2008. <span>«Time Series»</span>. En <em>The Concise Encyclopedia of Statistics</em>, 536-39. New York, NY: Springer New York. <a href="https://doi.org/10.1007/978-0-387-32833-1_401">https://doi.org/10.1007/978-0-387-32833-1_401</a>.
</div>
<div id="ref-downey22" class="csl-entry" role="doc-biblioentry">
Downey, L. 2022. <span>«Today’s Low»</span>. 2022. <a href="https://www.investopedia.com/terms/t/todayslow.asp">https://www.investopedia.com/terms/t/todayslow.asp</a>.
</div>
<div id="ref-drucker1996linear" class="csl-entry" role="doc-biblioentry">
Drucker, Harris, Christopher Burges, Linda Kaufman, Alex Smola, y Vladimir Vapnik. 1996. <span>«Linear support vector regression machines»</span>. <em>Advances in neural information processing systems</em> 9 (9): 155-61.
</div>
<div id="ref-espallargas2012" class="csl-entry" role="doc-biblioentry">
Espallargas, S. D., y M. V. Solís. 2012. <em>Econometría y series temporales: aplicaciones</em>. La Habana: Editorial Félix Varela.
</div>
<div id="ref-fletcher1971" class="csl-entry" role="doc-biblioentry">
Fletcher, Roger. 1971. <span>«A general quadratic programming algorithm»</span>. <em>IMA Journal of Applied Mathematics</em> 7 (1): 76-91.
</div>
<div id="ref-ganti20" class="csl-entry" role="doc-biblioentry">
Ganti, A. 2020. <span>«Adjusted Closing Price»</span>. 2020. <a href="https://www.investopedia.com/terms/a/adjusted_closing_price.asp">https://www.investopedia.com/terms/a/adjusted_closing_price.asp</a>.
</div>
<div id="ref-Goldfarb1982ANS" class="csl-entry" role="doc-biblioentry">
Goldfarb, Donald, y Ashok U. Idnani. 1982. <span>«Dual and primal-dual methods for solving strictly convex quadratic programs»</span>. En <em>Numerical Analysis</em>, editado por J. P. Hennart, 226-39. Berlin, Heidelberg: Springer Berlin Heidelberg.
</div>
<div id="ref-Goldfarb1983ANS" class="csl-entry" role="doc-biblioentry">
———. 1983. <span>«A numerically stable dual method for solving strictly convex quadratic programs»</span>. <em>Mathematical Programming</em> 27: 1-33.
</div>
<div id="ref-GOSWAMI2012620" class="csl-entry" role="doc-biblioentry">
Goswami, Nababithi, Supriyo K. Mondal, y Swapan Paruya. 2012. <span>«A Comparative Study of Dual Active-Set and Primal-Dual Interior-Point Method»</span>. <em>IFAC Proceedings Volumes</em> 45 (15): 620-25. https://doi.org/<a href="https://doi.org/10.3182/20120710-4-SG-2026.00029">https://doi.org/10.3182/20120710-4-SG-2026.00029</a>.
</div>
<div id="ref-Gunjan2023" class="csl-entry" role="doc-biblioentry">
Gunjan, Siddhartha, Abhishek &amp; Bhattacharyya. 2023. <span>«<span>A brief review of portfolio optimization techniques</span>»</span>. <em>Artificial Intelligence Review</em> 56 (5): 3847-86. <a href="https://doi.org/10.1007/s10462-022-10273-7">https://doi.org/10.1007/s10462-022-10273-7</a>.
</div>
<div id="ref-GURESEN201110389" class="csl-entry" role="doc-biblioentry">
Guresen, Erkam, Gulgun Kayakutlu, y Tugrul U. Daim. 2011. <span>«Using artificial neural network models in stock market index prediction»</span>. <em>Expert Systems with Applications</em> 38 (8): 10389-97. https://doi.org/<a href="https://doi.org/10.1016/j.eswa.2011.02.068">https://doi.org/10.1016/j.eswa.2011.02.068</a>.
</div>
<div id="ref-hayes21" class="csl-entry" role="doc-biblioentry">
Hayes, A. 2021. <span>«What Is Closing Price? Definition, How It’s Used, and Example»</span>. 2021. <a href="https://www.investopedia.com/terms/c/closingprice.asp">https://www.investopedia.com/terms/c/closingprice.asp</a>.
</div>
<div id="ref-haykin1998neural" class="csl-entry" role="doc-biblioentry">
Haykin, Simon. 1998. <em>Neural networks: a comprehensive foundation</em>. Prentice Hall PTR.
</div>
<div id="ref-hestenes1969" class="csl-entry" role="doc-biblioentry">
Hestenes, Magnus R. 1969. <span>«Multiplier and gradient methods»</span>. <em>Journal of optimization theory and applications</em> 4 (5): 303-20.
</div>
<div id="ref-Hestenes1952" class="csl-entry" role="doc-biblioentry">
Hestenes, Magnus R., y Eduard Stiefel. 1952. <span>«Methods of conjugate gradients for solving linear systems»</span>. <em>Journal of research of the National Bureau of Standards</em> 49: 409-35.
</div>
<div id="ref-SeppJur97" class="csl-entry" role="doc-biblioentry">
Hochreiter, Jürgen, Sepp &amp; Schmidhuber. 1997. <span>«<span>Long Short-Term Memory</span>»</span>. <em>Neural Computation</em> 9 (8): 1735-80. <a href="https://doi.org/10.1162/neco.1997.9.8.1735">https://doi.org/10.1162/neco.1997.9.8.1735</a>.
</div>
<div id="ref-IBM2021" class="csl-entry" role="doc-biblioentry">
IBM. 2021. <span>«Characteristics of time series»</span>. <a href="https://www.ibm.com/docs/en/spss-modeler/saas?topic=data-characteristics-time-series" class="uri">https://www.ibm.com/docs/en/spss-modeler/saas?topic=data-characteristics-time-series</a>. 2021.
</div>
<div id="ref-hongj20" class="csl-entry" role="doc-biblioentry">
Jing, Hong. 2020. <span>«How Convolutional Layers Work in Deep Learning Neural Networks?»</span> Jingles, Github Blog. 2020. <a href="https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/">https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/</a>.
</div>
<div id="ref-jorion2007" class="csl-entry" role="doc-biblioentry">
Jorion, Philippe. 2007. <em>Value at risk: the new benchmark for managing financial risk</em>. The McGraw-Hill Companies, Inc.
</div>
<div id="ref-karmarkar1984" class="csl-entry" role="doc-biblioentry">
Karmarkar, Narendra. 1984. <span>«A new polynomial-time algorithm for linear programming»</span>. En <em>Proceedings of the sixteenth annual ACM symposium on Theory of computing</em>, 302-11.
</div>
<div id="ref-kocenda2017" class="csl-entry" role="doc-biblioentry">
Kocenda, E., y A. Cerný. 2017. <em>Elements of Time Series Econometrics: An Applied Approach</em>. Prague: Karolinum Press.
</div>
<div id="ref-konno1991" class="csl-entry" role="doc-biblioentry">
Konno, Hiroshi, y Hiroaki Yamazaki. 1991. <span>«Mean-absolute deviation portfolio optimization model and its applications to Tokyo stock market»</span>. <em>Management science</em> 37 (5): 519-31.
</div>
<div id="ref-Larranaga07" class="csl-entry" role="doc-biblioentry">
Larrañaga, Iñaki &amp; Moujahid, Pedro &amp; Inza. 2007. <span>«Tema 14. Redes Neuronales»</span>. Departamento de Ciencias de la Computaci´on e Inteligencia Artificial, Universidad del Pa´ıs Vasco–Euskal Herriko Unibertsitatea. 2007. <a href="http://www.sc.ehu.es/ccwbayes/docencia/mmcc/docs/t14-neuronales.pdf">http://www.sc.ehu.es/ccwbayes/docencia/mmcc/docs/t14-neuronales.pdf</a>.
</div>
<div id="ref-cnn" class="csl-entry" role="doc-biblioentry">
Lecun, Y., L. Bottou, Y. Bengio, y P. Haffner. 1998. <span>«Gradient-based learning applied to document recognition»</span>. <em>Proceedings of the IEEE</em> 86 (11): 2278-2324. <a href="https://doi.org/10.1109/5.726791">https://doi.org/10.1109/5.726791</a>.
</div>
<div id="ref-markowitz1967" class="csl-entry" role="doc-biblioentry">
Markowitz, Harry M, y Harry M Markowitz. 1967. <em>Portfolio selection: efficient diversification of investments</em>. J. Wiley.
</div>
<div id="ref-McCarthy_Minsky_Rochester_Shannon_2006" class="csl-entry" role="doc-biblioentry">
McCarthy, John, Marvin L. Minsky, Nathaniel Rochester, y Claude E. Shannon. 2006. <span>«A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955»</span>. <em>AI Magazine</em> 27 (4): 12. <a href="https://doi.org/10.1609/aimag.v27i4.1904">https://doi.org/10.1609/aimag.v27i4.1904</a>.
</div>
<div id="ref-mitchell20" class="csl-entry" role="doc-biblioentry">
Mitchell, C. 2020. <span>«Market Price: Definition, Meaning, How To Determine, and Example»</span>. 2020. <a href="https://www.investopedia.com/terms/m/market-price.asp">https://www.investopedia.com/terms/m/market-price.asp</a>.
</div>
<div id="ref-COlah15" class="csl-entry" role="doc-biblioentry">
Olah, Christopher. 2015. <span>«Understanding LSTM networks»</span>. Colah’s blog. 2015. <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>.
</div>
<div id="ref-pinset21" class="csl-entry" role="doc-biblioentry">
Pinset, W. 2021. <span>«Understanding Stock Prices and Values»</span>. 2021. <a href="https://www.investopedia.com/articles/stocks/08/stock-prices-fool.asp">https://www.investopedia.com/articles/stocks/08/stock-prices-fool.asp</a>.
</div>
<div id="ref-powell1969" class="csl-entry" role="doc-biblioentry">
Powell, Michael JD. 1969. <span>«A method for nonlinear constraints in minimization problems»</span>. <em>Optimization</em>, 283-98.
</div>
<div id="ref-rockafellar2002" class="csl-entry" role="doc-biblioentry">
Rockafellar, R Tyrrell, y Stanislav Uryasev. 2002. <span>«Conditional value-at-risk for general loss distributions»</span>. <em>Journal of banking &amp; finance</em> 26 (7): 1443-71.
</div>
<div id="ref-rosen1961" class="csl-entry" role="doc-biblioentry">
Rosen, JB. 1961. <span>«The gradient projection method for nonlinear programming. Part II. Nonlinear constraints»</span>. <em>Journal of the Society for Industrial and Applied Mathematics</em> 9 (4): 514-32.
</div>
<div id="ref-rosen1960" class="csl-entry" role="doc-biblioentry">
Rosen, Jo Bo. 1960. <span>«The gradient projection method for nonlinear programming. Part I. Linear constraints»</span>. <em>Journal of the society for industrial and applied mathematics</em> 8 (1): 181-217.
</div>
<div id="ref-ruiz2011" class="csl-entry" role="doc-biblioentry">
Ruiz, M. C. 2011. <span>«Tema 5: Procesos Estocásticos»</span>. <a href="http://www.dmae.upct.es/~mcruiz/Telem06/Teoria/apuntes_procesos.pdf" class="uri">http://www.dmae.upct.es/~mcruiz/Telem06/Teoria/apuntes_procesos.pdf</a>; Departamento de Matemática y Estadística. Universidad Politécnica de Cartagena. 2011.
</div>
<div id="ref-samuelson1970" class="csl-entry" role="doc-biblioentry">
Samuelson, Paul A. 1970. <span>«The fundamental approximation theorem of portfolio analysis in terms of means, variances and higher moments»</span>. <em>The Review of Economic Studies</em> 37 (4): 537-42.
</div>
<div id="ref-SEZER2020106181" class="csl-entry" role="doc-biblioentry">
Sezer, Omer Berat, Mehmet Ugur Gudelek, y Ahmet Murat Ozbayoglu. 2020. <span>«Financial time series forecasting with deep learning : A systematic literature review: 2005–2019»</span>. <em>Applied Soft Computing</em> 90: 106181. https://doi.org/<a href="https://doi.org/10.1016/j.asoc.2020.106181">https://doi.org/10.1016/j.asoc.2020.106181</a>.
</div>
<div id="ref-shenoy2000bayesian" class="csl-entry" role="doc-biblioentry">
Shenoy, Catherine, y Prakash P Shenoy. 2000. <span>«Bayesian network models of portfolio risk and return»</span>. En. The MIT Press.
</div>
<div id="ref-rafid23" class="csl-entry" role="doc-biblioentry">
Siddiqui, J. Rafid. 2023. <span>«Why Convolve? Understanding Convolution and Feature Extraction in Deep Networks»</span>. Medium, Towards Data Science. 2023. <a href="https://towardsdatascience.com/why-convolve-understanding-convolution-and-feature-extraction-in-deep-networks-ee45d1fdd17c">https://towardsdatascience.com/why-convolve-understanding-convolution-and-feature-extraction-in-deep-networks-ee45d1fdd17c</a>.
</div>
<div id="ref-sutton2018reinforcement" class="csl-entry" role="doc-biblioentry">
Sutton, Richard S, y Andrew G Barto. 2018. <em>Reinforcement learning: An introduction</em>. MIT press.
</div>
<div id="ref-TEALAB2018334" class="csl-entry" role="doc-biblioentry">
Tealab, Ahmed. 2018. <span>«Time series forecasting using artificial neural networks methodologies: A systematic review»</span>. <em>Future Computing and Informatics Journal</em> 3 (2): 334-40. https://doi.org/<a href="https://doi.org/10.1016/j.fcij.2018.10.003">https://doi.org/10.1016/j.fcij.2018.10.003</a>.
</div>
<div id="ref-cfi23" class="csl-entry" role="doc-biblioentry">
Team, CFI. 2023. <span>«What is Stock Price?»</span> 2023. <a href="https://corporatefinanceinstitute.com/resources/capital-markets/stock-price/">https://corporatefinanceinstitute.com/resources/capital-markets/stock-price/</a>.
</div>
<div id="ref-tinvt22" class="csl-entry" role="doc-biblioentry">
Team, The Investopedia. 2022. <span>«Intrinsic Value Defined and How It’s Determined in Investing and Business»</span>. 2022. <a href="https://www.investopedia.com/terms/i/intrinsicvalue.asp">https://www.investopedia.com/terms/i/intrinsicvalue.asp</a>.
</div>
<div id="ref-villagarcia2006" class="csl-entry" role="doc-biblioentry">
Villagarcía, T. 2006. <span>«Series Temporales»</span>. <a href="https://halweb.uc3m.es/fjnm/estind/doc_grupo1/archivos/Apuntes%20de%20series.pdf" class="uri">https://halweb.uc3m.es/fjnm/estind/doc_grupo1/archivos/Apuntes%20de%20series.pdf</a>. 2006.
</div>
<div id="ref-villavicencio2010" class="csl-entry" role="doc-biblioentry">
Villavicencio, J. 2010. <span>«Introducción a las series de tiempo»</span>. <a href="http://www.estadisticas.gobierno.pr/iepr/LinkClick.aspx" class="uri">http://www.estadisticas.gobierno.pr/iepr/LinkClick.aspx</a>; Instituto de estadística de Puerto Rico. 2010.
</div>
<div id="ref-rwalk14" class="csl-entry" role="doc-biblioentry">
Walker, Ryan. 2014. <span>«Solving Quadratic Progams with R’s quadprog package»</span>. rwalk. 2014. <a href="https://rwalk.xyz/solving-quadratic-progams-with-rs-quadprog-package/">https://rwalk.xyz/solving-quadratic-progams-with-rs-quadprog-package/</a>.
</div>
<div id="ref-wongguo2010" class="csl-entry" role="doc-biblioentry">
Wong, W. K., y Z. X. Guo. 2010. <span>«<span>A hybrid intelligent model for medium-term sales forecasting in fashion retail supply chains using extreme learning machine and harmony search algorithm</span>»</span>. <em>International Journal of Production Economics</em> 128 (2): 614-24. <a href="https://ideas.repec.org/a/eee/proeco/v128y2010i2p614-624.html">https://ideas.repec.org/a/eee/proeco/v128y2010i2p614-624.html</a>.
</div>
</div>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copiado");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copiado");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./intro.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Introducción</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chapter1a.html" class="pagination-link">
        <span class="nav-page-text">Apéndices Capítulo 1</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>