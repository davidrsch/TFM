[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Trabajo Final de Máster",
    "section": "",
    "text": "Descripción\nEste es el sitio web para “Aplicación de redes neuronales artificiales y programación cuadrática en la gestión de carteras”, un Trabajo de Fin de Máster del Máster Universitario en Banca y Finanzas de la Universidade da Coruña. El trabajo fue realizado por David Díaz Rodríguez y tutelado por Xosé Manuel Martínez Filgueira. El sitio está construido usando Quarto.\nEl presente trabajo cuenta con un repositorio en el que se encuentra, además del código fuente para el presente sitio, algunos de los datos resultantes del procedimiento descrito a lo largo del trabajo, así como estructuras de los modelos de RNA utilizadas para la obtención de las predicciones.\nEl código del procedimiento expuesto en el presente trabajo, Anexo. 4 Códigos fue desarrollado usando la versión 4.3.1 de R y la versión 2023.06.1-524 del software RStudio. Los paquetes necesarios para la ejecución del código son los siguientes:\n\n\n\nPaquete\nVersión\n\n\n\n\nabind\n1.4.5\n\n\nDiagrammeR\n1.0.10\n\n\ndplyr\n1.1.2\n\n\nforecast\n8.21\n\n\nggplot2\n3.4.3\n\n\ngridExtra\n2.3\n\n\ngt\n0.9.0\n\n\njsonlite\n1.8.7\n\n\nkableExtra\n1.3.4\n\n\nkeras\n2.13.0\n\n\nknitr\n1.43\n\n\nlubridate\n1.9.2\n\n\nMatrix\n1.6.1\n\n\nMetrics\n0.1.4\n\n\nquadprog\n1.5.8\n\n\nquantmod\n0.4.25\n\n\nreadr\n2.1.4\n\n\nreadxl\n1.4.3\n\n\nsimplermarkdown\n0.0.6\n\n\nstringr\n1.5.0\n\n\ntensorflow\n2.13.0\n\n\ntibble\n3.2.1\n\n\ntidyr\n1.3.0\n\n\nTTR\n0.24.3\n\n\nxml2\n1.3.5\n\n\nxts\n0.13.1\n\n\nzoo\n1.8.12\n\n\n\nPuede consultar además:\n\n  \n    \n    Presentación\n  \n  \n    \n    Dashboard"
  },
  {
    "objectID": "greetings.html",
    "href": "greetings.html",
    "title": "Agradecimientos",
    "section": "",
    "text": "Quiero expresar mi más sincero agradecimiento a todas las personas que contribuyeron de manera significativa a la realización de este trabajo final de máster. Sin su apoyo, orientación y aliento, este logro no habría sido posible.\nDeseo agradecer a la Xunta de Galicia por proporcionarme los recursos para la realización de estudios de máster como beneficiario de la Becas Excelencia Juventud Exterior.\nDeseo agradecer a Universidade da Coruña por los conocimientos adquiridos al cursar el Máster Universitario en Banca y Finanzas. Su compromiso con la excelencia académica ha sido una fuente constante de inspiración.\nAgradezco profundamente a mi tutor Xosé Manuel Martínez Filgueira por su orientación experta y valiosas sugerencias a lo largo de este proceso. Sus conocimientos y dedicación fueron fundamentales para dar dirección y calidad a este trabajo.\nMis agradecimientos se extienden a mis compañeros de clase y amigos que brindaron un espacio para discusiones enriquecedoras y aportes valiosos que contribuyeron al desarrollo de este trabajo.\nTambién quiero expresar mi gratitud a mi familia por su constante apoyo emocional y comprensión durante los momentos desafiantes de este proceso académico."
  },
  {
    "objectID": "summaryes.html",
    "href": "summaryes.html",
    "title": "Resumen",
    "section": "",
    "text": "En el contexto del mundo financiero en constante cambio y complejidad, este trabajo aborda la aplicación de redes neuronales artificiales y programación cuadrática en la gestión de carteras financieras. Se destaca la importancia de caracterizar adecuadamente las series temporales financieras para realizar pronósticos más precisos y se examina el potencial de la combinación de las redes neuronales convolucionales y LSTM para mejorar la previsión de series de tiempo. En el proceso de composición de carteras, se aplica la programación cuadrática como una técnica eficiente para lograr una distribución óptima de activos financieros. En conclusión, el enfoque de combinar redes neuronales artificiales y programación cuadrática muestra promesa en la gestión de carteras financieras, pero es necesario un estudio más profundo y exhaustivo para determinar su eficiencia óptima. Este trabajo sienta las bases para futuras investigaciones, destacando la importancia de utilizar datos actualizados y configurar adecuadamente los modelos para lograr una gestión de carteras más informada y efectiva en un entorno financiero en constante evolución.\nPalabras clave: gestión de carteras, carteras, redes neuronales artificiales, programación cuadrática, series temporales financieras, predicción de precios, composición de carteras.\nNúmero de palabras: 14174"
  },
  {
    "objectID": "summaryen.html",
    "href": "summaryen.html",
    "title": "Abstract",
    "section": "",
    "text": "In the context of the financial world in constant change and complexity, this work deals with the application of artificial neural networks and quadratic programming in the management of financial portfolios. The importance of properly characterizing financial time series for more accurate forecasting is highlighted, and the potential of combining convolutional neural networks and LSTM to improve time series forecasting is examined. In the portfolio composition process, quadratic programming is applied as an efficient technique to achieve an optimal distribution of financial assets. In conclusion, the approach of combining artificial neural networks and quadratic programming shows promise in the management of financial portfolios, but a deeper and more exhaustive study is necessary to determine its optimal efficiency. This paper lays the groundwork for future research, highlighting the importance of using up-to-date data and properly configuring models to achieve more informed and effective portfolio management in an ever-evolving financial environment.\nKeywords: portfolio management, portfolios, artificial neural networks, quadratic programming, financial time series, price prediction, portfolio composition"
  },
  {
    "objectID": "summarygal.html",
    "href": "summarygal.html",
    "title": "Resumo",
    "section": "",
    "text": "No contexto do mundo financeiro en constante cambio e complexidade, este traballo trata sobre a aplicación das redes neuronais artificiais e da programación cuadrática na xestión de carteiras financeiras. Destaca a importancia de caracterizar adecuadamente as series temporales financeiras para unha previsión máis precisa e examínase o potencial de combinar redes neuronais convolucionais e LSTM para mellorar a previsión de series temporais. No proceso de composición da carteira aplícase a programación cuadrática como técnica eficiente para conseguir unha distribución óptima dos activos financeiros. En conclusión, o enfoque de combinar redes neuronais artificiais e programación cuadrática resulta prometedor na xestión de carteiras financeiras, pero é necesario un estudo máis profundo e exhaustivo para determinar a súa eficiencia óptima. Este traballo senta as bases para futuras investigacións, destacando a importancia de utilizar datos actualizados e de configurar adecuadamente os modelos para lograr unha xestión de carteira máis informada e eficaz nun entorno financeiro en constante evolución.\nPalabras clave: xestión de carteiras, carteiras, redes neuronais artificiais, programación cuadrática, series temporales financeiras, predición de prezos, composición da carteira."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1 Introducción",
    "section": "",
    "text": "En el ámbito financiero, la gestión eficiente de carteras es una tarea crucial para los inversionistas y administradores de activos, ya que busca maximizar los rendimientos y minimizar los riesgos asociados a las inversiones. En los últimos años, el campo de la inteligencia artificial y el aprendizaje automático ha experimentado un notable avance, lo que ha permitido la aplicación de técnicas innovadoras para mejorar el proceso de toma de decisiones financieras.\nEste trabajo se enfoca en la integración de dos poderosas herramientas: las redes neuronales artificiales y la programación cuadrática, para abordar el desafío de la gestión de carteras. La combinación de estas técnicas ofrece un enfoque sólido y prometedor para la previsión de series de tiempo financieras y la composición de carteras en un entorno financiero altamente dinámico y complejo.\nEl desarrollo del trabajo se estructura en varias secciones fundamentales para abordar de manera integral el tema. En primer lugar, se realiza una caracterización detallada de las series temporales financieras, examinando sus características y propiedades esenciales para entender mejor el comportamiento de los precios de los activos.\nA continuación, se explora el potencial de las redes neuronales artificiales en la previsión de las series de tiempo. Se presentan los antecedentes del uso de estas redes en este contexto y se destacan dos arquitecturas ampliamente utilizadas: las redes neuronales convolucionales y las redes Long Short-Term Memory (LSTM), ambas con la capacidad de capturar patrones complejos en los datos financieros.\nLa sección sobre composición de carteras aborda el problema y presenta diversas técnicas aplicadas en la gestión de activos. Es aquí donde se introduce la programación cuadrática como una herramienta relevante y eficiente para la construcción óptima de carteras de inversión.\nLa obtención de datos precisos y relevantes es crucial para cualquier análisis financiero y el trabajo con algoritmos de Machine Learning. Se describe la metodología aplicada para la obtención de datos, y como se computaron algunos de los indicadores más comunes utilizados en finanzas para usar como variables descriptivas del problema en conjunto con los datos históricos. Se expone además como se estructuran los vectores que se utilizarán en el modelado y entrenamiento de las redes neuronales.\nEn las últimas secciones, se aborda el proceso de modelado y entrenamiento, que implica la configuración adecuada de las redes neuronales y la implementación de la programación cuadrática para obtener resultados óptimos. Finalmente, se presentan los resultados obtenidos, incluyendo las predicciones generadas por las redes neuronales y la composición de carteras recomendadas, demostrando así la efectividad de la metodología propuesta en la gestión de carteras financieras.\nEn conjunto, este trabajo busca aportar una visión integral y actualizada sobre el uso de redes neuronales artificiales y programación cuadrática en la gestión de carteras, destacando su potencial como una opción para mejorar la toma de decisiones financieras y brindar a los inversionistas una herramienta valiosa para optimizar sus estrategias de inversión en un entorno cambiante y competitivo."
  },
  {
    "objectID": "body.html",
    "href": "body.html",
    "title": "2 Desarrollo del trabajo",
    "section": "",
    "text": "El presente trabajo se adentra en el campo del aprendizaje automático y la inteligencia artificial, específicamente en el uso combinado de redes neuronales artificiales y programación cuadrática. Esta poderosa sinergia busca ofrecer una solución efectiva y sofisticada para la previsión de series temporales financieras y la composición óptima de carteras de inversión. A través de la aplicación de estas técnicas, se persigue mejorar la toma de decisiones financieras y maximizar los rendimientos, al tiempo que se minimizan los riesgos asociados a las inversiones. En las siguientes secciones, se explorarán en detalle las distintas etapas del proceso, desde la caracterización de las series temporales y el funcionamiento de las redes neuronales, hasta la implementación de la programación cuadrática en la construcción de carteras eficientes."
  },
  {
    "objectID": "FSandP.html#series-de-tiempo-y-sus-características",
    "href": "FSandP.html#series-de-tiempo-y-sus-características",
    "title": "2.1 Caracterización de las series temporales financieras",
    "section": "2.1.1 Series de tiempo y sus características",
    "text": "2.1.1 Series de tiempo y sus características\nLas series cronológicas son un tipo de proceso estocástico que se caracteriza por ordenar las variables aleatorias según el tiempo. Esto significa que cada momento tiene asociado un valor de la variable que depende del azar y que puede cambiar a lo largo del tiempo. Según Ruiz (2011), un proceso estocástico es “una colección o familia de variables aleatorias, ordenadas según un subíndice que en general suele ser el tiempo” (p.01). El análisis de las series de tiempo puede tener distintos fines, como describir el comportamiento de las variables o predecir o pronosticar sus valores futuros, lo que es especialmente relevante para las series financieras.\nEl análisis de las series de tiempo es una herramienta estadística que permite estudiar el comportamiento de una variable a lo largo del tiempo. Sin embargo, no existe un consenso único sobre los componentes que se deben considerar en este tipo de análisis. Algunos autores, como Kocenda y Cerný (2017) y Anderson et al. (2017), proponen que las series de tiempo se pueden descomponer en tres componentes: tendencia, estacionalidad y ruido. Otros autores, como Dodge (2008) y Espallargas y Solís (2012), sugieren que se debe añadir un cuarto componente: el ciclo. Finalmente, hay autores que plantean que las series de tiempo pueden tener hasta cinco componentes estos son los casos de IBM (2021) y Chirinos (2018).\nTendencia: Es el patrón de cambio a largo plazo que se observa en una serie de datos. Se puede definir como la dirección general y persistente de las variaciones de la serie a lo largo del tiempo. Se puede clasificar en positiva (Figura 1), negativa (Figura 2) o nula (Figura 3), dependiendo de si la serie aumenta, disminuye o se mantiene constante en el largo plazo. La tendencia puede identificarse mediante el análisis gráfico o mediante métodos estadísticos. Este componente es importante para entender el comportamiento histórico y proyectar el futuro de una serie de datos, el mismo es común en los distintos criterios mencionados.\nEstacionalidad: Llamada también variación cíclica regular: Se refiere a la variación correspondiente a los movimientos de la serie que ocurren cada cierto periodo de tiempo, Figura 4. Este componente es, al igual que la tendencia, común en los criterios mencionados. Diferenciándose en que aquellos autores que exponen cuatro y cinco componentes llaman estacionalidad a las variaciones periódicas correspondientes a periodos menores o iguales a un año (como son periodicidad diaria, semanal, mensual, o anual), mientras que las variaciones periódicas correspondientes a periodos mayores las contemplan en un componente llamado, variaciones cíclicas. Por lo que para determinar la estacionalidad de una serie de tiempo es necesario analizarlas en un periodo no menor de dos años.\nUn componente que no se puede explicar por los otros elementos de la serie de tiempo es la variación irregular o error. Este componente también se conoce como variación aleatoria, ruido o residuo, y se muestra en la Figura 4. La variación irregular es común en los tres criterios mencionados anteriormente. Algunos autores distinguen entre la variación irregular, que es ocasional y aleatoria, y la variación atípica, que es causada por eventos aislados que alteran el comportamiento de la serie. La variación atípica se puede clasificar en varios tipos: aditiva, de innovación, de cambio de nivel, transiente, de estacionalidad aditiva y de tendencia local.\nUna forma de categorizar las series de tiempo es según el grado de variabilidad que presentan a lo largo del tiempo, según lo expuesto en Villagarcía (2006) se puede distinguir entre series homocedásticas y heterocedásticas. Las series homocedásticas son aquellas que mantienen un rango constante de variación, como se muestra en la Figura 3. Por el contrario, las series heterocedásticas son aquellas que cambian el rango de variación, aumentando o disminuyendo su amplitud, como se ilustra en las Figura 1 y Figura 2.\nUn concepto clave en el análisis de series de tiempo es el de estacionariedad. Una serie de tiempo es estacionaria cuando sus propiedades estadísticas, como la media, la varianza y la covarianza, no cambian con el tiempo. Esto implica que la serie no presenta tendencia, ciclos ni estacionalidad. Como señalan Castillo y Varela (2010), Villavicencio (2010) y Ruiz (2011), la estacionariedad es una condición necesaria para poder predecir el comportamiento futuro de una serie de tiempo usando técnicas estadísticas. En la Figura 3 se muestra un ejemplo de una serie de tiempo estacionaria.\nLas series de tiempo financieras presentan heterocedasticidad, es decir, varianzas que cambian en el tiempo. Esto implica que no son estacionarias y que su comportamiento depende de factores externos. Para verificar la estacionariedad de una serie de tiempo, se pueden utilizar diferentes métodos, como el correlograma, que muestra las funciones de autocorrelación y autocorrelación parcial de la serie, o las pruebas de raíz unitaria, como la de Dickey Fuller o la de Phillips Perron, que contrastan la hipótesis nula de que la serie tiene una raíz unitaria. Estos métodos se explican con más detalle en Castillo y Varela (2010), Villavicencio (2010) y Ruiz (2011). La Figura 5 ilustra un ejemplo de correlograma para una serie de tiempo financiera."
  },
  {
    "objectID": "FSandP.html#características-de-los-precios",
    "href": "FSandP.html#características-de-los-precios",
    "title": "2.1 Caracterización de las series temporales financieras",
    "section": "2.1.2 Características de los precios",
    "text": "2.1.2 Características de los precios\nInvertir en acciones o cualquier otro bien que cotice en el mercado de valores es una tarea compleja y desafiante, que requiere una comprensión profunda de las tendencias y fluctuaciones del mercado. En el centro de esta comprensión se encuentra la capacidad de analizar e interpretar los datos de precios del mercado de valores, lo que brinda información clave sobre el comportamiento de los participantes del mercado y los factores que impulsan los movimientos del mercado. El propósito de este sub-epígrafe es proporcionar una descripción general completa del entorno de los precios de las acciones y como son representados comúnmente los mismos, señalando los aspectos más importantes para la aplicación de las técnicas que se exploraran en los siguientes epígrafes.\nComo se explica en la CNMV (s. f.b) las bolsas de valores son mercados organizados donde se negocian acciones y otros valores, como renta fija, warrants, certificados y fondos cotizados. En BME (s. f.) se expone que, en España, existen cuatro bolsas tradicionales (Madrid, Barcelona, Bilbao y Valencia) que forman parte del holding BME (Bolsas y Mercados Españoles), que también integra otros segmentos y sistemas de negociación, compensación y liquidación de valores. Siendo, como se explica en CNMV (s. f.c), el Sistema de Interconexión Bursátil Español (SIBE) la plataforma que permite la contratación continua y electrónica de todos los valores admitidos a cotización en las cuatro bolsas españolas.\nComo expone la CNMV (s. f.a) las acciones son valores mobiliarios que representan una parte proporcional del capital social de una sociedad anónima, y sus tenedores son socios propietarios de la misma. Las acciones pueden negociarse en bolsas de valores o en otros mercados secundarios autorizados.\nA partir de lo expuesto en Mitchell (2020), Pinset (2021) y C. Team (2023) se puede concluir que, para explicar el precio de las acciones de una empresa, se pueden considerar los siguientes factores:\n\nLa oferta y la demanda de las acciones en el mercado: si hay más compradores que vendedores, el precio subirá y viceversa. Esto depende de las expectativas y la confianza de los inversores en el futuro de la empresa.\nLos cambios en la gestión o la producción de la empresa: si la empresa mejora su eficiencia, su rentabilidad o su innovación, el precio de sus acciones puede aumentar. Por el contrario, si la empresa tiene problemas internos, pierde competitividad o se ve afectada por crisis externas, el precio puede bajar.\nLa reputación de la empresa: si la empresa tiene una buena imagen pública, se asocia con éxitos o logros, o recibe buenas valoraciones de los analistas, el precio de sus acciones puede subir. Por el contrario, si la empresa se ve involucrada en escándalos, demandas o controversias, o recibe malas valoraciones de los analistas, el precio puede bajar.\n\nEn los textos Pinset (2021), T. I. Team (2022) y C. Team (2023) también señalan la importancia de diferenciar el precio de una empresa o acción de esta del valor intrínseco de esta. Pudiéndose resumir teniendo en cuenta lo señalado en esos textos y lo expuesto con anterioridad que el precio de una empresa o acción es lo que los compradores y vendedores están dispuestos a pagar por ella en un momento determinado, mientras que el valor intrínseco de una empresa o acción depende en gran medida de la metodología utilizada para valorar las empresas y los objetivos del evaluador.\nUna vez contextualizado de manera general el entorno en el que se encuentran los precios de las acciones y explicado algunos de los factores que pueden afectar a los mismos, se explica a continuación la estructura en la que estos datos aparecen habitualmente. Generalmente los precios de las acciones se encuentran registrados de forma periódica (diariamente, semanalmente, mensualmente, anualmente, etc). registrándose para cada periodo el precio de apertura, el precio más alto, el más bajo, el de cierre, el volumen y el de cierre ajustado, ver Tabla 1.\nA partir de lo expuesto en Barone (2022), Chen (2022), Downey (2022), Hayes (2021) y Ganti (2020) se puede entender que:\n\nEl precio de apertura es el primer precio al que se negocia un activo financiero en una sesión bursátil. Este precio puede ser diferente al precio de cierre de la sesión anterior, ya que puede haber cambios en la oferta y la demanda durante el periodo en que el mercado está cerrado. El precio de apertura suele indicar el tono o la tendencia del mercado para ese día.\nEl precio más alto es el mayor precio al que se negocia un activo financiero en una sesión bursátil. Este precio refleja el máximo nivel de interés de los compradores por ese activo en ese día. El precio más alto puede ser un indicador de la fortaleza o la debilidad de un activo, así como de su volatilidad.\nEl precio más bajo es el menor precio al que se negocia un activo financiero en una sesión bursátil. Este precio refleja el mínimo nivel de interés de los vendedores por ese activo en ese día. El precio más bajo puede ser un indicador de la presión o la resistencia de un activo, así como de su volatilidad.\nEl precio de cierre es el último precio al que se negocia un activo financiero en una sesión bursátil. Este precio es el que se utiliza para calcular el valor de mercado de ese activo al final del día. El precio de cierre suele ser el más importante para los inversores, ya que resume el resultado de las operaciones del día y muestra la dirección del mercado.\nEl volumen es la cantidad de unidades de un activo financiero que se negocian en una sesión bursátil. El volumen muestra el nivel de actividad o de liquidez de un mercado o de un activo. El volumen suele acompañar a los movimientos de los precios, ya que indica el grado de consenso o de divergencia entre los participantes del mercado.\nEl precio de cierre ajustado es el precio de cierre de un activo financiero que se modifica para tener en cuenta eventos como dividendos, splits, fusiones o adquisiciones que afectan al valor del activo. El precio de cierre ajustado permite comparar el rendimiento histórico de un activo con mayor precisión y consistencia.\n\nA partir de lo expuesto en Hayes (2021) y Ganti (2020) se entiende que la diferencia entre el precio de cierre y el precio de cierre ajustado es de gran importancia, ya que el primero puede dar una imagen distorsionada del rendimiento de una acción a lo largo del tiempo, mientras que el segundo refleja el valor real de la acción después de ajustar los factores que lo alteran.\n\nPor ejemplo, la junta directiva de una empresa puede decidir dividir las acciones de la empresa 3 por 1. Por lo tanto, las acciones en circulación de la empresa aumentan en un múltiplo de tres, mientras que el precio de sus acciones se divide por tres. Supongamos que una acción cerró a $300 el día anterior a su división de acciones. En este caso, el precio de cierre se ajusta a $100 ($300 divididos por 3) por acción para mantener un estándar de comparación consistente. De manera similar, todos los demás precios de cierre anteriores para esa empresa se dividirían por tres para obtener los precios de cierre ajustados. Ganti (2020)\n\nDebido a ello el precio de cierre ajustado es mejor para la aplicación de técnicas de análisis de series de tiempo, ya que permite comparar el comportamiento de una acción a lo largo del tiempo sin las distorsiones causadas por los eventos corporativos. Siendo la serie de tiempo más comúnmente utilizada en los estudios de los análisis de los precios de mercado la conformado por las rentabilidades calculadas a partir del precio de cierre ajustado.\n\n\n\n\nAnderson, D. R., D. J. Sweeney, T. A. Williams, D. J. Camm, y J. J Cochran. 2017. Statistics for business & economics. Boston: Cengage Learning.\n\n\nBarone, A. 2022. «Opening Price: Definition, Example, Trading Strategies». 2022. https://www.investopedia.com/terms/o/openingprice.asp.\n\n\nBME. s. f. «¿Qué es BME?» Accedido 24 de abril de 2023. https://www.bolsasymercados.es/esp/Sobre-BME/Que-es.\n\n\nCastillo, R. A., y R. Varela. 2010. ECONOMETRÍA PRÁCTICA: Fundamentos de Series de Tiempo. México: Universidad Autónoma de Baja California.\n\n\nChen, J. 2022. «Today’s High». 2022. https://www.investopedia.com/terms/t/todayshigh.asp.\n\n\nChirinos, S. 2018. «Series cronológicas». https://www.slideshare.net/SuedimarChirinos/series-cronologicas-119058959. 2018.\n\n\nCNMV. s. f.a. «Glosario Financiero: Acción». Accedido 24 de abril de 2023. https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&letra=A&idlng=1.\n\n\n———. s. f.b. «Glosario Financiero: Bolsa de valores». Accedido 24 de abril de 2023. https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&letra=B&idlng=1.\n\n\n———. s. f.c. «Glosario Financiero: Servicio de Interconexión Bursátil Español, SIBE». Accedido 24 de abril de 2023. https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&letra=S&idlng=1.\n\n\nDodge, Y. 2008. «Time Series». En The Concise Encyclopedia of Statistics, 536-39. New York, NY: Springer New York. https://doi.org/10.1007/978-0-387-32833-1_401.\n\n\nDowney, L. 2022. «Today’s Low». 2022. https://www.investopedia.com/terms/t/todayslow.asp.\n\n\nEspallargas, S. D., y M. V. Solís. 2012. Econometría y series temporales: aplicaciones. La Habana: Editorial Félix Varela.\n\n\nGanti, A. 2020. «Adjusted Closing Price». 2020. https://www.investopedia.com/terms/a/adjusted_closing_price.asp.\n\n\nHayes, A. 2021. «What Is Closing Price? Definition, How It’s Used, and Example». 2021. https://www.investopedia.com/terms/c/closingprice.asp.\n\n\nIBM. 2021. «Characteristics of time series». https://www.ibm.com/docs/en/spss-modeler/saas?topic=data-characteristics-time-series. 2021.\n\n\nKocenda, E., y A. Cerný. 2017. Elements of Time Series Econometrics: An Applied Approach. Prague: Karolinum Press.\n\n\nMitchell, C. 2020. «Market Price: Definition, Meaning, How To Determine, and Example». 2020. https://www.investopedia.com/terms/m/market-price.asp.\n\n\nPinset, W. 2021. «Understanding Stock Prices and Values». 2021. https://www.investopedia.com/articles/stocks/08/stock-prices-fool.asp.\n\n\nRuiz, M. C. 2011. «Tema 5: Procesos Estocásticos». http://www.dmae.upct.es/~mcruiz/Telem06/Teoria/apuntes_procesos.pdf; Departamento de Matemática y Estadística. Universidad Politécnica de Cartagena. 2011.\n\n\nTeam, CFI. 2023. «What is Stock Price?» 2023. https://corporatefinanceinstitute.com/resources/capital-markets/stock-price/.\n\n\nTeam, The Investopedia. 2022. «Intrinsic Value Defined and How It’s Determined in Investing and Business». 2022. https://www.investopedia.com/terms/i/intrinsicvalue.asp.\n\n\nVillagarcía, T. 2006. «Series Temporales». https://halweb.uc3m.es/fjnm/estind/doc_grupo1/archivos/Apuntes%20de%20series.pdf. 2006.\n\n\nVillavicencio, J. 2010. «Introducción a las series de tiempo». http://www.estadisticas.gobierno.pr/iepr/LinkClick.aspx; Instituto de estadística de Puerto Rico. 2010."
  },
  {
    "objectID": "ANNinTSF.html#antecedentes-del-uso-de-redes-neuronales-artificiales-en-la-previsión-de-series-de-tiempo",
    "href": "ANNinTSF.html#antecedentes-del-uso-de-redes-neuronales-artificiales-en-la-previsión-de-series-de-tiempo",
    "title": "2.2 Redes neuronales artificiales en la previsión de las series de tiempo",
    "section": "2.2.1 Antecedentes del uso de redes neuronales artificiales en la previsión de series de tiempo",
    "text": "2.2.1 Antecedentes del uso de redes neuronales artificiales en la previsión de series de tiempo\nEn Chollet y Allaire (2018) se plantea que el entorno de las RNA está conformado por la inteligencia artificial (en lo adelante IA), machine learning o aprendizaje automatizado (en lo adelante ML) y deep learning o aprendizaje profundo (en lo adelante DL), Figura 1. Por lo que es de vital importancia conocer los aspectos de estos campos que se encuentran íntimamente relacionados con las RNA y que se exponen brevemente a continuación.\n“Hacer que una máquina se comporte de tal manera que si un humano lo hiciera se le llamaría inteligente” (McCarthy et al. (2006), p.11) es la primera definición que se le dio al problema de IA. Con el objetivo de dar solución a este problema surgieron las primeras IA, las llamadas IA simbólicas.\nComo se explica en Haykin (1998), Banda (2014) y Chollet y Allaire (2018), estas primeras IA, involucraban reglas codificadas creadas por los programadores. Con el objetivo de lograr que estas reglas fueran aprendidas automáticamente por las máquinas al observar los datos surgió una nueva etapa dentro del desarrollo de las IA, la denominada ML. En esta nueva etapa se da pie al surgimiento de una nueva forma de programación, diferenciándose de la clásica, en que, en esta, los programadores introducen los datos y las respuestas esperadas a los mismos, y las computadoras son capaces de generar las reglas, Figura 2.\nPor lo que se entiende que los modelos de ML tratan de encontrar representaciones apropiadas para sus datos de entrada: transformaciones de los datos que hacen que sea más susceptible a la tarea en cuestión. En DL, que es un sub-campo específico de ML, estas representaciones de datos son modeladas a través de arquitecturas compuestas de capas sucesivas, las que son llamadas RNA Chollet y Allaire (2018).\nTras el estudio de lo expuesto en Haykin (1998), Larrañaga (2007), Banda (2014) y Chollet y Allaire (2018) sobre las RNA se puede afirmar que están inspiradas en el funcionamiento del cerebro humano, dichos textos confirman y concuerdan en que en una RNA se pueden diferenciar tres tipos de capas: de entrada, de salida y ocultas. Una capa de entrada está compuesta por neuronas que reciben los vectores de entradas. Una capa de salida se compone de neuronas que, durante el entrenamiento reciben los vectores de salidas y que luego generan la respuesta. Una capa oculta se encuentra conectada al entorno a través de las capas de entrada y salida, este tipo de capa oculta procesa la entrada recibida para obtener la salida correspondiente, Figura 3.\nUna de las aplicaciones de las RNA es la previsión de series temporales. cuyo objetivo es predecir los valores futuros de las variables en función de sus observaciones pasadas. Como se expuso con anterioridad las series de tiempo financieras a menudo son no lineales, ruidosas, caóticas y no estacionarias, lo que las hace difíciles de modelar y pronosticar. Las RNA tienen la ventaja de poder capturar relaciones no lineales complejas y adaptarse a condiciones cambiantes sin requerir suposiciones previas sobre la distribución o estructura de datos.\nLa historia de las RNA en la previsión de series temporales financieras se remonta a finales de la década de 1980 y principios de la de 1990, cuando los investigadores comenzaron a explorar el potencial de las RNA como una alternativa a los métodos estadísticos tradicionales, como el modelo autorregresivo integrado de media móviles, más conocido como ARIMA (por sus siglas en inglés Autoregressive Integrated Moving Average) y los modelos autorregresivos generalizados con heterocedasticidad condicional, más conocido como GARCH (por sus siglas en inglés Generalized Autoregressive Conditional Heteroskedasticity). Se demostró que las RNA tienen varias ventajas sobre estos métodos, como la capacidad de capturar relaciones no lineales y dinámicas, manejar datos ruidosos e incompletos y adaptarse a las condiciones cambiantes del mercado (B. Eddy Patuwo & Michael Y. Hu (1998)).\nSin embargo, las RNA también enfrentan algunas limitaciones y desafíos en el pronóstico de series temporales financieras, como la dificultad de elegir una arquitectura de red adecuada, un algoritmo de entrenamiento, una función de activación y variables de entrada; el riesgo de sobreajuste y problemas de generalización; la falta de interpretabilidad y transparencia; y el alto costo computacional y tiempo (Tealab (2018)).\nPara superar estas limitaciones y desafíos, los investigadores han propuesto varias mejoras y extensiones de RNA para el pronóstico de series temporales financieras en las últimas décadas. Algunos de los principales desarrollos incluyen:\n\nEl uso de modelos híbridos que combinan RNA con otras técnicas, como lógica difusa, algoritmos genéticos, análisis de ondículas, máquinas de vectores de soporte y aprendizaje profundo para mejorar el rendimiento y la solidez de las RNA (Wong y Guo (2010)).\nEl uso de redes neuronales recurrentes (en lo adelante RNR) o bidireccional, que son un tipo especial de RNA que pueden procesar datos secuenciales y capturar dependencias temporales. Se ha demostrado que las RNR superan a las redes neuronales unidireccionales en series temporales complejas y no lineales (Guresen, Kayakutlu, y Daim (2011)).\nEl uso de modelos de RNA más complejas mediante la combinación de distintas capas, como son las redes neuronales convolucionales (en lo adelante, CNN), las long short-term memory (en lo adelante, LSTM), las gated recurrent units (en lo adelante, GRU) se han aplicado a la previsión de series temporales financieras con resultados prometedores (Sezer, Gudelek, y Ozbayoglu (2020)).\n\nLa historia de las RNA en el pronóstico de series temporales financieras muestra que las mismas han ido evolucionando y mejorando con el tiempo para hacer frente a la complejidad y la incertidumbre de los mercados financieros. Sin embargo, todavía persisten algunos de los desafíos y limitaciones señalados con anterioridad como el sobreajuste, la generalización, la interpretabilidad, la robustez y el costo computacional."
  },
  {
    "objectID": "ANNinTSF.html#redes-neuronales-convolucionales",
    "href": "ANNinTSF.html#redes-neuronales-convolucionales",
    "title": "2.2 Redes neuronales artificiales en la previsión de las series de tiempo",
    "section": "2.2.2 Redes neuronales convolucionales",
    "text": "2.2.2 Redes neuronales convolucionales\nEl modelo de RNA que se usó en este trabajo está compuesto por varias capas siendo las más importantes la capa Conv1D, un tipo especifico de CNN, y la capa LSTM, ambas mencionadas en el sub-epígrafe anterior cuando se listaron las estructuras de ANN que más se utilicen en la actualidad. Este sub-epígrafe se centra en la Capa Conv1D, por lo que se exploran los conceptos fundamentales para comprender el funcionamiento de esta, explicándose la convolución, las redes neuronales convolucionales y Conv1D y su uso para el análisis de series temporales. Se brinda una descripción general de la convolución y cómo se puede aplicar a los datos de series temporales. Luego, se analizan las CNN y su arquitectura, que les permite aprender características automáticamente a partir de datos de series temporales. Finalmente, se explica Conv1D, un tipo específico de capa de red neuronal convolucional que es particularmente eficaz para procesar datos de series temporales.\nComo se expone en Siddiqui (2023) la convolución es una operación matemática que se usa comúnmente en el procesamiento de señales y el análisis de imágenes. Implica tomar dos funciones y producir una tercera función que representa cómo una de las funciones originales modifica a la otra. En el contexto de los datos de series temporales, la convolución se puede utilizar para extraer características de los datos aplicando un filtro a la serie temporal.\nAdemás de extraer características de los datos de series temporales, la convolución también se puede utilizar para otras tareas, como la reducción de ruido, la detección de anomalías y la predicción. Por ejemplo, se puede entrenar una CNN para predecir valores futuros de una serie temporal aprendiendo los patrones subyacentes en los datos. En general, la convolución es una herramienta poderosa para analizar datos de series temporales y sus aplicaciones son numerosas Siddiqui (2023).\nLas CNN fueron por primera vez introducidas en Lecun et al. (1998) son un tipo de modelo de aprendizaje profundo que se usa comúnmente para el análisis de imágenes. Sin embargo, como se ha mencionado con anterioridad también se pueden utilizar para el análisis de series temporales, ya que son muy adecuados para aprender características a partir de datos que tienen una estructura espacial o temporal.\nLa arquitectura de una CNN consta de una o más capas convolucionales, que aplican filtros a los datos de entrada para extraer características. Cada filtro es un conjunto de pesos que se aprenden durante el proceso de entrenamiento. Al deslizar el filtro sobre los datos de entrada, la capa convolucional calcula un producto escalar en cada posición, produciendo un nuevo mapa de características Lecun et al. (1998).\nEn un contexto de series de tiempo, una CNN puede aprender a extraer automáticamente características de los datos en diferentes escalas e intervalos de tiempo, lo que la convierte en una herramienta poderosa para el análisis de series de tiempo. Una ventaja clave de usar una CNN para el análisis de series de tiempo es que reduce la necesidad de ingeniería de características manual. En lugar de diseñar filtros a mano, CNN aprende a extraer automáticamente características de los datos, haciéndolo más flexible y adaptable a diferentes tipos de datos de series temporales.\nEn general, la arquitectura de una CNN le permite aprender características automáticamente a partir de datos de series temporales, lo que la convierte en una herramienta poderosa para el análisis de series temporales, siendo las Conv1D una de las estructuras de CNN más usadas para esta tarea.\nComo se explica en Jing (2020) Conv1D es un tipo específico de capa de CNN que está diseñado para procesar datos unidimensionales, como datos de series temporales. Mientras que las CNN tradicionales están diseñadas para procesar datos bidimensionales, Conv1D está optimizado específicamente para datos unidimensionales, lo que lo hace más eficiente y eficaz para el análisis de series temporales.\nLa arquitectura de una capa Conv1D es similar a la de una CNN tradicional, pero con algunas diferencias clave. En lugar de usar filtros bidimensionales, Conv1D usa filtros unidimensionales, que se aplican a la serie temporal de entrada para extraer características. Las características que se extraen de la serie dependerán de las distintas configuraciones usadas para la configuración del filtro y la cantidad de filtros utilizados, siendo la fórmula para calcular la cantidad de característica que extrae cada filtro la siguiente Ecuación 1 (Jing (2020)):\n\\[\n\\begin{aligned}\nL_{out} &= \\frac{L_{in} + 2*padding - dilation*(kerenel\\_size - 1)-1}{stride} + 1 \\\\\n\\end{aligned}\n\\tag{1}\\]\nDonde:\n\nLout: es la longitud de la salida del proceso de filtrado o la cantidad de características.\n\n\nLin: la longitud del vector de entrada, correspondiendo en el análisis de series de tiempo a la cantidad de observaciones que contienen las muestras de la serie de tiempo que se pasan al filtro.\n\n\nkernel_size: es el tamaño del filtro, lo que define cuantas observaciones del vector de entradas se pasan al filtro cada vez. Figura 4 representa como el tamaño del filtro puede afectar la longitud del vector de salida.\n\n\nstride: representa la cantidad de pasos u observaciones en las que se mueve la selección de observaciones que se pasa al filtro. Figura 5 representa como el parámetro stride puede afectar la longitud del vector de salida.\n\n\ndilation: es la distancia de las observaciones que pasan al filtro. Figura 6 representa como el parámetro dilation puede afectar la longitud del vector de salida.\n\n\npadding: representa la cantidad de ceros que se añade a cada extremo del vector. Figura 7 representa como el parámetro padding puede afectar la longitud del vector de salida.\n\nEn general, Conv1D es una herramienta poderosa para procesar datos de series temporales y sus ventajas incluyen la eficiencia computacional y la capacidad de capturar dependencias temporales en los datos. Sus casos de uso son numerosos y abarcan diferentes campos, lo que lo convierte en una herramienta valiosa para el análisis de series temporales."
  },
  {
    "objectID": "ANNinTSF.html#long-short-term-memory",
    "href": "ANNinTSF.html#long-short-term-memory",
    "title": "2.2 Redes neuronales artificiales en la previsión de las series de tiempo",
    "section": "2.2.3 Long short-term memory",
    "text": "2.2.3 Long short-term memory\nEn el presente sub-epígrafe se explica por qué las LSTM son una de las estructuras más usadas de RNA en la previsión de series de tiempo, partiendo de una breve explicación de las RNR y porque estas son de utilidad en la solución de problemas de previsión de series de tiempo, profundizando en por qué las LSTM se diferencian del resto de las RNN, y el funcionamiento de cada una de las capas que componen la estructura de una ca\nEn Olah (2015) se explica que una RNN puede considerarse como copias múltiples de la misma red, Figura 8, expone que este aspecto revela que las RNR están íntimamente relacionadas con secuencias y listas, lo que hace que este tipo de RNA sea el que se use naturalmente para el trabajo con series de tiempo.\nLas RNR convencionales presentan un problema en lo relacionado con la capacidad de retener la información, como se explica en Olah (2015), las RNN estándar se desempeñan con gran capacidad solo si, la información relevante para la situación actual es reciente, es decir donde la brecha entre la información relevante y el lugar en que se necesita es pequeña, Figura 9; expone además que a medida que crece la brecha, las RNN estándar son incapaces de acceder a la información relevante, Figura 10.\nComo se ha mencionado con anterioridad las LSTM son un tipo de RNR que puede aprender dependencias a largo plazo en datos secuenciales. Estas fueron propuestas en Hochreiter (1997) y ha sido ampliamente utilizado para diversas tareas como el modelado del lenguaje, el reconocimiento de voz, la traducción automática, la descripción de imágenes y la previsión de series de tiempo.\nLa idea principal de LSTM es introducir una celda de memoria que pueda almacenar y actualizar información durante largos pasos de tiempo. La celda de memoria está controlada por tres puertas: una puerta de entrada, una puerta de olvido y una puerta de salida. Estas puertas son redes neuronales que aprenden a regular el flujo de información dentro y fuera de la célula Figura 11.\nLa puerta de entrada decide qué cantidad de la nueva entrada agregar al estado de la celda. La puerta de olvido decide qué parte del estado de celda anterior mantener o borrar. La puerta de salida decide qué parte del estado de celda actual se va a enviar a la siguiente capa. Olah (2015) basado en lo expuesto en Hochreiter (1997), describe la operativa de las puertas en cuatro pasos:\n\nDecidir qué información se olvida del estado de la celda a través de la puerta, forget gate layer \\(f_t\\). Esta puerta ve a \\(h_{t-1}\\), estado oculto del período de tiempo anterior, y \\(x_{t}\\), entrada del instante de tiempo actual, y genera un número entre 0 (deshacerse) y 1 (mantener) para cada número en el estado de la celda \\(C_{t-1}\\), Figura 12, Ecuación 2.\n\n\\[\n\\begin{aligned}\nf_t &= \\sigma(W_f [h_{t-1}, x_t] + b_f) \\\\\n\\end{aligned}\n\\tag{2}\\]\n\nDecidir qué nueva información se almacena en el estado de la celda. Para esto primero la puerta llamada input gate layer decide qué valores actualizar y luego, una capa tanh (tangente hiperbólica) crea un vector de nuevos valores candidatos (\\(\\tilde{C}_t\\)) que podrían agregarse al estado, Figura 13, Ecuación 3 y Ecuación 4.\n\n\\[\n\\begin{aligned}\ni_t &= \\sigma(W_i [h_{t-1}, x_t] + b_i) \\\\\n\\end{aligned}\n\\tag{3}\\]\n\\[\n\\begin{aligned}\n\\tilde{C}_t &= tanh(W_c [h_{t-1}, x_t] + b_c) \\\\\n\\end{aligned}\n\\tag{4}\\]\n\nSe actualiza el estado de la celda anterior, \\(C_{t-1}\\) en el nuevo estado de la celda \\(C_{t}\\). Se multiplica el estado anterior por \\(f_{t}\\), olvidando lo necesario, luego se agrega \\(i_{t} * \\tilde{C}_{t}\\). Estos son los nuevos valores candidatos, escalados según cuánto se necesita actualizar cada valor de estado, Figura 14, Ecuación 5.\n\n\\[\n\\begin{aligned}\nC_t &= f_t * C_{t-1} + i_t * \\tilde{C}_t  \\\\\n\\end{aligned}\n\\tag{5}\\]\n\nSe genera una salida basándose en el estado de celda. Ejecutándose primero una capa sigmoidea que decide qué partes del estado de la celda es la salida; luego el estado de la celda pasa a través de una función tanh (escalando los valores entre −1 y 1) y se multiplican por la salida de la puerta, output gate, Figura 15, Ecuación 6 y Ecuación 7.\n\n\\[\n\\begin{aligned}\no_t &= \\sigma(W_o [h_{t-1}, x_t] + b_o) \\\\\n\\end{aligned}\n\\tag{6}\\] \\[\n\\begin{aligned}\nh_t &= o_t * tanh(C_t) \\\\\n\\end{aligned}\n\\tag{7}\\]\nLas LSTM pueden aprender a capturar dependencias a largo plazo ajustando los valores de la puerta a través de la propagación inversa. Por ejemplo, si una determinada entrada es relevante para una salida posterior, la puerta de entrada aprenderá a dejarla entrar, y la puerta olvidada aprenderá a conservarla en el estado de celda hasta que sea necesaria. Por el contrario, si una entrada es irrelevante u obsoleta, la puerta de entrada aprenderá a ignorarla, y la puerta olvidada aprenderá a borrarla del estado de la celda.\n\n\n\n\nB. Eddy Patuwo & Michael Y. Hu, Guoqiang Zhang &. 1998. «Forecasting with artificial neural networks:: The state of the art». International Journal of Forecasting 14 (1): 35-62. https://doi.org/https://doi.org/10.1016/S0169-2070(97)00044-7.\n\n\nBanda, Hugo. 2014. Inteligencia Artificial: Principios y Aplicaciones. Quito, Ecuador: Escuela Politécnica Nacional.\n\n\nChollet, F., y J. J. Allaire. 2018. Deep Learning with R. Manning Publications. https://books.google.es/books?id=xnIRtAEACAAJ.\n\n\nGuresen, Erkam, Gulgun Kayakutlu, y Tugrul U. Daim. 2011. «Using artificial neural network models in stock market index prediction». Expert Systems with Applications 38 (8): 10389-97. https://doi.org/https://doi.org/10.1016/j.eswa.2011.02.068.\n\n\nHaykin, Simon. 1998. Neural networks: a comprehensive foundation. Prentice Hall PTR.\n\n\nHochreiter, Jürgen, Sepp & Schmidhuber. 1997. «Long Short-Term Memory». Neural Computation 9 (8): 1735-80. https://doi.org/10.1162/neco.1997.9.8.1735.\n\n\nJing, Hong. 2020. «How Convolutional Layers Work in Deep Learning Neural Networks?» Jingles, Github Blog. 2020. https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/.\n\n\nLarrañaga, Iñaki & Moujahid, Pedro & Inza. 2007. «Tema 14. Redes Neuronales». Departamento de Ciencias de la Computaci´on e Inteligencia Artificial, Universidad del Pa´ıs Vasco–Euskal Herriko Unibertsitatea. 2007. http://www.sc.ehu.es/ccwbayes/docencia/mmcc/docs/t14-neuronales.pdf.\n\n\nLecun, Y., L. Bottou, Y. Bengio, y P. Haffner. 1998. «Gradient-based learning applied to document recognition». Proceedings of the IEEE 86 (11): 2278-2324. https://doi.org/10.1109/5.726791.\n\n\nMcCarthy, John, Marvin L. Minsky, Nathaniel Rochester, y Claude E. Shannon. 2006. «A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955». AI Magazine 27 (4): 12. https://doi.org/10.1609/aimag.v27i4.1904.\n\n\nOlah, Christopher. 2015. «Understanding LSTM networks». Colah’s blog. 2015. https://colah.github.io/posts/2015-08-Understanding-LSTMs/.\n\n\nSezer, Omer Berat, Mehmet Ugur Gudelek, y Ahmet Murat Ozbayoglu. 2020. «Financial time series forecasting with deep learning : A systematic literature review: 2005–2019». Applied Soft Computing 90: 106181. https://doi.org/https://doi.org/10.1016/j.asoc.2020.106181.\n\n\nSiddiqui, J. Rafid. 2023. «Why Convolve? Understanding Convolution and Feature Extraction in Deep Networks». Medium, Towards Data Science. 2023. https://towardsdatascience.com/why-convolve-understanding-convolution-and-feature-extraction-in-deep-networks-ee45d1fdd17c.\n\n\nTealab, Ahmed. 2018. «Time series forecasting using artificial neural networks methodologies: A systematic review». Future Computing and Informatics Journal 3 (2): 334-40. https://doi.org/https://doi.org/10.1016/j.fcij.2018.10.003.\n\n\nWong, W. K., y Z. X. Guo. 2010. «A hybrid intelligent model for medium-term sales forecasting in fashion retail supply chains using extreme learning machine and harmony search algorithm». International Journal of Production Economics 128 (2): 614-24. https://ideas.repec.org/a/eee/proeco/v128y2010i2p614-624.html."
  },
  {
    "objectID": "PC.html#problema-y-técnicas",
    "href": "PC.html#problema-y-técnicas",
    "title": "2.3 Composición de carteras",
    "section": "2.3.1 Problema y técnicas",
    "text": "2.3.1 Problema y técnicas\nComo se explica en Gunjan (2023) la optimización de cartera es el proceso de seleccionar la mejor combinación de activos para mantener en una cartera en función de objetivos predefinidos. Los objetivos pueden ser la maximización del rendimiento o la minimización del riesgo, o ambos. La optimización de la cartera implica encontrar las ponderaciones óptimas para cada activo de la cartera de manera que la cartera general cumpla con los objetivos deseados. Esto puede ser un problema desafiante debido a la gran cantidad de activos para elegir y las complejas relaciones entre ellos.\nLa optimización de la cartera es un proceso importante para los inversores, ya que les ayuda a minimizar el riesgo y maximizar el rendimiento de sus inversiones. Al seleccionar cuidadosamente los activos que mantendrán en su cartera, los inversores pueden lograr el nivel deseado de riesgo y rendimiento mientras diversifican sus inversiones para reducir el riesgo general. La optimización de la cartera es un mecanismo crucial que se utiliza para reducir el riesgo de la inversión.\nExisten diversas técnicas que se pueden utilizar para resolver el problema de optimización de cartera. En Gunjan (2023) estas técnicas se encuentran clasificadas en dos categorías: enfoques clásicos y enfoques inteligentes. A continuación, se explica de manera general algunas de las técnicas pertenecientes a cada enfoque.\nEnfoques clásicos:\n\nMedia-varianza: Esta técnica, propuesta en Markowitz y Markowitz (1967), se basa en la idea de minimizar la varianza para una determinada rentabilidad esperada o maximizar la rentabilidad esperada para una determinada varianza. Es una técnica de programación cuadrática paramétrica (en lo adelante, PQP) que se puede utilizar para resolver problemas de optimización cuadrática que surgen en la optimización de carteras (Aijun Zhang & Chun-hung Li & Agus Sudjianto (2008)). El enfoque de la varianza media supone que los inversores tienen aversión al riesgo y prefieren carteras con una varianza más baja. La técnica consiste en construir una frontera de cartera que representa el conjunto de carteras que ofrecen el rendimiento esperado más alto para un nivel de riesgo dado. A continuación, se selecciona la cartera óptima de esta frontera en función de las preferencias de riesgo del inversor.\nVarianza con asimetría: esta técnica amplía el enfoque de media-varianza teniendo en cuenta la asimetría de la distribución. Fue propuesta en Samuelson (1970) y se puede utilizar cuando la función de distribución no es de naturaleza cuadrática. La asimetría mide la asimetría de una distribución y puede proporcionar información adicional sobre los riesgos y rendimientos potenciales de una cartera. Al incorporar la asimetría en el proceso de optimización de la cartera, los inversores pueden comprender mejor los posibles riesgos a la baja y tomar decisiones más informadas.\nValor en riesgo (VaR): este enfoque estadístico mide la pérdida potencial de valor de una cartera durante un período definido para un intervalo de confianza dado. Fue introducido en la primera edición de Jorion (2007) en 1997 y requiere la determinación de tres parámetros: período de tiempo, nivel de confianza y unidad de valor en riesgo. El VaR proporciona una medida de la pérdida potencial máxima que podría ocurrir con una probabilidad dada en un horizonte de tiempo específico. Las instituciones financieras lo utilizan comúnmente para administrar su exposición al riesgo y cumplir con los requisitos reglamentarios.\nValor en riesgo condicional (CVaR): este enfoque amplía el VaR teniendo en cuenta la pérdida esperada que excede el VaR. Fue introducido en Rockafellar y Uryasev (2002) y puede manejar pérdidas extremas mediante el uso de pesos dinámicos derivados de datos históricos. CVaR proporciona una medida de la pérdida esperada que podría ocurrir más allá del umbral de VaR. También se conoce como Expected Shortfall (ES) o Tail Value-at-Risk (TVaR) y se considera una medida de riesgo más coherente que el VaR.\nDesviación media-absoluta (MAD): esta técnica se puede emplear para problemas de selección de carteras de gran escala y muy diversificados. Fue introducido en Konno y Yamazaki (1991) y penaliza tanto las desviaciones positivas como las negativas. MAD proporciona una medida de la desviación absoluta promedio de los rendimientos de la cartera de su valor medio. Se considera más sólida que las medidas basadas en la varianza, ya que es menos sensible a los valores atípicos.\nMinimax: Esta técnica utiliza la rentabilidad mínima como medida de riesgo. Fue introducido en Cai et al. (2004) y tiene ciertas ventajas cuando los rendimientos no se distribuyen normalmente. Minimax proporciona una medida del peor de los casos para una cartera al minimizar la pérdida potencial máxima que podría ocurrir. Puede ser útil para los inversores que están particularmente preocupados por los riesgos a la baja.\n\nEnfoques inteligentes:\n\nRedes bayesianas: estos modelos gráficos probabilísticos se pueden utilizar para modelar el riesgo y la rentabilidad. Fueron presentados en Shenoy y Shenoy (2000) y se pueden utilizar para visualizar la relación entre diferentes variables en un modelo. Las redes bayesianas proporcionan una forma de representar dependencias complejas entre variables utilizando gráficos acíclicos dirigidos (DAG). Se pueden usar para modelar relaciones inciertas entre variables y para hacer predicciones probabilísticas sobre eventos futuros. En el contexto de la gestión de carteras, las redes bayesianas se pueden utilizar para modelar las relaciones entre diferentes activos y hacer predicciones sobre sus rendimientos futuros en función de datos históricos y otra información relevante.\nRegresión de vectores de soporte (SVR): esta técnica de aprendizaje automático se puede utilizar para determinar la cantidad a comprar y vender. Fue introducido por Drucker et al. (1996) y tiene ciertas ventajas sobre las técnicas basadas en estadísticas, como su capacidad para aprender de datos históricos. SVR implica construir un hiperplano que separa puntos de datos con diferentes etiquetas mientras maximiza el margen entre ellos. Puede usarse para tareas de regresión donde el objetivo es predecir valores continuos en lugar de etiquetas discretas. En el contexto de la gestión de carteras, SVR se puede utilizar para predecir precios de activos futuros en función de datos históricos y otra información relevante.\nRedes neuronales artificiales: como se explicó con anterioridad estos modelos computacionales se pueden utilizar para resolver problemas computacionales y de aprendizaje complejos. En el contexto de la gestión de carteras, las redes neuronales se pueden utilizar para predecir futuros precios o rendimientos de activos en función de datos históricos y otra información relevante, que es para lo que se usan en el presente trabajo.\nAprendizaje por refuerzo: este tipo de aprendizaje automático involucra a un agente o modelo que interactúa con su entorno para aprender de sus acciones. Fue presentado en Sutton y Barto (2018) y funciona para maximizar la recompensa al agente. El aprendizaje por refuerzo implica aprender a través de interacciones de prueba y error con un entorno. El agente realiza acciones en función de su estado actual y recibe recompensas o sanciones en función de los resultados de esas acciones. Con el tiempo, el agente aprende a realizar acciones que maximicen su recompensa acumulada. En el contexto de la gestión de carteras, el aprendizaje por refuerzo se puede utilizar para desarrollar estrategias comerciales que maximicen los rendimientos mientras se gestiona el riesgo."
  },
  {
    "objectID": "PC.html#programación-cuadrática",
    "href": "PC.html#programación-cuadrática",
    "title": "2.3 Composición de carteras",
    "section": "2.3.1 Programación cuadrática",
    "text": "2.3.1 Programación cuadrática\nEn este sub-epígrafe se explica que es la programación cuadrática. Cuáles son algunas de las técnicas que existen dentro de esta disciplina de la optimización matemática. Se expone además como el problema de optimización de carteras se puede describir como un problema de cuadrática y se expone de manera breve cómo funciona una de las técnicas más usadas en esta disciplina, concretamente la denominada Dual Active Set Method, la cuál es usada en los capítulos posteriores.\nLa programación cuadrática se puede elegir entre las técnicas enumeradas en el sub-epígrafe anterior por varias razones. En primer lugar, es una técnica bien establecida que se ha utilizado ampliamente en la optimización de carteras. Puede manejar problemas de optimización complejos con múltiples restricciones y puede proporcionar una forma eficiente y efectiva de resolver el problema de optimización de cartera. Esto lo convierte en una herramienta útil para los inversores que buscan minimizar el riesgo mientras logran el nivel de rendimiento deseado. Finalmente, la programación cuadrática tiene una sólida base teórica y ha sido ampliamente estudiada en la literatura. Esto la convierte en una técnica confiable y bien entendida que se puede utilizar con confianza en la optimización de la cartera.\nExisten diversas técnicas de programación cuadrática, entre las más utilizadas se encuentran:\n\nInterior Point: Este es un método de programación lineal o no lineal que logra la optimización al pasar por el centro del sólido definido por el problema en lugar de alrededor de su superficie. Un algoritmo de programación lineal de tiempo polinómico utilizando un método de punto interior fue encontrado por Karmarkar (1984).\nActive Set: Este es un algoritmo utilizado para identificar las restricciones activas en un conjunto de restricciones de desigualdad. Las restricciones activas se expresan entonces como restricciones de igualdad, transformando así un problema restringido por la desigualdad en un subproblema más simple restringido por la igualdad. El método de conjunto activo fue introducido por primera vez en un artículo de Beale (1959) y desarrollado por Fletcher (1971) y Bunch y Kaufman (1977).\nDual Active Set: El método, como se expone en Goldfarb y Idnani (1982) y Goldfarb y Idnani (1983), es un algoritmo dual eficiente y numéricamente estable para la programación cuadrática definida positiva que aprovecha el hecho de que el mínimo sin restricciones de la función objetivo se puede usar como punto de partida.\nAugmented Lagrangian: Fue introducido independientemente en Magnus R. Hestenes (1969) y Powell (1969). Se utiliza para resolver problemas de optimización restringidos agregando un término de penalización a la función objetivo que penaliza cualquier violación de las restricciones. El término de penalización suele ser un múltiplo de una medida de infracción de restricción, como la suma de infracciones de restricción al cuadrado.\nConjugate Gradient: Este es un método iterativo para resolver sistemas de ecuaciones lineales con una matriz definida positiva simétrica. También se puede utilizar para resolver problemas de optimización sin restricciones al encontrar el mínimo de una función cuadrática. El método genera una secuencia de direcciones de búsqueda que se conjugan con respecto a la matriz que define el sistema de ecuaciones o función cuadrática. El método de gradiente conjugado fue introducido originalmente en un artículo de Magnus R. Hestenes y Stiefel (1952).\nGradient Projection: El método de proyección de gradiente fue introducido en J. B. Rosen (1960) y J. Rosen (1961). Este es un método iterativo para resolver problemas de optimización restringidos proyectando el gradiente en la región factible en cada iteración. El gradiente proyectado se utiliza entonces como dirección de búsqueda, y se realiza una búsqueda de línea a lo largo de esta dirección para encontrar una nueva iteración que satisfaga las restricciones y reduzca la función objetivo.\n\nDe las técnicas señaladas con anterioridad se seleccionó el algoritmo Dual Active Set Method (en lo adelante, DASM) que como se mencionó con anterioridad fue introducido en Goldfarb y Idnani (1982) y Goldfarb y Idnani (1983), es un algoritmo de optimización para resolver problemas de programación cuadrática. El algoritmo predice el conjunto activo de restricciones que se satisfacen con igualdad en la solución del problema. Calcula una secuencia de soluciones óptimas de problemas QP que involucran algunas de las restricciones del problema original, denominada secuencia de puntos factibles duales.\nA continuación, se presenta un ejemplo general de cómo podría funcionar el algoritmo DASM usando valores hipotéticos para un problema de optimización de cartera con 2 activos, el ejemplo fue construido a partir de lo expuesto en Goswami, Mondal, y Paruya (2012) y Walker (2014):\nBajo la suposición de que se trata de encontrar la mejor composición de una cartera en la que, por simplicidad, tenemos 2 activos, Se plantearía el problema cuadrático de la siguiente manera Ecuación 1:\n\\[\n\\begin{aligned}\nmin~~Q(\\vec{w}) &= \\vec{w}^TC\\vec{w}\\\\\nsujeto~a:\\\\\nw_{1}+w_{2}=1\\\\\n0\\leq{w_{i}}\\leq{1}\\\\\nw_{1}\\mathbb{E} + w_{2}\\mathbb{E} \\geq{0.005}\n\\end{aligned}\n\\tag{1}\\]\nSuponiendo que los cuales tienen unos rendimientos mensuales medios \\(r=\\begin{bmatrix} 0.02 & 0.03 \\end{bmatrix}\\) y matriz de covarianza \\(C=\\begin{bmatrix} 0.001 & 0.0008 \\\\ 0.0008 & 0.002 \\end{bmatrix}\\) . Se pueden construir los vectores y matrices necesarios para el algoritmo DASM de la siguiente manera:\n\nEl vector de rentabilidad media mensual sería \\(r=\\begin{bmatrix} 0.02 & 0.03 \\end{bmatrix}\\).\nLa matriz de covarianza C se usaría como la matriz D en DASM.\nLa restricción \\(w_{1}+w_{2}=1\\) se puede escribir en forma de matriz como \\(\\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} = 1\\). Esta sería la primera fila de la matriz \\(A\\) en DASM.\nEl requisito de rentabilidad mínima \\(w_{1}\\mathbb{E} + w_{2}\\mathbb{E} \\geq{0.005}\\) puede escribirse en forma de matriz como \\(\\begin{bmatrix} 0.02 & 0.03 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\geq{0.005}\\). Esta sería otra fila de la matriz \\(A\\) en DASM.\nLas restricciones \\(0\\leq{w_i}\\leq{1}\\) se pueden escribir en forma de matriz como \\(\\begin{bmatrix} 1 & 0 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\geq{0}\\) y \\(\\begin{bmatrix} 0 & 1 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\geq{0}\\) para límites inferiores y \\(\\begin{bmatrix} -1 & 0 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\geq{-1}\\) y \\(\\begin{bmatrix} 0 & -1 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\geq{-1}\\) para límites superiores.\nLa matriz \\(A\\) luciría así: \\(A=\\begin{bmatrix} 1 & 1 \\\\ 0.02 & 0.03 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ -1 & 0 \\\\ 0 & -1 \\end{bmatrix}\\)\n\nEl vector \\(b\\) correspondiente sería \\(\\begin{bmatrix} 1 & 0.005 & 0 & 0 & -1 & -1\\end{bmatrix}\\). Luego podemos usar el algoritmo DASM para resolver este problema de programación cuadrática y determinar la asignación óptima de activos en nuestra cartera.\nPaso 0: Encuentre el mínimo sin restricciones resolviendo el problema de programación cuadrática sin restricciones. Establecer el número de elementos del conjunto activo A (conjunto vacío) a cero.\nPaso 1: Elija una restricción violada, si la hay. En este caso, supongamos que se viola la restricción \\(w_{1}+w_{2}=1\\).\nPaso 2: Calcule las direcciones del paso primario y dual y la longitud del paso \\(t=min(t_{1},t_{2})\\). Supongamos que \\(t=t_{2}\\).\nPaso 3: Da un paso y actualiza el conjunto activo A y la solución (\\(S\\)) par (x, A). Como \\(t=t_{2}\\) , agregamos la p-ésima restricción (en este caso \\(w_1+w_2=1\\)) a \\(\\bar{N}\\) y actualizamos \\(H\\) y \\(N^{*}\\) en Ecuación 2.\n\\[\n\\begin{aligned}\nN^{*}=(\\bar{N}^{T}Q^{-1}\\bar{N})\\bar{N}^{T}Q^{-1}\\\\\nH=Q^{-1}(I-\\bar{N}N^{*})\n\\end{aligned}\n\\tag{2}\\]\nDonde:\n\n\\(N^{*}\\) es la pseudo-inversa o la inversa generalizada Moore-Penrose de \\(\\bar{N}\\).\n\n\n\\(\\bar{N}\\) es la matriz de los vectores normales de las restricciones en el conjunto activo \\(A\\).\n\n\n\\(H\\) es el operador hessiano inverso reducido de \\(Q\\).\n\nSe repiten estos pasos de manera iterativa hasta que se satisfagan todas las restricciones y se haya determinado la asignación óptima de activos.\n\n\n\n\nAijun Zhang & Chun-hung Li & Agus Sudjianto, Zhi-li Wu &. 2008. «Trace solution paths for SVMs via parametric quadratic programming». Researchgate. 2008. https://www.researchgate.net/publication/228577955_Trace_solution_paths_for_SVMs_via_parametric_quadratic_programming.\n\n\nBeale, EML. 1959. «On quadratic proramming». Naval Research Logistics Quarterly 6 (3): 227-43.\n\n\nBunch, James R, y Linda Kaufman. 1977. «Some stable methods for calculating inertia and solving symmetric linear systems». Mathematics of computation 31 (137): 163-79.\n\n\nCai, Xiaoqiang, Kok Lay Teo, XQ Yang, y Xun Yu Zhou. 2004. «Minimax portfolio optimization: empirical numerical study». Journal of the Operational Research Society 55 (1): 65-72.\n\n\nDrucker, Harris, Christopher Burges, Linda Kaufman, Alex Smola, y Vladimir Vapnik. 1996. «Linear support vector regression machines». Advances in neural information processing systems 9 (9): 155-61.\n\n\nFletcher, Roger. 1971. «A general quadratic programming algorithm». IMA Journal of Applied Mathematics 7 (1): 76-91.\n\n\nGoldfarb, Donald, y Ashok U. Idnani. 1982. «Dual and primal-dual methods for solving strictly convex quadratic programs». En Numerical Analysis, editado por J. P. Hennart, 226-39. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\n———. 1983. «A numerically stable dual method for solving strictly convex quadratic programs». Mathematical Programming 27: 1-33.\n\n\nGoswami, Nababithi, Supriyo K. Mondal, y Swapan Paruya. 2012. «A Comparative Study of Dual Active-Set and Primal-Dual Interior-Point Method». IFAC Proceedings Volumes 45 (15): 620-25. https://doi.org/https://doi.org/10.3182/20120710-4-SG-2026.00029.\n\n\nGunjan, Siddhartha, Abhishek & Bhattacharyya. 2023. «A brief review of portfolio optimization techniques». Artificial Intelligence Review 56 (5): 3847-86. https://doi.org/10.1007/s10462-022-10273-7.\n\n\nHestenes, Magnus R. 1969. «Multiplier and gradient methods». Journal of optimization theory and applications 4 (5): 303-20.\n\n\nHestenes, Magnus R., y Eduard Stiefel. 1952. «Methods of conjugate gradients for solving linear systems». Journal of research of the National Bureau of Standards 49: 409-35.\n\n\nJorion, Philippe. 2007. Value at risk: the new benchmark for managing financial risk. The McGraw-Hill Companies, Inc.\n\n\nKarmarkar, Narendra. 1984. «A new polynomial-time algorithm for linear programming». En Proceedings of the sixteenth annual ACM symposium on Theory of computing, 302-11.\n\n\nKonno, Hiroshi, y Hiroaki Yamazaki. 1991. «Mean-absolute deviation portfolio optimization model and its applications to Tokyo stock market». Management science 37 (5): 519-31.\n\n\nMarkowitz, Harry M, y Harry M Markowitz. 1967. Portfolio selection: efficient diversification of investments. J. Wiley.\n\n\nPowell, Michael JD. 1969. «A method for nonlinear constraints in minimization problems». Optimization, 283-98.\n\n\nRockafellar, R Tyrrell, y Stanislav Uryasev. 2002. «Conditional value-at-risk for general loss distributions». Journal of banking & finance 26 (7): 1443-71.\n\n\nRosen, JB. 1961. «The gradient projection method for nonlinear programming. Part II. Nonlinear constraints». Journal of the Society for Industrial and Applied Mathematics 9 (4): 514-32.\n\n\nRosen, Jo Bo. 1960. «The gradient projection method for nonlinear programming. Part I. Linear constraints». Journal of the society for industrial and applied mathematics 8 (1): 181-217.\n\n\nSamuelson, Paul A. 1970. «The fundamental approximation theorem of portfolio analysis in terms of means, variances and higher moments». The Review of Economic Studies 37 (4): 537-42.\n\n\nShenoy, Catherine, y Prakash P Shenoy. 2000. «Bayesian network models of portfolio risk and return». En. The MIT Press.\n\n\nSutton, Richard S, y Andrew G Barto. 2018. Reinforcement learning: An introduction. MIT press.\n\n\nWalker, Ryan. 2014. «Solving Quadratic Progams with R’s quadprog package». rwalk. 2014. https://rwalk.xyz/solving-quadratic-progams-with-rs-quadprog-package/."
  },
  {
    "objectID": "Data.html#sec-obtdat",
    "href": "Data.html#sec-obtdat",
    "title": "2.4 Datos",
    "section": "2.4.1 Obtención de Datos",
    "text": "2.4.1 Obtención de Datos\nUna explicación más detallada, en lo respecto a el código utilizado para la realización del procedimiento expuesto en el presente sub-epígrafe se encuentra en el Anexo 4 - Obtención de Datos.\nCon el objetivo de ejemplificar como las redes neuronales artificiales y la programación cuadrática pueden ser usadas en una estrategia de gestión de cartera, se decidió en el presente trabajo utilizar datos de la bolsa del mercado español. Por lo que se decidió trabajar con la información correspondiente a las empresas que se encuentran en la lista de empresas cotizadas que se expone en «Empresas cotizadas» (s. f.) y puede ver en Tabla 2.\nEn la Tabla 2 se recogen los datos de r nrow(empresas) empresas. Siendo los datos recogidos el nombre, ticker, sector y subsector, mercado, índice de cada una de las empresas y si fueron seleccionadas o no para realizar el resto del procedimiento después de la realización de los pasos expuestos en el presente sub-epígrafe.\nCon el objetivo de obtener los datos de las empresas y analizarlas para seleccionar aquellas con las que se trabajó en el resto del procedimiento se usó como fuente  (s. f.a). A continuación, se expone el proceso llevado a cabo para la obtención y selección de los datos.\nSe decidió descargar los datos mensuales de cada una de las empresas recogidas en Tabla 2. Obteniéndose todos los datos comprendidos entre el 31 de enero del 2000 al 28 de febrero del 2023 de cada una de las entidades.\nTras obtener los datos se pasó evaluar la calidad de estos. Se comenzó la evaluación con un análisis exploratorio visual de los precios ajustado ya que, como se explicó en el capítulo anterior estos son los ideales para usar en cualquier metodología de análisis históricos.\nDurante el mencionado análisis exploratorio visual, se detectó que existían irregularidades en los precios ajustados de algunas de las series. Las irregularidades detectadas consistían en el incorrecto registro de los precios ajustados, así como errores en el cálculo de estos. Estos errores se detectaron fácilmente al observar en las gráficas de los valores del precio de cierre ajustado tendencia constante en periodos largos de tiempo, como se observa en Figura 6, lo que indica un registro erróneo de las variaciones de los precios; así como cambios bruscos de hasta más de un 100% en los mismos en un solo período de tiempo, lo que puede indicar un mal cálculo en el precio ajustado, como se ve en Figura 7, en este último caso se verificó con otras fuentes como  (s. f.b), para comprobar que de echo estaban mal computados los precios.\nDado el tiempo con el que se contaba para realizar el estudio expuesto en el procedimiento y la extensa cantidad de tiempo que requeriría la investigación a realizar para sustituir los valores erróneos en las series se decidió eliminar estas irregularidades mediante el uso solo de los valores posteriores a enero del 2005, que ya no presentaban inconsistencia en el cálculo del precio ajustado, posteriormente se eliminaron aquellas series que aún contenían valores faltantes y que presentaban irregularidad en el registro de las variaciones, para esto último se eliminaron aquellas series en las que las variaciones de los precios sin registrar sea en más de 10 observaciones.\nQuedando tras los ajustes realizados r length(returns_emps3) empresas, como se ve en la columna seleccionadas de la Tabla 2, algunas de estas empresas cuentan con distintos números de observaciones, debido a que no todas existían o habían salido al mercado bursátil antes de enero del 2005.\nUna vez seleccionadas las empresas con las que se trabajó se computaron las rentabilidades de estas a partir de los precios ajustados. Además de las rentabilidades correspondientes a las empresas seleccionadas se usaron las rentabilidades del precio de cierre ajustado del IBEX 35, además se computaron otras variables que sirven como indicadores del comportamiento de las rentabilidades, y la relación de estas con las del índice, en este caso las del IBEX 35. Entre estas variables se encuentran las volatilidades de las empresas y el índice, la correlación entre los valores de las series y el IBEX, y la beta de las empresas en relación con el IBEX."
  },
  {
    "objectID": "Data.html#indicadores",
    "href": "Data.html#indicadores",
    "title": "2.4 Datos",
    "section": "2.4.2 Indicadores",
    "text": "2.4.2 Indicadores\nEn el presente sub-epígrafe se expone una breve explicación de las variables computadas para usar como variables de entradas en conjunto con los valores históricos de las rentabilidades de las empresas. Una explicación más detallada, en lo respecto a el código utilizado para la realización del procedimiento expuesto en el presente sub-epígrafe se encuentra en Anexo 4 - Indicadores.\n\n2.4.2.1 Volatilidad\nA partir de lo expuesto en Hargrave (2023) y Hayes (2023) la desviación estándar y la volatilidad son dos conceptos relacionados que miden cuánto fluctúa el precio de una acción u otro activo a lo largo del tiempo. La desviación estándar es un término estadístico que cuantifica la dispersión de un conjunto de puntos de datos alrededor de su valor medio. La volatilidad es un término financiero que describe el grado de variación en los rendimientos de un activo durante un período de tiempo determinado.\nLa desviación estándar y la volatilidad son importantes en el análisis del mercado de valores porque indican el riesgo y la incertidumbre asociados con la inversión en un activo en particular. Una desviación estándar o volatilidad alta significa que el precio del activo puede cambiar significativamente en cualquier dirección, lo que implica un mayor potencial de ganancias o pérdidas. Una desviación estándar o volatilidad baja significa que el precio del activo es relativamente estable y predecible, lo que implica un menor potencial de ganancias o pérdidas Hayes (2023).\nPara calcular la volatilidad de una acción o índice se calcula la desviación estándar de las rentabilidades. Siendo por tanto los cálculos necesarios los que se muestran a continuación en la Ecuación 1.\n\\[R_i = \\frac{P_i - P_{i-1}}{P_{i-1}}\\]\n\\[\\sigma = \\sqrt{\\frac{\\sum_{i=1}^N (R_i - \\bar{R})^2}{N} } \\tag{1}\\]\ndonde:\n\n\\(R_i\\) es la rentabilidad de la acción en el periodo \\(i\\)\n\\(P_i\\) y \\(P_{i-1}\\) son los precios de una acción en periodos de tiempo \\(i\\) e \\(i-1\\), respectivamente.\n\\(\\sigma\\) es la desviación estandar - \\(N\\) es el número de observaciones\n\\(\\bar{R}\\) es la rentabilidad media de la acción.\n\nLa desviación estándar y la volatilidad son herramientas útiles para que los inversores y analistas evalúen el equilibrio riesgo-recompensa de diferentes activos y carteras. También pueden ayudar a comparar el rendimiento de diferentes activos y carteras a lo largo del tiempo y en diferentes condiciones de mercado.\n\n\n2.4.2.2 Correlación\nComo se explica en Edwards (2022) la correlación es una medida estadística que determina cómo dos variables se mueven entre sí. En el análisis del mercado de valores, la correlación puede ayudar a comprender el comportamiento de diferentes acciones o indicadores del mercado a lo largo del tiempo. Tomando como ejemplo los datos que se usan en este trabajo, si los precios de una de las empresas seleccionadas tienden a subir y bajar junto con el IBEX 35, estos precios tienen una correlación positiva. Si al contrario los precios de la empresa tienden a subir cuando el indicador del IBEX 35 baja, tienen una correlación negativa. Un coeficiente de correlación de cero significa que no existe una relación lineal entre las variables, siendo en este caso los valores del IBEX 35 y los precios de una de las empresas determinadas.\nComo se expone en Ross (2022) la correlación entre dos variables se calcula usando la siguiente ecuación, Ecuación 2:\n\\[\\rho_{xy} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}} \\tag{2}\\]\ndonde:\n\n\\(\\rho_{xy}\\) es el coeficiente de correlación\n\\(n\\) es el número de observaciones\n\\(x_i\\) y \\(y_i\\) son los valores de las dos variables para la \\(i\\) observación\n\\(\\bar{x}\\) y \\(\\bar{y}\\) son las medias de las dos variables.\n\nComo se explica también en Edwards (2022), el coeficiente de correlación se encuentra en un rango de -1 a 1, donde -1 indica una correlación negativa perfecta, 1 indica una correlación positiva perfecta, y 0 indica que no existe correlación alguna. Pudiéndose entender que, mientras más cercano se encuentre el coeficiente de correlación tanto a -1 como a 1, más fuerte es la relación lineal entre las variables analizadas.\nComo ya se explicó con anterioridad el coeficiente de correlación, en el presente trabajo, se puede utilizar para analizar qué tan parecido se mueven las rentabilidades de una empresa en comparación con las del IBEX 35. La correlación también se puede utilizar para diversificar una cartera eligiendo acciones que tengan una correlación baja o negativa entre sí, como se explica en Boyte-White (2022). Esto puede ayudar a reducir el riesgo general de la cartera, ya que las pérdidas de una acción pueden compensarse con las ganancias de otra. Sin embargo, la correlación no es constante y puede cambiar con el tiempo debido a varios factores, como las condiciones del mercado, los eventos económicos o las noticias de la empresa. Por lo tanto, es importante monitorear la correlación de acciones regularmente y ajustar la cartera en consecuencia Boyte-White (2022).\nLa correlación es una herramienta valiosa en el análisis del mercado de valores, pero no implica causalidad. Tener una correlación alta o baja entre dos variables no implica que una variable cause cambios en la otra. La correlación simplemente mide la fuerza y dirección de la relación lineal entre dos variables, sin considerar otros factores que puedan influir en ellas.\nComo se expone también en Edwards (2022), la correlación guarda una íntima relación con la volatilidad del mercado y de las acciones, pudiéndose ver que, durante períodos de mayor volatilidad, como la crisis financiera de 2008, las acciones pueden tener una tendencia a estar más correlacionadas, incluso si se encuentran en diferentes sectores. Los mercados internacionales también pueden volverse altamente correlacionados durante tiempos de inestabilidad. Los inversores pueden querer incluir activos en sus carteras que tengan una baja correlación de mercado con los mercados de valores para ayudar a administrar su riesgo.\n\n\n2.4.2.3 Beta\nComo se explica en Kenton (2022) Beta es una medida de cuán sensibles son los rendimientos de una acción a los cambios en los rendimientos del mercado. Se calcula como la pendiente de la línea de regresión que se ajusta a los rendimientos históricos de la acción y del mercado. Una beta de 1 significa que la acción se mueve en sincronía con el mercado, una beta superior a 1 significa que la acción es más volátil que el mercado y una beta inferior a 1 significa que la acción es menos volátil que el mercado.\nBeta es importante en el análisis del mercado de valores porque, como se explica en Kenton (2022), ayuda a los inversores a evaluar el riesgo y el rendimiento de una cartera. Al conocer la versión beta de cada acción en una cartera, los inversores pueden estimar cuánto fluctuará la cartera con los movimientos del mercado y ajustar su asignación de activos en consecuencia. Por ejemplo, si un inversor quiere reducir el riesgo de su cartera, puede elegir acciones con valores beta bajos o negativos que tienden a moverse en dirección opuesta al mercado.\nComo se explica en Monaghan (2019) Beta está relacionado con la correlación, pero no son lo mismo. Como se explicó con anterioridad la correlación es una medida de cuán linealmente relacionadas están dos variables, Beta, por otro lado, es una medida de cuán fuertemente relacionadas están dos variables, lo que indica cuánto cambia una variable cuando otra variable cambia en una unidad. Beta se puede calcular a partir de la correlación usando la siguiente ecuación, Ecuación 3:\n\\[\\beta = \\frac{\\rho_{xy} \\sigma_x}{\\sigma_y} \\tag{3}\\]\ndonde:\n\n\\(\\rho_{xy}\\) es el coeficiente de correlación entre \\(x\\) y \\(y\\)\n\\(\\sigma_x\\) es la volatilidad de x\n\\(\\sigma_y\\) es la volatilidad de y."
  },
  {
    "objectID": "Data.html#vectores",
    "href": "Data.html#vectores",
    "title": "2.4 Datos",
    "section": "2.4.3 Vectores",
    "text": "2.4.3 Vectores\nEn el presente sub-epígrafe se expone el procedimiento llevado a cabo para la creación de los vectores de entrada y salida a partir de los datos resultantes del procedimiento expuesto en el sub-epígrafe anterior. Una explicación más detallada, en lo respecto a el código utilizado para la realización del procedimiento expuesto en el presente sub-epígrafe se encuentra en Anexo 4 - Vectores.\nLa estructura del conjunto de vectores de entradas y salidas es de vital importancia en el modelado de técnicas de ML teniendo un impacto importante en su eficacia. El conjunto debe de vectores debe crearse de manera representativa del problema a resolver, por lo que los pasos descritos a continuación explican en detalle los aspectos del problema a dar respuesta en el presente trabajo y como dar forma al conjunto de vectores de entradas y salidas para ello.\nComo se ha mencionado con anterioridad el objetivo del presente trabajo es exponer un procedimiento para la utilización de modelos de RNA y programación cuadrática en una estrategia de inversión. El modelado atiende a la necesidad de obtener unas predicciones lo más acertada posible para posteriormente, basándose en las predicciones y en los datos históricos, hallar la composición de cartera idónea. Por lo que el problema a representar con los conjuntos de vectores de entradas y salidas es como explicar el comportamiento de la rentabilidad de una empresa en un instante de tiempo \\(i+1\\) con los valores de varias variables en el instante de tiempo \\(i\\).\nPara la representación de este problema se crearon vectores tridimensionales, siguiendo lo expuesto en Chollet y Allaire (2018). Las dimensiones de estos vectores se explican de la siguiente manera:\n\nLa primera dimensión está comprendida por el número de muestras obtenidas al seccionar las observaciones de las distintas series en vectores bidimensionales consecutivos.\nLa segunda dimensión está comprendida por el número de observaciones, de las distintas series, recogido en cada vector bidimensional.\nLa tercera dimensión es el número de series en cada vector bidimensional.\n\nPor lo que para la correcta obtención de estas muestras se deben definir primero que series serán utilizadas para los vectores de entrada y salida. Las series utilizadas en los vectores de entrada fueron definidas en el epígrafe anterior, siendo estas: las rentabilidades históricas de la empresa y el IBEX, las volatilidades históricas de la empresa y el IBEX, la correlación histórica de la empresa y el IBEX, y el Beta histórico de la empresa y el IBEX. La serie utilizada para los vectores de salida es las rentabilidades históricas de la empresa.\nPosteriormente se definió el horizonte temporal que se desea prever, este es un aspecto clave en la creación de los conjuntos de entradas y de salidas. El número de observaciones definido como horizonte temporal determina las observaciones los vectores de salida, en el presente trabajo se determinó como horizonte temporal una observación ya que se desea predecir la rentabilidad del próximo mes de las distintas empresas seleccionadas.\nY el último aspecto a definir es cuantas observaciones debe observar el modelo para inferir la salida deseada. Esto define la cantidad de observaciones que se tomarán de cada serie de tiempo para conformar los vectores de entrada. Para determinar este aspecto se debe realizar un proceso iterativo probando distintas cantidades y evaluar los resultados obtenidos por los modelos que se entrenen con dichos. Para simplificar el proceso, en el presente trabajo se determinó probar distintos tamaños de entrada siendo estos 1, 2 y 3 observaciones. Probándose así de cierta manera como el tamaño de las entradas afecta la predicción obtenida.\nSi tenemos una de serie para los vectores de entrada contiene unas r dim(returns_indc[[1]])[1] observaciones se puede calcular el número de muestras que se obtuvo de esta serie siguiendo la siguiente ecuación, Ecuación 4:\n\\[\nm = n - (i-1+o)\n\\tag{4}\\]\ndonde:\n\n\\(m\\) el número de muestras\n\\(n\\) la cantidad de observaciones de las series\n\\(i\\) y \\(o\\) el número de observaciones en los vecotres de entrada y salida respectivamente.\n\nEn la Tabla 3 se expone la cantidad de muestras obtenidas para los distintos tamaños de vectores de entrada planteados, para lo que se tuvo en cuenta los distintos números de observaciones con las que cuentan las r length(returns_indc) seleccionadas. En la Figura 16 se expone como lucen los vectores de entrada y salida, en el caso en el que el vector de entrada cuenta con 3 observaciones.\n\n\n\n\ns. f.a. Yahoo Finance. https://finance.yahoo.com/.\n\n\n———. s. f.b. Investing. https://www.investing.com/.\n\n\nBoyte-White, C. 2022. «How Does Correlation Affect the Stock Market?» 2022. https://www.investopedia.com/ask/answers/021716/how-does-correlation-affect-stock-market.asp.\n\n\nChollet, F., y J. J. Allaire. 2018. Deep Learning with R. Manning Publications. https://books.google.es/books?id=xnIRtAEACAAJ.\n\n\nEdwards, J. 2022. «Why Market Correlation Matters?» 2022. https://www.investopedia.com/articles/financial-advisors/022516/4-reasons-why-market-correlation-matters.asp.\n\n\n«Empresas cotizadas». s. f. BME Exchange. Accedido 21 de mayo de 2023. https://www.bolsasymercados.es/bme-exchange/es/Mercados-y-Cotizaciones/Acciones/Mercado-Continuo/Empresas-Cotizadas.\n\n\nHargrave, M. 2023. «Standard Deviation Formula and Uses vs. Variance». 2023. https://www.investopedia.com/terms/s/standarddeviation.asp.\n\n\nHayes, A. 2023. «Volatility: Meaning In Finance and How it Works with Stocks». 2023. https://www.investopedia.com/terms/v/volatility.asp.\n\n\nKenton, W. 2022. «Beta: Definition, Calculation, and Explanation for Investors». 2022. https://www.investopedia.com/terms/b/beta.asp.\n\n\nMonaghan, B. 2019. «Correlation vs. Beta: What is The Difference and Why Does It Matter?» 2019. https://www.mackenzieinvestments.com/content/dam/final/corporate/mackenzie/docs/investment-teams/multi-asset-team/en/Correlation%20vs.%20Beta_%20What%20is%20The%20Difference%20and%20Why%20Does%20It%20Matter_%20_%20Mackenzie%20Investments.pdf.\n\n\nRoss, S. 2022. «How Do I Calculate Correlation Between Market Indicators and Specific Stocks?» 2022. https://www.investopedia.com/ask/answers/032315/how-do-i-calculate-correlation-between-market-indicators-and-specific-stocks.asp."
  },
  {
    "objectID": "MandT.html#sec-modelado",
    "href": "MandT.html#sec-modelado",
    "title": "2.5 Modelado y entrenamiento",
    "section": "2.5.1 Modelado",
    "text": "2.5.1 Modelado\nComo se explicó con anterioridad los principales elementos de los modelos de redes neuronales artificiales utilizados son una capa CNN y una capa LSTM. Además de esto se utilizó una capa de entrada y una capa de salida, que son las encargadas de suministrar a los modelos la información de los vectores constituidos con anterioridad. Una explicación más detallada, en lo respecto a el código utilizado para la realización del procedimiento expuesto en el presente sub-epígrafe se encuentra en Anexo 4 - Modelado.\nDado que se definieron tres tamaños distintos de observaciones para tener en cuenta para realizar una predicción fue necesario construir tres estructuras de modelos distintas que se adaptasen a las dimensiones de los distintos vectores de entrada, las distintas estructuras se pueden observar en la Figura 17.\nLa primera diferencia notable entre las estructuras son las salidas de las capas de entrada, esta diferencia se debe a los tamaños de las muestras si se ha escogido usar 1,2 o 3 observaciones para construir el modelo. Como se observa el tamaño de la salida de la capa de entrada modifica por consiguiente el tamaño de las entradas y las salidas de la capa CNN.\nComo se mencionó con anterioridad las variaciones en la segunda dimensión en las salidas de la capa CNN se pueden explicar por los distintos tamaños de los vectores de entrada. Pero como se observa el tamaño de la tercera dimensión de la salida de esta capa es igual en todas las estructuras, 64, lo que señala el número de filtros escogidos a utilizar, uno de los principales parámetros para tener en cuenta durante la configuración de estas capas. Esto último significa que las observaciones correspondientes a las 6 variables utilizadas fueron divididas en 64 variables que permiten al modelo una mejor comprensión de la relación entre las variables.\nOtro aspecto que se modificó en la capa CNN de las estructuras fue la función de activación que de manera predetermina es la denominada ReLU (por sus siglas en inglés, Rectified Linear Unit) se cambió a Leaky ReLU debido a que como se explica en OmG (2021), ReLU es una función de activación no lineal que genera cero para entradas negativas, lo que puede hacer que algunas neuronas dejen de aprender si muchas de sus entradas son negativas, ya que sus gradientes serán cero.\nDado lo explicado con anterioridad y que algunas de las variables que se usan en los valores de entradas poseen un alto número de observaciones negativas, como es el caso de las rentabilidades o la correlación de algunas de las series en ciertos periodos de tiempo el uso de la función de activación ReLU no parecía una buena opción. Por lo que se decidió utilizar como función de activación Leaky Relu que como se explica en OmG (2021), esta es una variante que permite un pequeño gradiente constante, distinto de cero, para entradas negativas. Esto significa que esta función de activación permite que algunas neuronas sigan aprendiendo de las entradas negativas.\nEn la Figura 18 se observa el dominio de la función ReLU y Leaky ReLU lo que le permitirá una mejor comprensión de lo expuesto con anterioridad.\nLa capa CNN en todas las estructuras se encuentran enlazada a una capa LSTM, la cual en todos los casos contó con 64 neuronas. La salida de esta capa se encontraba enlazada con la capa de salida la cuál devuelve un solo valor.\nPara concluir con la construcción de los modelos se determinó usar el error cuadrático medio (en lo adelante MSE, por sus siglas en inglés, Mean Squared Error) como la función utilizada para evaluar una solución candidata los resultados del modelo y el optimizador SGD (por sus siglas en inglés, Stochastic Gradient Descent) con un Alpha de 0.0005."
  },
  {
    "objectID": "MandT.html#sec-entrenamiento",
    "href": "MandT.html#sec-entrenamiento",
    "title": "2.5 Modelado y entrenamiento",
    "section": "2.5.2 Entrenamiento",
    "text": "2.5.2 Entrenamiento\nUna explicación más en detalle sobre el código utilizado durante el procedimiento expuesto en este sub-epígrafe se encuentra en Anexo 4 - Entrenamiento.\nEl entrenamiento de algoritmos de Machine Learning en la previsión de series de tiempo tienes sus peculiaridades a como se entrenan modelos con el objetivo de solucionar otro tipo de problemas. Por lo que en este sub-epígrafe se cubre de manera breve la metodología de entrenamiento utilizada, que es la denominada walk forward validation o validación de avance.\nComo ya se mencionó la validación de avance es un método utilizado para evaluar modelos de aprendizaje automático en datos de series temporales. Esto se debe a que como se explica en Brownlee (2019) proporciona la evaluación más realista de modelos de aprendizaje automático en datos de series temporales. Los métodos tradicionales de evaluación de modelos a partir del aprendizaje automático, como la validación cruzada de k-fold o la división en datos de entrenamiento y validación no funcionan en el caso de datos de series temporales porque ignoran los componentes temporales inherentes al problema. La validación Walk-forward tiene en cuenta estos componentes temporales y proporciona una evaluación más realista de cómo funcionará el modelo cuando se use operativamente.\nAl evaluar un modelo, nos interesa su rendimiento del modelo en datos que no se usaron para entrenarlo. En el aprendizaje automático, se llaman datos no vistos o fuera de la muestra. Comúnmente para la resolución de otros problemas se dividen los datos en distintos subconjuntos: entrenamiento, prueba y validación, los que tiene como objetivo entrenar y validar el modelo. Con la metodología walk forward validation los datos se dividen por periodos de tiempo y se entrena y valida al modelo de forma consecutiva lo que permite evaluar como el modelo entiende la dependencia temporal de los datos.\nAl dividir los datos por periodos de tiempo nos permite evaluar el funcionamiento real del modelo si se hubiese aplicado desde el primer periodo, así como analizar su comportamiento a lo largo de todos los periodos, observándose si su desempeño mejora o no.\nDe lo expuesto en el presente sub-epígrafe se entiende que los modelos fueron entrenados usando los conjuntos de muestras correspondientes, pasándose todas las muestras disponibles en un determinado periodo de tiempo antes de continuar con el siguiente periodo. Obteniéndose como resultado de lo anterior una predicción correspondiente a cada periodo de tiempo contemplado, a excepción de los dos primeros que se usarían para entrenar el modelo por primera vez, como se ve en el siguiente diagrama de la Figura 19.\n\n\n\n\nBrownlee, J. 2019. «How To Backtest Machine Learning Models for Time Series Forecasting». 2019. https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/.\n\n\nOmG. 2021. «Difference between ReLU, ELU and Leaky ReLU. Their pros and cons majorly». 2021. https://datascience.stackexchange.com/a/102485."
  },
  {
    "objectID": "Results.html#sec-predicciones",
    "href": "Results.html#sec-predicciones",
    "title": "2.6 Resultado",
    "section": "2.6.1 Predicciones",
    "text": "2.6.1 Predicciones\nUna explicación más en detalle sobre el código utilizado durante el procedimiento expuesto en este sub-epígrafe se encuentra en Anexo 4 - Predicciones.\nComo se explicó con anterioridad a la par que se entrenaba el modelo se obtuvieron las predicciones. Al igual que se realizó con los modelos de redes neuronales artificiales, las predicciones se computaron para las distintas observaciones usando la media aritmética de las observaciones. Se usó la media aritmética porque es una de las medidas que se usa con mayor frecuencia como indicador de posible comportamiento futuro en el estudio de las series de tiempo financieras.\nLas predicciones se evaluarán mediante el computo del \\(MSE\\) y los valores reales y el \\(R^2\\) de los resultados obtenidos por los modelos de redes neuronales artificiales y las medias aritméticas.\nComo se explica en Glen (2023) el \\(MSE\\) te dice qué tan cerca está una línea de regresión de un conjunto de puntos. Lo hace tomando las distancias desde los puntos hasta la línea de regresión (estas distancias son los “errores”) y elevándolas al cuadrado. Elevar al cuadrado es necesario para eliminar cualquier signo negativo. También da más peso a las diferencias más grandes. Se llama el error cuadrático medio ya que estás encontrando el promedio de un conjunto de errores. Cuanto menor sea el \\(MSE\\), mejor será el pronóstico, la muestra Ecuación 1 como se calcula.\n\\[\n\\begin{aligned}\nMSE &= \\frac{1}{n} * \\sum_{i=1}^{n}{(Y_i-\\hat{Y_i})^2} \\\\\n\\end{aligned}\n\\tag{1}\\]\nDonde:\n\n\\(n\\): número de observaciones\n\n\n\\(Y_i\\): valor real\n\n\n\\(\\hat{Y_i}\\): valor esperado\n\nComo se explica en Nandakumar (2020) \\(R^2\\) se utiliza comúnmente para explicar que tan bien lo hace un modelo en comparación con la media total de las observaciones, Ecuación 2:\n\\[\n\\begin{aligned}\nR^2 &= 1-\\frac{SSR}{SST}\\\\\nR^2 &= 1-\\frac{\\sum_{i=1}^{n}{(Y_i-\\hat{Y_i})^2}}{\\sum_{i=1}^{n}{(Y_i-\\tilde{Y})^2}}\\\\\n\\end{aligned}\n\\tag{2}\\]\nDonde:\n\n\\(\\tilde{Y}\\): media aritmética de todas las observaciones\n\nPero este puede ser un indicador injusto del desempeño de un modelo de regresión ya que se asume que se conocen todas las observaciones sobre las que se computa una media, y como ya se mencionó con anterioridad este no es el caso para los modelos de redes neuronales artificiales entrenados usando la metodología de walk forward validation. Debido a esto se modificó el cálculo del \\(R^2\\), como se ha hecho en otras investigaciones como Gu, Kelly, y Xiu (2018), para que el modelo con el que se comparan los resultados obtenidos por las RNA utilizadas sea el comprendido por las medias aritméticas de las observaciones anteriores a la que se predice.\nA continuación, se describirán de manera breve los distintos resultados obtenidos por los distintos modelos construidos. Cabe destacar que, aunque se plantearon 3 modelos diversos de cada uno de ellos se construyeron 10, con el objetivo de estandarizar los resultados obtenidos, ya que el proceso de construcción y entrenamiento de redes neuronales contiene un factor aleatorio. Por lo que los resultados descritos a continuación son los resultados medios obtenidos por los diversos modelos construidos.\n\n2.6.1.1 Una observación\nLos resultados obtenidos por aquellos modelos que fueron entrenados con vectores de entrada que contaban con una observación de cada serie mostraron, como se ve en la Figura 8, que en los primeros periodos los modelos presentaron mejores predicciones que las obtenidas por la media aritmética. Se puede observar que la efectividad de los modelos en comparación con las medias decae según el modelo va avanzando en el tiempo y aprendiendo de las nuevas observaciones. Se ve claramente también que en la mayoría de los periodos el \\(R^2\\) de este modelo es negativo. Además, se observa un pico en el \\(MSE\\) del modelo a principios del 2020, lo que se entiende como una pérdida de efectividad del modelo, esta pérdida de efectividad del modelo pudiese estar relacionados con los movimientos bruscos de mercado consecuentes de las afectaciones económicas de la Covid-19.\nEl análisis anterior del comportamiento de los indicadores de estos modelos por periodo nos da una visión general de como se desempeñaron estos modelos, pero dado que para la composición de cartera es fundamental los resultados obtenidos en las empresas, a continuación, analizaremos el comportamiento observado en los resultados obtenidos por las 20 empresas que presentaron los mejores y peores resultados, basándose como criterio el \\(R^2\\) obtenido.\nObservando los resultados de los indicadores expuestos en la Tabla 4 aquellas empresas que presentaron un peor \\(R^2\\) presentan también un \\(MSE\\) bajo lo que indica que es menos probable que la composición de cartera se vea alterada por los resultados obtenidos por estas empresas. Por otro lado, entre las empresas que obtuvieron un mejor \\(R^2\\) se encuentran algunas que obtuvieron un \\(MSE\\) alto acompañado de un \\(R^2\\) mayor de un 5%. Esto indica que se pudieran generar diferencias entre las composiciones de las carteras debido a las diferencias en las predicciones y que además se trata de empresas que no tienen un buen \\(MSE\\).\nLos resultados descritos en el párrafo anterior son similares para los casos de los modelos construidos con dos y tres observaciones respectivamente.\n\n\n2.6.1.2 Dos observaciones\nLos resultados obtenidos por aquellos modelos que fueron entrenados con vectores de entrada que contaban con dos observaciones de cada serie se encontró, como se ve en la Figura 9, que en los primeros periodos los modelos presentaron mejores predicciones que las obtenidas por la media aritmética. Se puede observar que la efectividad de los modelos en comparación con las medias decae según el modelo va avanzando en el tiempo, pero decaen a un ritmo más lento que aquellos modelos entrenados con vectores de entrada con una observación. Se ve claramente también que el \\(R^2\\) de estos modelos tiene una menor variación que el \\(R^2\\) de los modelos analizados con anterioridad, viéndose que para estos modelos el \\(R^2\\) es positivo en la mayoría de los periodos. Además, al igual que en el caso de los modelos analizados con anterioridad también se observa un pico en el \\(MSE\\) del modelo a principios del 2020.\n\n\n2.6.1.3 Tres observaciones\nLos resultados obtenidos por aquellos modelos que fueron entrenados con vectores de entrada que contaban con tres observaciones de cada serie se encontró, como se ve en la Figura 10, que en los primeros periodos los modelos presentaron mejores predicciones que las obtenidas por la media aritmética. Se puede observar que la efectividad de los modelos en comparación con las medias decae según el modelo va avanzando en el tiempo, pero decaen a un ritmo más lento que aquellos modelos entrenados con vectores de entrada con una observación. Se ve claramente que el \\(R^2\\) de estos modelos tiene una mayor variación que el \\(R^2\\) de los modelos analizados con anterioridad, observándose como esta variación disminuye para aquellas predicciones después del 2015. Estos modelos, al igual que los primeros presentaron un \\(R^2\\) negativo en la mayoría de los periodos. Además, al igual que en los casos anteriores también se observa un pico en el \\(MSE\\) del modelo a principios del 2020."
  },
  {
    "objectID": "Results.html#sec-cc",
    "href": "Results.html#sec-cc",
    "title": "2.6 Resultado",
    "section": "2.6.2 Composición de carteras",
    "text": "2.6.2 Composición de carteras\nUna explicación más en detalle sobre el código utilizado durante el procedimiento expuesto en este sub-epigrafe se encuentra en Anexo 4 - Composición de carteras.\nEn el presente sub-epígrafe se describen los resultados obtenidos tras aplicar programación cuadrática para determinar la composición de la cartera. Esto al igual que las predicciones se realizó periodo a periodo con el objetivo de emular una situación real en la que se aplicaran las técnicas en su conjunto. Por lo que el presente análisis se enfoca en el comportamiento observado al usar los diversos modelos y la comparación de estos resultados con los obtenidos con el uso de las medias.\nComo se observa en la Figura 11 las carteras conformadas a partir de las predicciones obtenidas por los modelos de redes neuronales que contaban con una observación obtuvieron en lo general mejores resultados que las carteras compuestas a partir de las predicciones usando la media. Se observa que ambos grupos de cartera presentaron una rentabilidad menor que el índice, IBEX, en el periodo comprendido entre el 2009 y el 2016.\nAl realizar el análisis del comportamiento de las rentabilidades obtenidas por los modelos con dos observaciones de entrada, Figura 12, se observa: el comportamiento de las rentabilidades obtenidas por los distintos modelos varía menos que los analizados con anterioridad; en este caso y contrario al caso anterior las rentabilidades se mantienen similares en el periodo comprendido entre 2009 y 2016; y aunque el resultado final dista del resultado obtenido por las medias, es inferior al obtenido por los modelos anteriores, esto último se debe a que la evaluación de los modelos en este caso comienza en un periodo anterior a las de los modelos analizados con anterioridad.\nAl observar los resultados obtenidos por los últimos modelos, Figura 13, se observa: una distribución de las rentabilidades superior a aquellos entrenados con dos observaciones pero inferior a los que se entrenaron con una observación; se observa que las rentabilidades comienzan a superar las del índice después del 2013 en lugar del 2016 como en años anteriores; y además se observa que las rentabilidades de los modelos de RNA son superiores a los de las medias y constituyen también las rentabilidades máximas obtenidas entre las distintas estructuras de modelos de RNA.\n\n\n\n\nGlen, S. 2023. «Mean Squared Error: Definition and Example». 2023. https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-squared-error/.\n\n\nGu, Shihao, Bryan Kelly, y Dacheng Xiu. 2018. «Empirical Asset Pricing via Machine Learning». Working Paper 25398. Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w25398.\n\n\nNandakumar, S. 2020. «How can r-squared be negative when the correlation between prediction and truth is positive?» 2020. https://stackoverflow.com/a/63311778/12660035."
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "3 Conclusiones",
    "section": "",
    "text": "Durante el desarrollo de este trabajo, se ha abordado la aplicación de redes neuronales artificiales y programación cuadrática en la gestión de carteras financieras. A través de una cuidadosa caracterización de las series temporales financieras, se pudo comprender la importancia de analizar sus características y patrones para realizar pronósticos más precisos.\nSe probaron distintas configuraciones de modelos de redes neuronales artificiales, conformadas por la combinación de capas convolucionales y LSTM, que diferían en la cantidad de observaciones históricas que usarían como entradas antes de realizarse una predicción. Las predicciones obtenidas a partir de los modelos antes mencionados se compararon con predicciones obtenidas mediante el uso de la media aritmética, que es uno de los indicadores que más comúnmente se utiliza. Como resultado de la comparación antes expuestas se obtuvo que los modelos en dependencia de la cantidad de observaciones que usaron como entradas: 1, 2 o 3; obtuvieron un R2 de: -0.00287, 0.0611 y 0.0179 respectivamente.\nLas predicciones obtenidas, tanto con los modelos de RNA como con las medias aritméticas en conjunto con los comportamientos históricos fueron usados para, mediante el uso programación cuadrática, buscar la composición de carteras de menor riesgo. Tras la realización de una simulación de gestión de carteras se obtuvo que las carteras conformadas a partir de las predicciones de los modelos de RNA obtuvieron al final del periodo estudiado en comparación con las conformadas por las predicciones usando la media aritmética unas rentabilidades: 5.63% superior, para los modelos que usaron 1 observación como entrada; un 35.67% superior para los que usaron 2; y un 25.51% para los que usaron 3. Además, se observó que las carteras conformadas con los modelos de RNA obtuvieron unas rentabilidades superiores al índice, IBEX, en un 40.86%, 39.78% y 60.54%, para los modelos que usaron 1, 2 y 3 observaciones como entradas respectivamente.\nLos resultados antes expuestos demuestran que el uso combinado de estas herramientas, las RNA y la programación cuadrática, pueden ofrecer una ventaja competitiva significativa en la gestión de sus activos financieros a empresas y organizaciones, permitiendo una toma de decisiones más efectiva, optimizando la composición de carteras y maximizando los rendimientos.\nNo obstante, es importante destacar que los resultados expuestos en el presente trabajo necesitan un estudio más profundo para analizar, entre otros aspectos, el peso que tienen los resultados de las predicciones de las distintas empresas en la composición de carteras. Por esta razón se considera el presente trabajo como el comienzo de una investigación más exhaustiva en la cual: se deben obtener datos de mayor calidad y se contrastará el uso de diversas técnicas, tanto para la obtención de las predicciones como para hallar la composición de cartera adecuada."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliografía",
    "section": "",
    "text": "n.d.b. Yahoo Finance. https://finance.yahoo.com/.\n\n\n———. n.d.a. Investing. https://www.investing.com/.\n\n\nAijun Zhang & Chun-hung Li & Agus Sudjianto, Zhi-li Wu &.\n2008. “Trace Solution Paths for SVMs via Parametric Quadratic\nProgramming.” Researchgate. 2008. https://www.researchgate.net/publication/228577955_Trace_solution_paths_for_SVMs_via_parametric_quadratic_programming.\n\n\nAnderson, D. R., D. J. Sweeney, T. A. Williams, D. J. Camm, and J. J\nCochran. 2017. Statistics for Business & Economics. Boston:\nCengage Learning.\n\n\nB. Eddy Patuwo & Michael Y. Hu, Guoqiang Zhang &. 1998.\n“Forecasting with Artificial Neural Networks:: The State of the\nArt.” International Journal of Forecasting 14 (1):\n35–62. https://doi.org/https://doi.org/10.1016/S0169-2070(97)00044-7.\n\n\nBanda, Hugo. 2014. Inteligencia Artificial: Principios y\nAplicaciones. Quito, Ecuador: Escuela Politécnica Nacional.\n\n\nBarone, A. 2022. “Opening Price: Definition, Example, Trading\nStrategies.” 2022. https://www.investopedia.com/terms/o/openingprice.asp.\n\n\nBeale, EML. 1959. “On Quadratic Proramming.” Naval\nResearch Logistics Quarterly 6 (3): 227–43.\n\n\nBerwin A. Turlach R port by Andreas Weingessel\n&lt;Andreas.Weingessel@ci.tuwien.ac.at&gt; Fortran contributions from\nCleve Moler dpodi/LINPACK), S original by. 2019. Quadprog: Functions\nto Solve Quadratic Programming Problems. https://CRAN.R-project.org/package=quadprog.\n\n\nBME. n.d. “¿Qué Es BME?” Accessed April 24, 2023. https://www.bolsasymercados.es/esp/Sobre-BME/Que-es.\n\n\nBoyte-White, C. 2022. “How Does Correlation Affect the Stock\nMarket?” 2022. https://www.investopedia.com/ask/answers/021716/how-does-correlation-affect-stock-market.asp.\n\n\nBrownlee, J. 2019. “How to Backtest Machine Learning Models for\nTime Series Forecasting.” 2019. https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/.\n\n\nBunch, James R, and Linda Kaufman. 1977. “Some Stable Methods for\nCalculating Inertia and Solving Symmetric Linear Systems.”\nMathematics of Computation 31 (137): 163–79.\n\n\nCai, Xiaoqiang, Kok Lay Teo, XQ Yang, and Xun Yu Zhou. 2004.\n“Minimax Portfolio Optimization: Empirical Numerical\nStudy.” Journal of the Operational Research Society 55\n(1): 65–72.\n\n\nCastillo, R. A., and R. Varela. 2010. ECONOMETRÍA PRÁCTICA:\nFundamentos de Series de Tiempo. México: Universidad Autónoma de\nBaja California.\n\n\nChen, J. 2022. “Today’s High.” 2022. https://www.investopedia.com/terms/t/todayshigh.asp.\n\n\nChirinos, S. 2018. “Series Cronológicas.” https://www.slideshare.net/SuedimarChirinos/series-cronologicas-119058959.\n2018.\n\n\nChollet, F., and J. J. Allaire. 2018. Deep Learning with r.\nManning Publications. https://books.google.es/books?id=xnIRtAEACAAJ.\n\n\nCNMV. n.d.a. “Glosario Financiero: Acción.” Accessed April\n24, 2023. https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&letra=A&idlng=1.\n\n\n———. n.d.b. “Glosario Financiero: Bolsa de Valores.”\nAccessed April 24, 2023. https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&letra=B&idlng=1.\n\n\n———. n.d.c. “Glosario Financiero: Servicio de Interconexión\nBursátil Español, SIBE.” Accessed April 24, 2023. https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&letra=S&idlng=1.\n\n\nDodge, Y. 2008. “Time Series.” In The Concise\nEncyclopedia of Statistics, 536–39. New York, NY: Springer New\nYork. https://doi.org/10.1007/978-0-387-32833-1_401.\n\n\nDowney, L. 2022. “Today’s Low.” 2022. https://www.investopedia.com/terms/t/todayslow.asp.\n\n\nDrucker, Harris, Christopher Burges, Linda Kaufman, Alex Smola, and\nVladimir Vapnik. 1996. “Linear Support Vector Regression\nMachines.” Advances in Neural Information Processing\nSystems 9 (9): 155–61.\n\n\nEdwards, J. 2022. “Why Market Correlation Matters?” 2022.\nhttps://www.investopedia.com/articles/financial-advisors/022516/4-reasons-why-market-correlation-matters.asp.\n\n\n“Empresas Cotizadas.” n.d. BME Exchange. Accessed May 21,\n2023. https://www.bolsasymercados.es/bme-exchange/es/Mercados-y-Cotizaciones/Acciones/Mercado-Continuo/Empresas-Cotizadas.\n\n\nEspallargas, S. D., and M. V. Solís. 2012. Econometría y Series\nTemporales: Aplicaciones. La Habana: Editorial Félix Varela.\n\n\nFletcher, Roger. 1971. “A General Quadratic Programming\nAlgorithm.” IMA Journal of Applied Mathematics 7 (1):\n76–91.\n\n\nGanti, A. 2020. “Adjusted Closing Price.” 2020. https://www.investopedia.com/terms/a/adjusted_closing_price.asp.\n\n\nGlen, S. 2023. “Mean Squared Error: Definition and\nExample.” 2023. https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-squared-error/.\n\n\nGoldfarb, Donald, and Ashok U. Idnani. 1982. “Dual and Primal-Dual\nMethods for Solving Strictly Convex Quadratic Programs.” In\nNumerical Analysis, edited by J. P. Hennart, 226–39. Berlin,\nHeidelberg: Springer Berlin Heidelberg.\n\n\n———. 1983. “A Numerically Stable Dual Method for Solving Strictly\nConvex Quadratic Programs.” Mathematical Programming 27:\n1–33.\n\n\nGoswami, Nababithi, Supriyo K. Mondal, and Swapan Paruya. 2012. “A\nComparative Study of Dual Active-Set and Primal-Dual Interior-Point\nMethod.” IFAC Proceedings Volumes 45 (15): 620–25.\nhttps://doi.org/https://doi.org/10.3182/20120710-4-SG-2026.00029.\n\n\nGu, Shihao, Bryan Kelly, and Dacheng Xiu. 2018. “Empirical Asset\nPricing via Machine Learning.” Working Paper 25398. Working Paper\nSeries. National Bureau of Economic Research. https://doi.org/10.3386/w25398.\n\n\nGunjan, Siddhartha, Abhishek & Bhattacharyya. 2023. “A brief review of portfolio optimization\ntechniques.” Artificial Intelligence Review 56\n(5): 3847–86. https://doi.org/10.1007/s10462-022-10273-7.\n\n\nGuresen, Erkam, Gulgun Kayakutlu, and Tugrul U. Daim. 2011. “Using\nArtificial Neural Network Models in Stock Market Index\nPrediction.” Expert Systems with Applications 38 (8):\n10389–97. https://doi.org/https://doi.org/10.1016/j.eswa.2011.02.068.\n\n\nHargrave, M. 2023. “Standard Deviation Formula and Uses Vs.\nVariance.” 2023. https://www.investopedia.com/terms/s/standarddeviation.asp.\n\n\nHayes, A. 2021. “What Is Closing Price? Definition, How It’s Used,\nand Example.” 2021. https://www.investopedia.com/terms/c/closingprice.asp.\n\n\n———. 2023. “Volatility: Meaning in Finance and How It Works with\nStocks.” 2023. https://www.investopedia.com/terms/v/volatility.asp.\n\n\nHaykin, Simon. 1998. Neural Networks: A Comprehensive\nFoundation. Prentice Hall PTR.\n\n\nHestenes, Magnus R. 1969. “Multiplier and Gradient\nMethods.” Journal of Optimization Theory and\nApplications 4 (5): 303–20.\n\n\nHestenes, Magnus R., and Eduard Stiefel. 1952. “Methods of\nConjugate Gradients for Solving Linear Systems.” Journal of\nResearch of the National Bureau of Standards 49: 409–35.\n\n\nHochreiter, Jürgen, Sepp & Schmidhuber. 1997. “Long\nShort-Term Memory.” Neural Computation 9 (8):\n1735–80. https://doi.org/10.1162/neco.1997.9.8.1735.\n\n\nIannone, Richard. 2023. DiagrammeR: Graph/Network\nVisualization. https://CRAN.R-project.org/package=DiagrammeR.\n\n\nIBM. 2021. “Characteristics of Time Series.” https://www.ibm.com/docs/en/spss-modeler/saas?topic=data-characteristics-time-series.\n2021.\n\n\nJing, Hong. 2020. “How Convolutional Layers Work in Deep Learning\nNeural Networks?” Jingles, Github Blog. 2020. https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/.\n\n\nJorion, Philippe. 2007. Value at Risk: The New Benchmark for\nManaging Financial Risk. The McGraw-Hill Companies, Inc.\n\n\nKarmarkar, Narendra. 1984. “A New Polynomial-Time Algorithm for\nLinear Programming.” In Proceedings of the Sixteenth Annual\nACM Symposium on Theory of Computing, 302–11.\n\n\nKenton, W. 2022. “Beta: Definition, Calculation, and Explanation\nfor Investors.” 2022. https://www.investopedia.com/terms/b/beta.asp.\n\n\nKocenda, E., and A. Cerný. 2017. Elements of Time Series\nEconometrics: An Applied Approach. Prague: Karolinum Press.\n\n\nKonno, Hiroshi, and Hiroaki Yamazaki. 1991. “Mean-Absolute\nDeviation Portfolio Optimization Model and Its Applications to Tokyo\nStock Market.” Management Science 37 (5): 519–31.\n\n\nLarrañaga, Iñaki & Moujahid, Pedro & Inza. 2007. “Tema 14.\nRedes Neuronales.” Departamento de Ciencias de la Computaci´on e\nInteligencia Artificial, Universidad del Pa´ıs Vasco–Euskal Herriko\nUnibertsitatea. 2007. http://www.sc.ehu.es/ccwbayes/docencia/mmcc/docs/t14-neuronales.pdf.\n\n\nLecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998.\n“Gradient-Based Learning Applied to Document Recognition.”\nProceedings of the IEEE 86 (11): 2278–2324. https://doi.org/10.1109/5.726791.\n\n\nMarkowitz, Harry M, and Harry M Markowitz. 1967. Portfolio\nSelection: Efficient Diversification of Investments. J. Wiley.\n\n\nMcCarthy, John, Marvin L. Minsky, Nathaniel Rochester, and Claude E.\nShannon. 2006. “A Proposal for the Dartmouth Summer Research\nProject on Artificial Intelligence, August 31, 1955.” AI\nMagazine 27 (4): 12. https://doi.org/10.1609/aimag.v27i4.1904.\n\n\nMitchell, C. 2020. “Market Price: Definition, Meaning, How to\nDetermine, and Example.” 2020. https://www.investopedia.com/terms/m/market-price.asp.\n\n\nMonaghan, B. 2019. “Correlation Vs. Beta: What Is the Difference\nand Why Does It Matter?” 2019. https://www.mackenzieinvestments.com/content/dam/final/corporate/mackenzie/docs/investment-teams/multi-asset-team/en/Correlation%20vs.%20Beta_%20What%20is%20The%20Difference%20and%20Why%20Does%20It%20Matter_%20_%20Mackenzie%20Investments.pdf.\n\n\nNandakumar, S. 2020. “How Can r-Squared Be Negative When the\nCorrelation Between Prediction and Truth Is Positive?” 2020. https://stackoverflow.com/a/63311778/12660035.\n\n\nOlah, Christopher. 2015. “Understanding LSTM Networks.”\nColah’s blog. 2015. https://colah.github.io/posts/2015-08-Understanding-LSTMs/.\n\n\nOmG. 2021. “Difference Between ReLU, ELU and Leaky ReLU. Their\nPros and Cons Majorly.” 2021. https://datascience.stackexchange.com/a/102485.\n\n\nPinset, W. 2021. “Understanding Stock Prices and Values.”\n2021. https://www.investopedia.com/articles/stocks/08/stock-prices-fool.asp.\n\n\nPowell, Michael JD. 1969. “A Method for Nonlinear Constraints in\nMinimization Problems.” Optimization, 283–98.\n\n\nRallabandi, S. 2023. “Activation Functions ReLU Vs. Leaky\nReLU.” 2023. https://medium.com/mlearning-ai/activation-functions-relu-vs-leaky-relu-b8272dc0b1be.\n\n\nRockafellar, R Tyrrell, and Stanislav Uryasev. 2002. “Conditional\nValue-at-Risk for General Loss Distributions.” Journal of\nBanking & Finance 26 (7): 1443–71.\n\n\nRosen, JB. 1961. “The Gradient Projection Method for Nonlinear\nProgramming. Part II. Nonlinear Constraints.” Journal of the\nSociety for Industrial and Applied Mathematics 9 (4): 514–32.\n\n\nRosen, Jo Bo. 1960. “The Gradient Projection Method for Nonlinear\nProgramming. Part i. Linear Constraints.” Journal of the\nSociety for Industrial and Applied Mathematics 8 (1): 181–217.\n\n\nRoss, S. 2022. “How Do i Calculate Correlation Between Market\nIndicators and Specific Stocks?” 2022. https://www.investopedia.com/ask/answers/032315/how-do-i-calculate-correlation-between-market-indicators-and-specific-stocks.asp.\n\n\nRuiz, M. C. 2011. “Tema 5: Procesos Estocásticos.” http://www.dmae.upct.es/~mcruiz/Telem06/Teoria/apuntes_procesos.pdf;\nDepartamento de Matemática y Estadística. Universidad Politécnica de\nCartagena. 2011.\n\n\nRyan, Jeffrey A., and Joshua M. Ulrich. 2023. Quantmod: Quantitative\nFinancial Modelling Framework. https://CRAN.R-project.org/package=quantmod.\n\n\nSamuelson, Paul A. 1970. “The Fundamental Approximation Theorem of\nPortfolio Analysis in Terms of Means, Variances and Higher\nMoments.” The Review of Economic Studies 37 (4): 537–42.\n\n\nSezer, Omer Berat, Mehmet Ugur Gudelek, and Ahmet Murat Ozbayoglu. 2020.\n“Financial Time Series Forecasting with Deep Learning : A\nSystematic Literature Review: 2005–2019.” Applied Soft\nComputing 90: 106181. https://doi.org/https://doi.org/10.1016/j.asoc.2020.106181.\n\n\nShenoy, Catherine, and Prakash P Shenoy. 2000. “Bayesian Network\nModels of Portfolio Risk and Return.” In. The MIT Press.\n\n\nSiddiqui, J. Rafid. 2023. “Why Convolve? Understanding Convolution\nand Feature Extraction in Deep Networks.” Medium, Towards Data\nScience. 2023. https://towardsdatascience.com/why-convolve-understanding-convolution-and-feature-extraction-in-deep-networks-ee45d1fdd17c.\n\n\nSutton, Richard S, and Andrew G Barto. 2018. Reinforcement Learning:\nAn Introduction. MIT press.\n\n\nTealab, Ahmed. 2018. “Time Series Forecasting Using Artificial\nNeural Networks Methodologies: A Systematic Review.” Future\nComputing and Informatics Journal 3 (2): 334–40. https://doi.org/https://doi.org/10.1016/j.fcij.2018.10.003.\n\n\nTeam, CFI. 2023. “What Is Stock Price?” 2023. https://corporatefinanceinstitute.com/resources/capital-markets/stock-price/.\n\n\nTeam, The Investopedia. 2022. “Intrinsic Value Defined and How\nIt’s Determined in Investing and Business.” 2022. https://www.investopedia.com/terms/i/intrinsicvalue.asp.\n\n\nVillagarcía, T. 2006. “Series Temporales.” https://halweb.uc3m.es/fjnm/estind/doc_grupo1/archivos/Apuntes%20de%20series.pdf.\n2006.\n\n\nVillavicencio, J. 2010. “Introducción a Las Series de\nTiempo.” http://www.estadisticas.gobierno.pr/iepr/LinkClick.aspx;\nInstituto de estadística de Puerto Rico. 2010.\n\n\nWalker, Ryan. 2014. “Solving Quadratic Progams with r’s Quadprog\nPackage.” rwalk. 2014. https://rwalk.xyz/solving-quadratic-progams-with-rs-quadprog-package/.\n\n\nWong, W. K., and Z. X. Guo. 2010. “A hybrid\nintelligent model for medium-term sales forecasting in fashion retail\nsupply chains using extreme learning machine and harmony search\nalgorithm.” International Journal of Production\nEconomics 128 (2): 614–24. https://ideas.repec.org/a/eee/proeco/v128y2010i2p614-624.html."
  },
  {
    "objectID": "Annex1.html",
    "href": "Annex1.html",
    "title": "Anexo. 1 Figuras",
    "section": "",
    "text": "Figura 1: Relación entre IA-ML-DL\n\n\n\n\nTomada de: Deep learning with R de Chollet y Allaire (2018).\n\n\n\nFigura 2: Programación clásica y machine learning\n\n\n\n\nTomada de: Deep learning with R de Chollet y Allaire (2018).\n\n\n\nFigura 3: Estructura básica de una red neuronal artificial\n\n\n\n\nTomada de: Tema 14: redes neuronales de Larrañaga (2007).\n\n\n\n\n\nFigura 4: Como el tamaño del filtro afecta el vector de salida\n\n\n\n\n\n\nElaboración propia: Elaborada a partir de Jing (2020). Muestra como el tamaño del vector de salida cambia según el tamaño de filtro que se usa.\n\n\n\n\n\nFigura 5: Como stride afecta el vector de salida\n\n\n\n\n\n\nElaboración propia: Elaborada a partir de Jing (2020). Muestra como el parámetro stride afecta el tamaño del vector de salida.\n\n\n\n\n\nFigura 6: Como dilation afecta el vector de salida\n\n\n\n\n\n\nElaboración propia: Elaborada a partir de Jing (2020). Muestra como el parámetro dilation afecta el tamaño del vector de salida.\n\n\n\n\n\nFigura 7: Como padding afecta el vector de salida\n\n\n\n\n\n\nElaboración propia: Elaborada a partir de Jing (2020). Muestra como el parámetro padding afecta el tamaño del vector de salida.\n\n\n\nFigura 8: Despliegue del bucle de una red neuronal recurrente estándar\n\n\n\n\nTomada de: Understanding LSTM networks, Olah (2015).\n\n\n\nFigura 9: Información relevante cercana\n\n\n\n\nTomada de: Understanding LSTM networks, Olah (2015).\n\n\n\nFigura 10: Información relevante lejana\n\n\n\n\nTomada de: Understanding LSTM networks, Olah (2015).\n\n\n\nFigura 11: Diferencia entre los módulos de repetición\n\n\n\n\nTomada de: Understanding LSTM networks, Olah (2015).\n\n\n\nFigura 12: LSTM funcionalidad: Representación del paso 1\n\n\n\n\nTomada de: Understanding LSTM networks, Olah (2015).\n\n\n\nFigura 13: LSTM funcionalidad: Representación del paso 2\n\n\n\n\nTomada de: Understanding LSTM networks, Olah (2015).\n\n\n\nFigura 14: LSTM funcionalidad: Representación del paso 3\n\n\n\n\nTomada de: Understanding LSTM networks, Olah (2015).\n\n\n\nFigura 15: LSTM funcionalidad: Representación del paso 4\n\n\n\n\nTomada de: Understanding LSTM networks, Olah (2015).\n\n\n\n\n\nFigura 16: Visualización de vectores de entrada y salida\n\n\n\n\n\n\nElaboración propia: Elaborada a partir de imagen en Chollet y Allaire (2018). Muestra como lucen los vectores tridimensionales de entradas y salidas correspondientes a los datos de una empresa, en el caso de que se usen tres observaciones para crear el vector de entrada.\n\n\n\nFigura 17: Distintas estructras según los distintos tamaños de vectores de entrada\n\n\n\n\nElaboración propia: Elaborada a partir de las distintos modelos construidos usando los paquetes keras y tensorflow en R, y fueron gráficadas mediante el uso del paquete Iannone (2023).\n\n\n\nFigura 18: Dominio de ReLU y Leaky ReLU\n\n\n\n\nElaboración propia: Elaborada a partir de las imagenes que se observan en Rallabandi (2023).\n\n\n\nFigura 19: Diagrama de flujo de la metodología de Walk Forward Validation\n\n\n\n\nElaboración propia\n\n\n\n\n\nChollet, F., y J. J. Allaire. 2018. Deep Learning with R. Manning Publications. https://books.google.es/books?id=xnIRtAEACAAJ.\n\n\nIannone, Richard. 2023. DiagrammeR: Graph/Network Visualization. https://CRAN.R-project.org/package=DiagrammeR.\n\n\nJing, Hong. 2020. «How Convolutional Layers Work in Deep Learning Neural Networks?» Jingles, Github Blog. 2020. https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/.\n\n\nLarrañaga, Iñaki & Moujahid, Pedro & Inza. 2007. «Tema 14. Redes Neuronales». Departamento de Ciencias de la Computaci´on e Inteligencia Artificial, Universidad del Pa´ıs Vasco–Euskal Herriko Unibertsitatea. 2007. http://www.sc.ehu.es/ccwbayes/docencia/mmcc/docs/t14-neuronales.pdf.\n\n\nOlah, Christopher. 2015. «Understanding LSTM networks». Colah’s blog. 2015. https://colah.github.io/posts/2015-08-Understanding-LSTMs/.\n\n\nRallabandi, S. 2023. «Activation functions ReLU vs. Leaky ReLU». 2023. https://medium.com/mlearning-ai/activation-functions-relu-vs-leaky-relu-b8272dc0b1be."
  },
  {
    "objectID": "Annex2.html",
    "href": "Annex2.html",
    "title": "Anexo. 2 Gráficas",
    "section": "",
    "text": "Figura 1: Tendencia alcista y heterocedástica\n\n\n\n\n\n\nElaboración propia: Mediante el uso de RStudio con la base de datos histórico del IBEX, obtenida de https://finance.yahoo.com/, en el periodo comprendido entre el 01-1995 al 01-1997.\n\n\n\n\n\nFigura 2: Tendencia bajista y heterocedástica\n\n\n\n\n\n\nElaboración propia: Mediante el uso de RStudio con la base de datos histórico del IBEX, obtenida de https://finance.yahoo.com/, en el periodo comprendido entre el 01-2000 al 01-2003.\n\n\n\n\n\nFigura 3: Sin tendencia, homocedástica y estacionaria\n\n\n\n\n\n\nElaboración propia: Mediante el uso de RStudio con la base de datos histórico del IBEX obtenida de https://finance.yahoo.com/, en el periodo comprendido entre el 01-2000 al 01-2003, usando las rentabilidades calculadas a partir del precio de cierre.\n\n\n\n\n\nFigura 4: Descomposición: estacionalidad y error\n\n\n\n\n\n\nElaboración propia: Mediante el uso de RStudio con la base de datos histórico del IBEX obtenida de https://finance.yahoo.com/, descomponiendo la serie de tiempo conformada por las observaciones que abarcan el periodo del 01-2000 al 01-2023.\n\n\n\n\n\nFigura 5: Correlograma\n\n\n\n\n\n\nElaboración propia: Mediante el uso de RStudio.\n\n\n\n\n\nFigura 6: Tendencia constante en los precios de cierre ajustado de la empresa “Nueva Expresión Textil S.A”\n\n\n\n\n\n\nElaboración propia: A partir de los datos obtenidos de  (s. f.) correspondientes a la empresa “Nueva Expresión Textil S.A” en el periodo del 31 de enero del 2000 al 28 de febrero del 2023.\n\n\n\n\n\nFigura 7: Cambio brusco de precios que reflejan calculo erróneo de los precios de cierre ajustado, “BANKINTER,S.A.”\n\n\n\n\n\n\nElaboración propia: A partir de los datos obtenidos de  (s. f.) correspondientes a la empresa “Nueva Expresión Textil S.A” en el periodo del 31 de enero del 2000 al 28 de febrero del 2023.\n\n\n\n\n\nFigura 8: Evolución de los indicadores – Entradas con una observación\n\n\n\n\n\n\nElaboración propia: Mediante el uso de R y Rstudio.\n\n\n\n\n\nFigura 9: Evolución de los indicadores – Entradas con dos observaciónes\n\n\n\n\n\n\nElaboración propia: Mediante el uso de R y Rstudio.\n\n\n\n\n\nFigura 10: Evolución de los indicadores – Entradas con tres observaciónes\n\n\n\n\n\n\nElaboración propia: Mediante el uso de R y Rstudio.\n\n\n\n\n\nFigura 11: Evolución de las carteras y el IBEX – Entradas con una observación\n\n\n\n\n\n\nElaboración propia: Mediante el uso de R y Rstudio.\n\n\n\n\n\nFigura 12: Evolución de las carteras y el IBEX – Entradas con dos observaciones\n\n\n\n\n\n\nElaboración propia: Mediante el uso de R y Rstudio.\n\n\n\n\n\nFigura 13: Evolución de las carteras y el IBEX – Entradas con tres observaciones\n\n\n\n\n\n\nElaboración propia: Mediante el uso de R y Rstudio.\n\n\n\n\n\ns. f. Yahoo Finance. https://finance.yahoo.com/."
  },
  {
    "objectID": "Annex3.html",
    "href": "Annex3.html",
    "title": "Anexo. 3 Tablas",
    "section": "",
    "text": "Tabla 1: Estructura de datos de precios\n\n\nDate\nOpen\nHigh\nLow\nClose\nVolume\nAdjusted\n\n\n\n\n2001-05-24\n3.600\n3.620\n3.510\n3.608\n216270100\n-0.1317839\n\n\n2001-05-25\n3.600\n3.676\n3.580\n3.602\n50448300\n-0.1315648\n\n\n2001-05-28\n3.560\n3.604\n3.544\n3.580\n26118945\n-0.1307612\n\n\n2001-05-29\n3.562\n3.626\n3.562\n3.614\n26910070\n-0.1320031\n\n\n2001-05-30\n3.606\n3.648\n3.602\n3.620\n48229995\n-0.1322222\n\n\n2001-05-31\n3.620\n3.676\n3.610\n3.670\n24806710\n-0.1340484\n\n\n\n\n\nElaboración propia: Mediante el uso de RStudio con la base de datos histórico de “INDITEX”, obtenida de https://finance.yahoo.com/, en el periodo comprendido entre el 24-05-2001 al 31-05-2001.\n\n\n\n\nTabla 2: Lista de empresas cotizadas\n\n\n\n\n\n\n\n\n\n\nNOMBRE\nTICKERS\nSECTOR-SUBSECTOR\nMERCADO\nINDICE\nSeleccionadas\n\n\n\n\nACCIONA,S.A.\nANA.MC\nMat.Basicos, Industria y Construcción - Construcción\nMC\nIBEX 35®, IGBM\nX\n\n\nACERINOX, S.A.\nACX.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nACS,ACTIVIDADES DE CONST.Y SERVICIOS S.A\nACS.MC\nMat.Basicos, Industria y Construcción - Construcción\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nADOLFO DOMINGUEZ, S.A.\nADZ.MC\nBienes de Consumo - Textil, Vestido y Calzado\nMC\nIGBM\nX\n\n\nAEDAS HOMES, S.A.\nAEDAS.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nIGBM\nX\n\n\nAENA, S.M.E., S.A.\nAENA.MC\nServicios de Consumo - Transporte y Distribución\nMC\nIBEX 35®, IGBM\nX\n\n\nAIRBUS SE\nAIR.MC\nMat.Basicos, Industria y Construcción - Aerospacial\nMC\nIGBM\nX\n\n\nAIRTIFICIAL INTELLIGENCE STRUCTURES S.A.\nAI.MC\nMat.Basicos, Industria y Construcción - Ingeniería y Otros\nMC\nIGBM\n\n\n\nALANTRA PARTNERS, S.A.\nALNT.MC\nServicios Financieros - Cartera y Holding\nMC\nIGBM\n\n\n\nALMIRALL, S.A.\nALM.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIGBM\nX\n\n\nAMADEUS IT GROUP, S.A.\nAMS.MC\nTecnología y Telecomunicaciones - Electrónica y Software\nMC\nIBEX 35®, IGBM\nX\n\n\nAMPER, S.A.\nAMP.MC\nTecnología y Telecomunicaciones - Electrónica y Software\nMC\nIGBM\nX\n\n\nAMREST HOLDINGS, S.E.\nEAT.MC\nServicios de Consumo - Ocio, Turismo y Hostelería\nMC\nIGBM\nX\n\n\nAPERAM, SOCIETE ANONYME\nAPAM.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nNA\nX\n\n\nAPPLUS SERVICES, S.A.\nAPPS.MC\nMat.Basicos, Industria y Construcción - Ingeniería y Otros\nMC\nIGBM\nX\n\n\nARCELORMITTAL, S.A.\nMTS.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nIBEX 35®, IGBM\nX\n\n\nÁRIMA REAL ESTATE SOCIMI, S.A.\nARM.MC\nServicios Inmobiliarios - SOCIMI\nMC\nIGBM\nX\n\n\nATRESMEDIA CORP. DE MEDIOS DE COM. S.A.\nA3M.MC\nServicios de Consumo - Medios de Comunicación y Publicidad\nMC\nIGBM\nX\n\n\nATRYS HEALTH, S.A.\nATRY.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIGBM\nX\n\n\nAUDAX RENOVABLES, S.A.\nADX.MC\nPetróleo y Energía - Energías Renovables\nMC\nIGBM\n\n\n\nAZKOYEN S.A.\nAZK.MC\nMat.Basicos, Industria y Construcción - Fabric. y Montaje Bienes de Equipo\nMC\nIGBM\nX\n\n\nBANCO BILBAO VIZCAYA ARGENTARIA, S.A.\nBBVA.MC\nServicios Financieros - Bancos y Cajas de Ahorro\nMC\nIBEX 35®, IGBM\nX\n\n\nBANCO DE SABADELL, S.A.\nSAB.MC\nServicios Financieros - Bancos y Cajas de Ahorro\nMC\nIBEX 35®, IGBM\nX\n\n\nBANCO SANTANDER, S.A.\nSAN.MC\nServicios Financieros - Bancos y Cajas de Ahorro\nMC\nIBEX 35®, IGBM\nX\n\n\nBANKINTER,S.A.\nBKT.MC\nServicios Financieros - Bancos y Cajas de Ahorro\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nBERKELEY ENERGIA LIMITED\nBKY.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nIGBM\nX\n\n\nBODEGAS RIOJANAS, S.A.\nRIO.MC\nBienes de Consumo - Alimentación y Bebidas\nMC\nIGBM\nX\n\n\nBORGES AGRICULTURAL & INDUST. NUTS, S.A.\nBAIN.MC\nBienes de Consumo - Alimentación y Bebidas\nMC\nNA\n\n\n\nCAIXABANK, S.A.\nCABK.MC\nServicios Financieros - Bancos y Cajas de Ahorro\nMC\nIBEX 35®, IGBM\nX\n\n\nCAJA DE AHORROS DEL MEDITERRANEO\nCAM.MC\nServicios Financieros - Bancos y Cajas de Ahorro\nMC\nNA\n\n\n\nCASH, S.A.\nCASH.MC\nServicios de Consumo - Otros Servicios\nMC\nIGBM, IBEXTD®\nX\n\n\nCELLNEX TELECOM, S.A.\nCLNX.MC\nTecnología y Telecomunicaciones - Telecomunicaciones y Otros\nMC\nIBEX 35®, IGBM\nX\n\n\nCIA. DE DIST. INTEG. LOGISTA HOLDINGS\nLOG.MC\nServicios de Consumo - Transporte y Distribución\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nCIA. ESPAÑOLA VIVIENDAS EN ALQUILER,S.A\nCEV.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nNA\n\n\n\nCIA.LEVANTINA, EDIFICACION DE O.PUBLICAS\nCLEO.MC\nMat.Basicos, Industria y Construcción - Construcción\nMC\nNA\n\n\n\nCIE AUTOMOTIVE, S.A.\nCIE.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nIGBM\nX\n\n\nCLINICA BAVIERA, S.A.\nCBAV.MC\nServicios de Consumo - Otros Servicios\nMC\nIGBM\nX\n\n\nCOCA-COLA EUROPACIFIC PARTNERS PLC\nCCEP.MC\nBienes de Consumo - Alimentación y Bebidas\nMC\nIGBM\nX\n\n\nCONSTRUCC. Y AUX. DE FERROCARRILES, S.A.\nCAF.MC\nMat.Basicos, Industria y Construcción - Fabric. y Montaje Bienes de Equipo\nMC\nIGBM\nX\n\n\nCORP. ACCIONA ENERGÍAS RENOVABLES, S.A.\nANE.MC\nPetróleo y Energía - Energías Renovables\nMC\nIBEX 35®, IGBM\nX\n\n\nCORPORACION FINANCIERA ALBA, S.A.\nALB.MC\nServicios Financieros - Cartera y Holding\nMC\nIGBM\nX\n\n\nDEOLEO, S.A.\nOLE.MC\nBienes de Consumo - Alimentación y Bebidas\nMC\nIGBM\nX\n\n\nDESA\nDESA.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nNA\n\n\n\nDIA-DISTRIBUIDORA INT. DE ALIMENT. S.A.\nDIA.MC\nServicios de Consumo - Comercio\nMC\nIGBM\nX\n\n\nDURO FELGUERA, S.A.\nMDF.MC\nMat.Basicos, Industria y Construcción - Ingeniería y Otros\nMC\nIGBM\nX\n\n\nEBRO FOODS, S.A.\nEBRO.MC\nBienes de Consumo - Alimentación y Bebidas\nMC\nIGBM, IBEXTD®\nX\n\n\nEDREAMS ODIGEO, S.A.\nEDR.MC\nServicios de Consumo - Ocio, Turismo y Hostelería\nMC\nIGBM\nX\n\n\nELECNOR S. A.\nENO.MC\nMat.Basicos, Industria y Construcción - Fabric. y Montaje Bienes de Equipo\nMC\nIGBM, IBEXTD®\nX\n\n\nENAGAS, S.A.\nENG.MC\nPetróleo y Energía - Electricidad y Gas\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nENCE ENERGIA Y CELULOSA, S.A.\nENC.MC\nBienes de Consumo - Papel y Artes Gráficas\nMC\nIGBM\nX\n\n\nENDESA, SOCIEDAD ANONIMA\nELE.MC\nPetróleo y Energía - Electricidad y Gas\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nERCROS S.A.\nECR.MC\nMat.Basicos, Industria y Construcción - Industria Química\nMC\nIGBM\nX\n\n\nFAES FARMA, S.A.\nFAE.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIGBM, IBEXTD®\nX\n\n\nFERROVIAL, S.A.\nFER.MC\nMat.Basicos, Industria y Construcción - Construcción\nMC\nIBEX 35®, IGBM\nX\n\n\nFLUIDRA, S.A.\nFDR.MC\nMat.Basicos, Industria y Construcción - Ingeniería y Otros\nMC\nIBEX 35®, IGBM\nX\n\n\nFOMENTO DE CONSTR. Y CONTRATAS S.A.\nFCC.MC\nMat.Basicos, Industria y Construcción - Construcción\nMC\nIGBM, IBEXTD®\nX\n\n\nGENERAL DE ALQUILER DE MAQUINARIA, S.A.\nGAM.MC\nMat.Basicos, Industria y Construcción - Ingeniería y Otros\nMC\nIGBM\n\n\n\nGESTAMP AUTOMOCION, S.A.\nGEST.MC\nMat.Basicos, Industria y Construcción - Fabric. y Montaje Bienes de Equipo\nMC\nIGBM\nX\n\n\nGLOBAL DOMINION ACCESS, S.A.\nDOM.MC\nTecnología y Telecomunicaciones - Telecomunicaciones y Otros\nMC\nIGBM\nX\n\n\nGRENERGY RENOVABLES, S.A.\nGRE.MC\nPetróleo y Energía - Energías Renovables\nMC\nIGBM\nX\n\n\nGRIFOLS, S.A.\nGRF.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIBEX 35®, IGBM\nX\n\n\nGRUPO CATALANA OCCIDENTE, S.A.\nGCO.MC\nServicios Financieros - Seguros\nMC\nIGBM\nX\n\n\nGRUPO ECOENER, S.A.\nENER.MC\nPetróleo y Energía - Energías Renovables\nMC\nIGBM\nX\n\n\nGRUPO EMPRESARIAL SAN JOSE, S.A.\nGSJ.MC\nMat.Basicos, Industria y Construcción - Construcción\nMC\nIGBM\nX\n\n\nGRUPO EZENTIS, S.A.\nEZE.MC\nTecnología y Telecomunicaciones - Telecomunicaciones y Otros\nMC\nNA\nX\n\n\nIBERDROLA, S.A.\nIBE.MC\nPetróleo y Energía - Electricidad y Gas\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nIBERPAPEL GESTION, S.A.\nIBG.MC\nBienes de Consumo - Papel y Artes Gráficas\nMC\nIGBM\nX\n\n\nINDRA SISTEMAS, S.A., SERIE A\nIDR.MC\nTecnología y Telecomunicaciones - Electrónica y Software\nMC\nIBEX 35®, IGBM\nX\n\n\nINDUSTRIA DE DISEÑO TEXTIL, SA “INDITEX”\nITX.MC\nBienes de Consumo - Textil, Vestido y Calzado\nMC\nIBEX 35®, IGBM\nX\n\n\nINMOBILIARIA COLONIAL SOCIMI, S.A.\nCOL.MC\nServicios Inmobiliarios - SOCIMI\nMC\nIBEX 35®, IGBM\nX\n\n\nINMOBILIARIA DEL SUR, S.A.\nISUR.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nIGBM\nX\n\n\nINNOVATIVE SOLUTIONS ECOSYSTEM, S.A.\nISE.MC\nServicios de Consumo - Comercio\nMC\nNA\n\n\n\nINTERNATIONAL CONSOLIDAT. AIRLINES GROUP\nIAG.MC\nServicios de Consumo - Transporte y Distribución\nMC\nIBEX 35®, IGBM\nX\n\n\nLABORATORIO REIG JOFRE, S.A.\nRJF.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIGBM\nX\n\n\nLABORATORIOS FARMACEUTICOS ROVI, S.A.\nROVI.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIBEX 35®, IGBM\nX\n\n\nLAR ESPAÑA REAL ESTATE, SOCIMI, S.A.\nLRE.MC\nServicios Inmobiliarios - SOCIMI\nMC\nIGBM, IBEXTD®\nX\n\n\nLIBERTAS 7, S.A.\nLIB.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nNA\n\n\n\nLINEA DIRECTA ASEGURADORA, S.A.\nLDA.MC\nServicios Financieros - Seguros\nMC\nIGBM\nX\n\n\nLINGOTES ESPECIALES, S.A.\nLGT.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nNA\nX\n\n\nMAPFRE, S.A.\nMAP.MC\nServicios Financieros - Seguros\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nMELIA HOTELS INTERNATIONAL, S.A.\nMEL.MC\nServicios de Consumo - Ocio, Turismo y Hostelería\nMC\nIBEX 35®, IGBM\nX\n\n\nMERLIN PROPERTIES, SOCIMI, S.A.\nMRL.MC\nServicios Inmobiliarios - SOCIMI\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nMETROVACESA, S.A.\nMVC.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nIGBM\nX\n\n\nMIQUEL Y COSTAS & MIQUEL, S.A.\nMCM.MC\nBienes de Consumo - Papel y Artes Gráficas\nMC\nIGBM, IBEXTD®\nX\n\n\nMONTEBALITO, S.A.\nMTB.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nNA\nX\n\n\nNATURGY ENERGY GROUP, S.A.\nNTGY.MC\nPetróleo y Energía - Electricidad y Gas\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nNATURHOUSE HEALTH, S.A.\nNTH.MC\nBienes de Consumo - Alimentación y Bebidas\nMC\nIGBM\nX\n\n\nNEINOR HOMES, S.A.\nHOME.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nIGBM\nX\n\n\nNH HOTEL GROUP, S.A.\nNHH.MC\nServicios de Consumo - Ocio, Turismo y Hostelería\nMC\nIGBM\nX\n\n\nNICOLAS CORREA S.A.\nNEA.MC\nMat.Basicos, Industria y Construcción - Fabric. y Montaje Bienes de Equipo\nMC\nIGBM, IBEXTD®\nX\n\n\nNUEVA EXPRESION TEXTIL, S.A.\nNXT.MC\nBienes de Consumo - Textil, Vestido y Calzado\nMC\nIGBM\n\n\n\nNYESA VALORES CORPORACION, S.A.\nNYE.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nIGBM\n\n\n\nOBRASCON HUARTE LAIN, S.A.\nOHLA.MC\nMat.Basicos, Industria y Construcción - Construcción\nMC\nIGBM\nX\n\n\nOPDENERGY HOLDING, S.A.\nOPDE.MC\nPetróleo y Energía - Energías Renovables\nMC\nIGBM\nX\n\n\nORYZON GENOMICS, S.A.\nORY.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIGBM\nX\n\n\nPESCANOVA, S.A.\nPVA.MC\nBienes de Consumo - Alimentación y Bebidas\nMC\nIGBM\n\n\n\nPHARMA MAR, S.A.\nPHM.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIGBM\nX\n\n\nPRIM, S.A.\nPRM.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIGBM\nX\n\n\nPROMOTORA DE INFORMACIONES,S.A.\nPRS.MC\nServicios de Consumo - Medios de Comunicación y Publicidad\nMC\nIGBM\nX\n\n\nPROSEGUR , CIA. DE SEGURIDAD, S.A.\nPSG.MC\nServicios de Consumo - Otros Servicios\nMC\nIGBM, IBEXTD®\nX\n\n\nREALIA BUSINESS, S.A.\nRLIA.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nIGBM\nX\n\n\nRED ELECTRICA CORPORACION, S.A.\nRED.MC\nPetróleo y Energía - Electricidad y Gas\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nRENTA 4 BANCO, S.A.\nR4.MC\nServicios Financieros - Servicios de Inversión\nMC\nIGBM\nX\n\n\nRENTA CORPORACION REAL ESTATE, S.A.\nREN.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nIGBM\n\n\n\nREPSOL, S.A.\nREP.MC\nPetróleo y Energía - Petróleo\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nSACYR, S.A.\nSCYR.MC\nMat.Basicos, Industria y Construcción - Construcción\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nSOLARIA ENERGIA Y MEDIO AMBIENTE, S.A.\nSLR.MC\nPetróleo y Energía - Energías Renovables\nMC\nIBEX 35®, IGBM\nX\n\n\nSOLTEC POWER HOLDINGS, S.A.\nSOL.MC\nPetróleo y Energía - Energías Renovables\nMC\nIGBM\nX\n\n\nSQUIRREL MEDIA, S.A\nSQRL.MC\nServicios de Consumo - Medios de Comunicación y Publicidad\nMC\nNA\n\n\n\nTALGO, S.A.\nTLGO.MC\nMat.Basicos, Industria y Construcción - Fabric. y Montaje Bienes de Equipo\nMC\nIGBM\nX\n\n\nTECNICAS REUNIDAS, S.A.\nTRE.MC\nMat.Basicos, Industria y Construcción - Ingeniería y Otros\nMC\nIGBM\nX\n\n\nTELEFONICA, S.A.\nTEF.MC\nTecnología y Telecomunicaciones - Telecomunicaciones y Otros\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nTUBACEX, S.A.\nTUB.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nIGBM\nX\n\n\nTUBOS REUNIDOS,S.A.\nTRG.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nIGBM\nX\n\n\nUNICAJA BANCO, S.A.\nUNI.MC\nServicios Financieros - Bancos y Cajas de Ahorro\nMC\nIBEX 35®, IGBM\nX\n\n\nURBAS GRUPO FINANCIERO, S.A.\nUBS.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nIGBM\n\n\n\nVIDRALA S.A.\nVID.MC\nBienes de Consumo - Otros Bienes de Consumo\nMC\nIGBM\nX\n\n\nVISCOFAN, S.A.\nVIS.MC\nBienes de Consumo - Alimentación y Bebidas\nMC\nIGBM, IBEXTD®\nX\n\n\nVOCENTO, S.A.\nVOC.MC\nServicios de Consumo - Medios de Comunicación y Publicidad\nMC\nIGBM\nX\n\n\n\n\n\n\nObtenido de: La información expuesta en el sitio oficial de Bolsas y Mercados españoles, «Empresas cotizadas» (s. f.). Nota: MC en MERCADO significa Mercado Continuo, IBEXTD en INDICE significa IBEX TOP Dividendo.\n\n\n\nTabla 3: Cantidades de muestras utilizadas para entrenar los modelos\n\n\nEntradas\nMuestras.totales\n\n\n\n\n1\n17347\n\n\n2\n17244\n\n\n3\n17141\n\n\n\n\n\nObtenido de: La información expuesta en el sitio oficial de Bolsas y Mercados españoles, «Empresas cotizadas» (s. f.).\n\n\n\n\n\nTabla 4: Mejores y peroes empresas según los resultados obtenidos de los cálculos de los indicadores\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n\nTICKER\nR2\nMSE\nTICKER\nR2\nMSE\nTICKER\nR2\nMSE\n\n\n\n\nOPDE.MC\n0.4976431\n0.0140459\nOPDE.MC\n0.3889262\n0.0119095\nOPDE.MC\n0.4861395\n0.0121918\n\n\nOLE.MC\n0.1408607\n0.2092006\nSOL.MC\n0.3037137\n0.0225800\nANE.MC\n0.3485550\n0.0063473\n\n\nANE.MC\n0.1073977\n0.0066219\nANE.MC\n0.1965812\n0.0064902\nSOL.MC\n0.3160949\n0.0207540\n\n\nDOM.MC\n0.1032658\n0.0055531\nBKY.MC\n0.1687609\n0.1873538\nBKY.MC\n0.1983894\n0.2024927\n\n\nMVC.MC\n0.1005222\n0.0095915\nTLGO.MC\n0.1390272\n0.0087595\nLOG.MC\n0.1928642\n0.0045066\n\n\nBKY.MC\n0.0612159\n0.2068808\nOLE.MC\n0.1077772\n0.2036521\nLDA.MC\n0.1502733\n0.0035126\n\n\nTLGO.MC\n0.0586490\n0.0091608\nAEDAS.MC\n0.1069500\n0.0069291\nTLGO.MC\n0.1165425\n0.0081263\n\n\nMRL.MC\n0.0558508\n0.0062275\nMVC.MC\n0.0990202\n0.0097977\nOLE.MC\n0.0676713\n0.2072154\n\n\nAEDAS.MC\n0.0504877\n0.0072116\nENER.MC\n0.0977418\n0.0098403\nCBAV.MC\n0.0590027\n0.0116354\n\n\nNTH.MC\n0.0496536\n0.0108446\nCCEP.MC\n0.0865251\n0.0041225\nENER.MC\n0.0523626\n0.0111998\n\n\nEBRO.MC\n-0.1131305\n0.0026464\nAENA.MC\n-0.0116025\n0.0059543\nBBVA.MC\n-0.1015487\n0.0106436\n\n\nVID.MC\n-0.1182183\n0.0054714\nPRS.MC\n-0.0147394\n0.0274017\nSAN.MC\n-0.1064919\n0.0101428\n\n\nAMS.MC\n-0.1195827\n0.0061822\nDIA.MC\n-0.0251285\n0.0218383\nEBRO.MC\n-0.1097231\n0.0026198\n\n\nATRY.MC\n-0.1372633\n0.0122878\nRIO.MC\n-0.0251470\n0.0029560\nRED.MC\n-0.1307882\n0.0038899\n\n\nENER.MC\n-0.1393435\n0.0124522\nVIS.MC\n-0.0288599\n0.0034482\nMVC.MC\n-0.1332527\n0.0125188\n\n\nPRM.MC\n-0.1685250\n0.0073096\nTEF.MC\n-0.0294417\n0.0054187\nRIO.MC\n-0.1363685\n0.0032897\n\n\nAPPS.MC\n-0.2295015\n0.0080938\nITX.MC\n-0.0311148\n0.0077844\nALB.MC\n-0.1487811\n0.0052084\n\n\nR4.MC\n-0.2788249\n0.0022699\nGRE.MC\n-0.0456889\n0.0285741\nIBE.MC\n-0.1519585\n0.0056253\n\n\nVIS.MC\n-0.2859081\n0.0042912\nATRY.MC\n-0.1516661\n0.0125130\nHOME.MC\n-0.1528376\n0.0073972\n\n\nLDA.MC\n-0.3181947\n0.0054316\nLDA.MC\n-0.1579126\n0.0042955\nVIS.MC\n-0.1825075\n0.0039782\n\n\n\n\n\n\n\n\n\nElaboración propia.\n\n\n\n\n\n«Empresas cotizadas». s. f. BME Exchange. Accedido 21 de mayo de 2023. https://www.bolsasymercados.es/bme-exchange/es/Mercados-y-Cotizaciones/Acciones/Mercado-Continuo/Empresas-Cotizadas."
  },
  {
    "objectID": "Annex4.html#datos",
    "href": "Annex4.html#datos",
    "title": "Anexo. 4 Códigos",
    "section": "Datos",
    "text": "Datos\n\nObtención de Datos\nLo primero que se realizó fue cargar la tabla de las empresas.\n\nempresas &lt;- read_excel(\"data/000_empresas.xlsx\")\n\nLuego se extrageron los ticks de las empresas.\n\nticks &lt;- empresas |&gt; \n  select(TICKERS) |&gt; \n  pull()\n\nUna vez almacenados los ticks de las empresas en la variable ticks se procedio a descargar los datos correspondientes a dichas empresas desde Yahoo Finance usando el paquete quantmod de Ryan y Ulrich (2023).\n\nnombres_colum &lt;- c(\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"Adjusted\")\nqmd_data &lt;- list()\nfor (i in 1:length(ticks)) {\n  tick &lt;- ticks[i]\n  value &lt;- getSymbols(\n    tick,\n    from = \"2000-01-02\",\n    to = \"2023-03-01\",\n    auto.assign = F,\n    periodicity = \"monthly\") |&gt;\n    as.data.frame()\n  dates &lt;- row.names(value)\n  row.names(value) &lt;- NULL\n  value &lt;- cbind(dates,value)\n  names(value) &lt;- nombres_colum\n  qmd_data[[tick]] &lt;-  value\n}\n\nCon el objetivo de realizar un análisis exploratorio de los datos, se decidió realizar una evaluación visual de los datos históricos del precio ajustado para lo que se ejecutó:\n\nlapply(qmd_data, function(x){\n  x |&gt;\n    ggplot(aes(x=as.Date(Date), y=Adjusted))+\n             geom_line(color=\"#065AD8\")\n})\n\nTras el análisis visual ejecutado con el fragmento de código anterior se persivió la existencia de precios constantes, así como calculos erroneos en el precio ajustado correspondiente a los primeros años de algunas series. Con el objetivo de eliminar estas irregularidades se seleccionaron solo aquellas observaciones posteriores a enero del 2005.\n\nreturns_emps &lt;- qmd_data |&gt;\n  lapply(function(x){\n    emps &lt;- x |&gt;\n      filter(Date &gt;= \"2005-01-31\")\n  })\n\nCon el objetivo de determinar si los datos que habían sido importados contaban con valores faltantes se ejecutó el siguiente código:\n\nna_values &lt;- returns_emps |&gt;\n  sapply(function(x){\n    na &lt;- length(which(is.na(x)))\n  })\nemp_con_na &lt;- which(na_values &gt; 0)\n\nCon el objetivo de solucionar el problema con respecto al incorrecto registro de los datos se decidio eliminar aquellas que no presentaran variaciones en los precios en más de 10 observaciones. Para lo que primero se computaron las rentabilidades ejecutando el siguiente código, mediante el cual además se eliminaron las series con valores faltantes.\n\nreturns_emps2 &lt;- returns_emps[-emp_con_na] |&gt;\n  lapply(function(x){\n    returns &lt;- x |&gt;\n      select(Date, Adjusted) |&gt;\n      mutate(Return_Ad = Delt(Adjusted)[,1]) |&gt;\n      na.omit() |&gt;\n      select(Date, Return_Ad)\n  })\n\nUna vez computadas las rentabilidades se eliminaron aquellas series que presentaban en más de 10 observaciones rentabilidad 0, para lo que se ejecutó el siguiente código.\n\nzero_values &lt;- returns_emps2 |&gt;\n  sapply(function(x){\n    zeros &lt;- length(which(x[,2]==0))\n  })\nreturns_emps3 &lt;- returns_emps2[zero_values&lt;10]\n\n\n\nIndicadores\nA continuación, se expone el código utilizado durante el proceso expuesto en el sub-epígrafe indicadores del capítulo 2.\nPrimero se descargaron los datos del IBEX, se computaron las rentabilidades del precio ajustado del mismo y se seleccionaron los valores posteriores a enero del 2005.\n\n#Importando IBEX\nIBEXsel &lt;- getSymbols(\n  \"^IBEX\",\n  from = \"1990-01-01\",\n  to = \"2023-03-01\",\n  auto.assign = F,\n  periodicity = \"monthly\") |&gt;\n  as.data.frame()\ndates &lt;- row.names(IBEXsel)\nrow.names(IBEXsel) &lt;- NULL\nIBEXsel &lt;- cbind(dates,IBEXsel)\nnames(IBEXsel) &lt;- nombres_colum\n# Calculando rentabilidad y seleccionando observaciones posteriores a\n# enero del 2005.\nIBEXsel &lt;- IBEXsel |&gt;\n  mutate(Return_I = Delt(Adjusted)[,1]) |&gt;\n  na.omit() |&gt;\n  filter(Date &gt;= \"2005-01-31\") |&gt;\n  select(Date, Return_I)\n\nLuego se agregaron los valores de las rentabilidades del IBEX a las tablas de las rentabilidades de las acciones de las empresas seleccionadas, y se computaron y agregaron las variables listadas a continuación a cada una de las tablas:\n\nVolatilidad de la empresa\nVolatilidad del índice\nCorrelación entre las rentabilidades de la empresa y el indice\nLa Beta entre la empresa y el indice\n\n\nreturns_indc &lt;- returns_emps3 |&gt;\n  lapply(function(x, ind = IBEXsel){\n    emp &lt;- x |&gt;\n      left_join(ind) |&gt;\n      mutate(\n        VE = sqrt(cumsum((Return_Ad - cummean(Return_Ad))^2)/1:length(Return_Ad)),\n        VI = sqrt(cumsum((Return_I - cummean(Return_I))^2)/1:length(Return_I)),\n        Cor = cumsum((Return_Ad-cummean(Return_Ad))*(Return_I-cummean(Return_I)))/(sqrt(cumsum((Return_Ad-cummean(Return_Ad))^2))*sqrt(cumsum((Return_I-cummean(Return_I))^2)))\n      )|&gt;\n      na.omit() |&gt;\n      mutate(\n        Beta = (Cor*VE)/VI\n      )\n  })\n\n\n\nVectores\nA continuación se expone el código utilizado durante el proceso expuesto en el sub-epígrafe vectores del epígrafe modelado del Capítulo 2.\nEl primer paso llevado a cabo para la ejecución del proceso explicado en el sub-epígrafe en cuestión fue crear una función que permitió obtener las muestras consecutivas para cada serie utilizada. La función expuesta a continuación, como ya se mencionó, permite obtener las muestras consecutivas de una serie, para lo que se utilizan los parametros mencionados en el sub-epígrafe, número de observaciones de entradas y número de observaciones de salida, así como un parametro condicional con el que se indica si el vector a crear es de entrada o de salida.\n\nvector2dmaker &lt;- function(vec, ent, sal, eos=T){\n  if(eos==T){\n    emp &lt;- 1\n    term &lt;- (length(vec) - (ent+sal-1))\n    ob &lt;- ent\n  }else{\n    emp &lt;- ent + 1\n    term &lt;- (length(vec)-sal+1)\n    ob &lt;- sal\n  }\n  \n  vec2d &lt;- sapply(emp:term,\n               function(x) vec[x:(x + ob-1)]) |&gt;\n    matrix(nrow = ob) |&gt;\n    t()\n  \n  return(vec2d)\n}\n\nA continuación se muestra el código utilizado para la creación de los vectores de entrada de correspondiente a cada una de las series. Para lo que primero se crearon dos funciones una para las entradas y otra para las salidas.\n\n# Función que se utlizará para crear las entradas tridimensionales\ninput3dmaker &lt;- function(x,inp,out){\n  empre &lt;- x\n  series &lt;- 2:dim(x)[2]\n  for (i in series) {\n    if(i==series[1]){\n      vec3d &lt;- vector2dmaker(empre[[i]],ent=inp,sal=out)\n    }else{\n      vec3d &lt;- abind(vec3d,vector2dmaker(empre[[i]],ent=inp,sal=out), along = 3)\n    }\n  }\n  return(vec3d)\n}\n\n# Función que se utlizará para crear las salidas tridimensionales\noutput3dmaker &lt;- function(x,inp,out){\n  empre &lt;- x[[\"Return_Ad\"]]\n  vec3d &lt;- vector2dmaker(empre,ent=inp,sal=out,F)\n  dim(vec3d) &lt;- c(dim(vec3d),1)\n  return(vec3d)\n}\n\nLuego se crearon las listas de vectores tridimensionales de entradas y salidas por empresa, ejecutandosé el siguiente código otras dos veces con el objetivo de crear las listas vecs3d2e y vecs3d3e que corresponden a aquellos casos en los que se seleccionaron 2 y 3 entradas.\n\n#Se define el horizonte temporal\nht &lt;- 1\n\n#Se define el observaciones de entrada\noe &lt;- 1\n\n#Se crean los vectores de entrada 3d y el vector 2d de salida para tamaño de entrada 1\nvecs3d1e &lt;- list()\nfor(i in 1:length(returns_indc)){\n  emp &lt;- returns_indc[[i]]\n  inps &lt;- input3dmaker(emp, oe, ht)\n  outs &lt;- output3dmaker(emp, oe, ht)\n  dates &lt;- emp[(oe + ht):dim(emp)[1],1]\n  id &lt;- rep(names(returns_indc)[i],length(dates))\n  tibblex &lt;- tibble(\n    Date = dates,\n    ID = id,\n    inputs = inps,\n    outputs = outs\n  )\n  vecs3d1e[[names(returns_indc)[i]]] &lt;- tibblex\n}"
  },
  {
    "objectID": "Annex4.html#modelado-y-entrenamiento",
    "href": "Annex4.html#modelado-y-entrenamiento",
    "title": "Anexo. 4 Códigos",
    "section": "Modelado y entrenamiento",
    "text": "Modelado y entrenamiento\nA continuación, se presenta el código utilizado durante el proceso descrito en los distintos sub-epígrafes del epígrafe Modelado y entrenamiento.\n\nModelado\nPara la creación de los modelos el primer paso a ejecutar es obtener la información de los vectores para los que se va a construir el modelo, lo que se hizo ejecutando el siguiente código:\n\ndata &lt;- bind_rows(vecs3d1e)\ndata &lt;- data  |&gt;\n  arrange(Date)\ninputsinfo &lt;- data|&gt;\n  select(inputs) |&gt;\n  pull() |&gt;\n  dim()\noutputsinfo &lt;- data|&gt;\n  select(outputs) |&gt;\n  pull() |&gt;\n  dim()\n\n# Definir parámetros\nn_ob_pas &lt;- inputsinfo[2]\nn_variables &lt;- inputsinfo[3]\nn_ob_fut &lt;- outputsinfo[2]\n\nLuego se constituyo la estructura de los modelos con los aspectos descritos en 2.5.1 Modelado.\n\n# Capa de entrada\ninp &lt;- layer_input(\n  shape = c(NULL,n_ob_pas,n_variables))\n\n# Capas ocultas\n# - CNN\ncnn &lt;- inp |&gt;\n  layer_conv_1d(\n    filters = 64,\n    kernel_size = 1,\n    activation = layer_activation_leaky_relu())\n# - LSTM\nlstm &lt;- cnn |&gt;\n  layer_lstm(64)\n\n# Capa de Salida\nout &lt;- lstm |&gt; \n  layer_dense(\n    n_ob_fut*1)\n\n# Juntar las capas para constituir el modelo \nmodel &lt;- keras_model(inp, out)\n# Estableciendo parámetros de aprendizaje\nmodel |&gt; \n  compile(loss = \"mse\", optimizer = optimizer_sgd(0.0005))\n\n\n\n\n\n\n\nNota\n\n\n\nPuede encontrar modelos sin entrenar en la carpeta data del repositorio en el que se encuentra el presente trabajo. Los modelos fueron guardados usando la extensión hdf5 y bajo los nombres model1e, model2e y model3e.\n\n\n\n\nEntrenamiento\nEl primer paso es definir la función a utilizar para el entrenamiento de los modelos. Esta función fue construida con el objetivo de emplear el método de entrenamiento descrito en 2.5.2 Entrenamiento. Como resultado esta función devolvera una lista que contendra las predicciones obtenidas y el modelo después de haber sido entrenado y tomará como entradas principales el tibble llamado data constituido en el primer paso que se expone en este Anexo en la sección Modelado y el modelo además de otros argumentos que permite la utilización de la función con unos inputs principales que no sean los utilizados en el presente trabajo.\n\nwfv_train &lt;- function(x, modelo, seq_var_name, inp_var_name = \"inputs\", out_var_name = \"outputs\", progress_bar=T){\n  \n  predictions &lt;- c()\n  seq_val &lt;- unique(x[[seq_var_name]])\n  \n  if(progress_bar){\n    pb &lt;- txtProgressBar(min = 0, max = length(seq_val), initial = 0, style = 3)\n  }\n  \n  \n  # Iteración que se ejecutará para cada valor unico en la variable que define la secuencia de los datos. Por ello es de vital importancia que los datos en el tibble x se encuentren ordenados por la variable de secuencia cuyo nombre se pasa a seq_var_name\n  \n  for (i in 1:length(seq_val)) {\n    val_seq &lt;- seq_val[i]\n    #Extraer entradas y salidas correspondiente al periodo en la variable de secuencia actual\n    inputs &lt;- x |&gt;\n      filter(!!sym(seq_var_name) == val_seq) |&gt;\n      select(!!sym(inp_var_name)) |&gt;\n      pull()\n    outputs &lt;- x |&gt;\n      filter(!!sym(seq_var_name) == val_seq) |&gt;\n      select(!!sym(out_var_name)) |&gt;\n      pull()\n    outputs &lt;- outputs[,,1]\n    \n    #Usar entradas para obtener predicciones para los periodos en la variable secuencia a excepción del primero\n    if(i &gt; 1){\n      pred &lt;- modelo |&gt;\n        predict(inputs, verbose = 3)\n      predictions &lt;- rbind(predictions, pred)\n    }\n    \n    # Entrenar el modelo\n    modelo |&gt;\n      fit(\n        inputs,\n        outputs,\n        epochs = 1,\n        batch_size = 10,\n        shuffle = F,\n        verbose = 0)\n    \n    if(progress_bar){\n      setTxtProgressBar(pb,i)\n      }\n    \n  }\n  \n  if(progress_bar){\n    close(pb)\n  }\n  \n  results &lt;- list()\n  results[['predicciones']] &lt;- predictions\n  results[['modelo']] &lt;- modelo\n  return(results)\n}\n\nUna vez creada la función se obtuvieron las predicciones utilizando el siguiente código:\n\nresultados &lt;- wfv_train(data,model,'Date')\npredicciones1e &lt;- resultados$predicciones\n\n\n\n\n\n\n\nNota\n\n\n\nPuede encontrar modelos entrenados en la carpeta data del repositorio en el que se encuentra el presente trabajo. Los modelos fueron guardados usando la extensión hdf5 y bajo los nombres model1etd, model2etd y model3etd.\n\n\nComo se explica en 2.6.1 Predicciones además de las predicciones obtenidas por los modelos se computaron predicciones obtenidas a partir del uso de la media aritmetica, para comparar con las obtenidas con los modelos. Para el computo de estas predicciones se creo la siguiente función:\n\nwfv_means &lt;- function(x, seq_var_name, inp_var_name = \"inputs\", out_var_name = \"outputs\", id_var_name, progress_bar=T){\n  \n  means &lt;- c()\n  seq_val &lt;- unique(x[[seq_var_name]])\n  \n  if(progress_bar){\n    pb &lt;- txtProgressBar(min = 0, max = length(seq_val), initial = 0, style = 3)\n  }\n  \n  for (i in 1:length(seq_val)) {\n    val_seq &lt;- seq_val[i]\n    inputs &lt;- x |&gt;\n      filter(!!sym(seq_var_name) == val_seq) |&gt;\n      select(!!sym(inp_var_name)) |&gt;\n      pull()\n    inputspred &lt;- x |&gt;\n      filter(!!sym(seq_var_name) == val_seq) |&gt;\n      select(!!sym(inp_var_name)) |&gt;\n      pull()\n    outputs &lt;- x |&gt;\n      filter(!!sym(seq_var_name) == val_seq) |&gt;\n      select(!!sym(out_var_name)) |&gt;\n      pull()\n    outputs &lt;- outputs[,,1]\n    \n    ids &lt;- x |&gt;\n      filter(!!sym(seq_var_name) == val_seq) |&gt;\n      select(!!sym(id_var_name)) |&gt;\n      pull()\n    \n    if(i==1){\n      dfmeans &lt;- inputs[,,1] |&gt;\n        as.data.frame() |&gt;\n        cbind(ID = ids)\n    }else{\n      dfmeansupd &lt;- inputs[,dim(inputs)[2],1] |&gt;\n        as.data.frame() |&gt;\n        cbind(ID = ids)\n      names(dfmeansupd)[1] &lt;- paste0(\"V\",(dim(dfmeans)[2]))\n      idsdf &lt;- unique(c(ids, dfmeans[[id_var_name]]))\n      idsdf &lt;- data.frame(ID = idsdf)\n      dfmeansupd &lt;- dplyr::left_join(idsdf, dfmeansupd, by = \"ID\")\n      ifelse(\n        dim(dfmeansupd)[1] &gt; dim(dfmeans)[1],\n        dfmeans &lt;- dplyr::left_join(dfmeansupd, dfmeans, by = \"ID\"),\n        dfmeans &lt;- dplyr::left_join(dfmeans, dfmeansupd, by = \"ID\")\n        )\n    }\n    \n    if(i &gt; 1){\n      MEANS &lt;-  dfmeans |&gt;\n        rowwise() |&gt;\n        mutate(\n          means = mean(c_across(-!!sym(id_var_name)), na.rm = T)) |&gt;\n        slice(match(ids,!!sym(id_var_name))) |&gt;\n        pull(means) |&gt;\n        as.matrix()\n      means &lt;- rbind(means, MEANS)\n    }\n    \n    if(progress_bar){\n      setTxtProgressBar(pb,i)\n    }\n    \n  }\n  \n  if(progress_bar){\n    close(pb)\n  }\n  \n  return(means)\n}\n\nUna vez creada la función se obtuvieron las predicciones utilizando el siguiente código:\n\nmeanse1 &lt;- wfv_means(data,'Date',id_var_name = \"ID\")\n\n\n\n\n\n\n\nNota\n\n\n\nEn adición a lo expuesto con anterioridad se crearon dos funciones getconfig y plot_modelk, en el archivo .Rprofile del repositorio en el que se encuentra este trabajo, que permiten graficar la estructura de los modelos mediante el uso del paquete Iannone (2023), como se ve en la Figura 17. El código a utilizar sería:\n\n# Las funciones están creadas para graficar las estructuras utilizadas en el presente trabajo.\nmodel |&gt;\n  getconfig() |&gt;\n  plot_modelk() |&gt;\n  grViz()\n\n\n\nEl procedimiento expuesto en las secciones Modelado y Entrenamiento del presente anexo fue repetido para construir los 10 modelos realizados a partir de cada grupo de vectores tridimensionales, sustituyendo en el primer código expuesto la llamada a vecs3d1e por vecs3d2e y vecs3d3e,según el grupo de vectores tridimensionanales utilizado."
  },
  {
    "objectID": "Annex4.html#resultado",
    "href": "Annex4.html#resultado",
    "title": "Anexo. 4 Códigos",
    "section": "Resultado",
    "text": "Resultado\nA continuación, se presenta el código utilizado durante el proceso descrito en los distintos sub-epígrafes del epígrafe Resultado.\n\nPredicciones\nEl análisis expuesto en 2.6.1 Predicciones fue realizado a partir de gráficas (vea Figura 8, Figura 9 y Figura 10), en las que se recogen los valores de los indicadores \\(MSE\\) y \\(R^2\\) para cada una de las estructuras probadas.\nEl primer paso para la obtención de estas gráficas fue el de computar los indicadores, para cada periodo de tiempo, para cada una de las predicciones obtenidas a partir de los distintos modelos construidos con cada estructura. Esto se realizo mediante el siguiente código.\n\n#Extraer salidas reales\nsalidas &lt;- data |&gt;\n  filter(\n    Date &gt; data$Date[1]\n  ) |&gt;\n  select(outputs) |&gt;\n  pull()\nsalidas &lt;- salidas[,,1]\n\n#Computar indicadores MSE y R2\nindicadores &lt;- data |&gt;\n  filter(Date &gt; data$Date[1]) |&gt;\n  cbind(predicciones = predicciones1e[,1]) |&gt;\n  cbind(means = meanse1) |&gt;\n  mutate(salidas = salidas) |&gt;\n  select(Date, predicciones, means, salidas) |&gt;\n  group_by(Date) |&gt;\n  summarise(\n    r2 = 1 - (sum((salidas - predicciones)^2)/sum((salidas - means)^2)),\n    mse = mse(predicciones, salidas),\n  )\n\nLos diferentes indicadores computados para cada uno de las 10 modelos entrenados con cada una de las estructuras fueron guardados en una lista llamada list_indicadores. Esto se realizo utilizando el siguiente código:\n\nlist_indicadores[[\"indicadores1\"]] &lt;-  indicadores\n\nUna vez realizado esto se obtiene una lista que contiene 10 data frames (indicadores1,…,indicadores10), los cuales a su vez contienen los valores de los de \\(MSE\\) y \\(R^2\\) de las predicciones obtenidas por los modelos de RNA para cada una de las empresas agrupados por fecha. Por lo que luego se construyo la gráfica mediante el uso del siguiente código.\n\n# Agrupar la información de las distintas construcciones en un solo data frame\nindi_graf_data &lt;- do.call(cbind,list_indicadores)\n\n# Obtener los resultados medios, para cada periodo de tiempo, usando las distintas construcciones\nindi_graf_data |&gt;\n  rowwise() |&gt;\n  mutate(\n    Date = `indicadores1.Date`,\n    meanmse = mean(c_across(contains(\"mse\"))),\n    meanr2 = mean(c_across(contains(\"r2\")))\n    ) |&gt;\n  select(\n    Date, meanmse,meanr2\n  )|&gt;\n  # Graficar\n  mutate(\n    Date = as.Date(Date)) |&gt;\n  ggplot(aes(x = Date, group = 1)) +\n  geom_line(aes(y = meanmse, color = \"MSE\")) +\n  geom_line(aes(y = meanr2, color = \"R2\")) +\n  scale_color_manual(values = c(\"blue\", \"green\")) +\n  theme(axis.text.x = element_text(angle = 90)) +\n  labs(x = \"Fecha\", y = \"Indicadores\", color = \"Indicadores\")\n\nAdemás de las gráficas se utilzo también en el análisis de los resultados la Tabla 4, en la que se ecuentran las empresas que obtuvieron mejores y peores indicadores para cada estructura, para la obtención de estos datos se uso el siguiente código:\n\nindicadores_X_emp &lt;- data |&gt;\n  filter(Date &gt; data$Date[1]) |&gt;\n  cbind(predicciones = predicciones1e[,1]) |&gt;\n  cbind(means = meanse1) |&gt;\n  mutate(salidas = salidas) |&gt;\n  select(Date, predicciones, means, salidas, ID) |&gt;\n  group_by(ID) |&gt;\n  summarise(\n    r2 = 1 - (sum((salidas - predicciones)^2)/sum((salidas - means)^2)),\n    mse = mse(predicciones, salidas)\n  ) |&gt;\n  select(ID, r2, mse)\n\nAl igual que los indicadores computados por fecha para guardar los indicadores computados por empresa se creo una lista denominada list_indic_emp. Luego de haber almacenado los 10 data frames de indicadores por empresa en la lista se extrajeron las empresas con los mejores y peores resultados mediante el siguiente código:\n\n# Agrupar la información de las distintas construcciones en un solo data frame\nind_emp_t &lt;- do.call(rbind, list_indic_emp)\n\n# Computar los R2 y MSE medios por empresa\nind_emp_t &lt;- ind_emp_t |&gt;\n  group_by(ID) |&gt;\n  summarize(\n    r2 = mean(r2),\n    mse= mean(mse)) |&gt;\n  ungroup() |&gt;\n  arrange(desc(r2))\n\n# Obtener las 10 empresas con mejores y peores indicadores\nmejores10 &lt;- head(ind_emp_t,10)\npeores10 &lt;- tail(ind_emp_t,10)\n\nY mediante la utilización de las variables anteriores y el uso de las funciones rbind() y cbind fue como se creo la Tabla 4.\n\n\nComposición de carteras\nEn esta sección se explica como se realizo el análisis del compratamiento de los resultados obtenidos por las distintas carteras (ver Figura 11, Figura 12 y Figura 13). Para ello es necesario primero obtener la composición de las carteras, por fecha, a partir de las predicciones obtenidas mediante el uso de las medias aritmeticas y los modelos de RNA.\nPara el calculo de la composición de las carteras se uso el paquete de R Berwin A. Turlach R port by Andreas Weingessel &lt;Andreas.Weingessel@ci.tuwien.ac.at&gt; Fortran contributions from Cleve Moler dpodi/LINPACK) (2019), a continuación se muestra el código utilizado para hallar la composición de carteras a partir de las predicciones de la media:\n\n# Se creo un data frame en el que se guardo toda la información:\n#   - Valores del IBEX, como indice de referencia\n#   - Valores de las predicciones, tanto las obtenidas por el modelo de RNA como por las medias aritmeticas\n\nDATA &lt;- data |&gt;\n  left_join(IBEXsel, by =\"Date\") |&gt;\n  mutate(IBEX = Return_I) |&gt;\n  arrange(Date) |&gt;\n  filter(\n    Date &gt; data$Date[1]\n  ) |&gt;\n  mutate(predicciones = predicciones1e[,1]) |&gt;\n  mutate(\n    Real = salidas,\n    RNA = predicciones,\n    Means = meanse1\n  ) |&gt;\n  select(Date, Real, IBEX, RNA, Means, ID)\n\n# A partir del data frame DATA se crearon:\n#    - Un data frame cuyas columnas son los datos reales de cada una de las empresas para cada uno de los periodos de tiempo para los que se obtuvieron predicciones.\n#    - Un data frame cuyas columnas son los datos obtenidos mediante el uso de las medias aritmeticas de cada una de las empresas para cada uno de los periodos de tiempo para los que se obtuvieron predicciones.\n\npvtReal &lt;- DATA |&gt;\n  select(Date, Real, ID) |&gt;\n  pivot_wider(\n    names_from = ID,\n    values_from = Real\n  )\n\npvtMeans &lt;- DATA |&gt;\n  select(Date, Means, ID) |&gt;\n  pivot_wider(\n    names_from = ID,\n    values_from = Means\n  )\n\n# Se creo el data frame en el que se guardara la composición de las carteras para cada uno de los periodos para los que se obtuvo predicción\nweightsm &lt;- data.frame()\n\n# Iteración mediante la cual se halla la composición de las carteras\n\npb &lt;- txtProgressBar(min = 0, max = length(unique(data$Date)[-1]), initial = 0, style = 3)\n\nfor (i in 1:length(unique(data$Date)[-1])) {\n  if(i&gt;1){\n    \n    # Se crea el data frame que comprende los datos a utilizar para hallar la composición de la cartera, este esta creado por los datos reales hasta la fecha y la previsión del siguiente periodo\n    datamQP &lt;- pvtReal |&gt;\n      filter(Date &lt; unique(data$Date)[-1][i]) |&gt;\n      rbind(pvtMeans |&gt;\n              filter(Date == unique(data$Date)[-1][i])\n      )\n    \n    # Elimina aquellas empresas que no tengan ni datos reales o de previsión\n    nare &lt;- which(is.na(datamQP[dim(datamQP)[1],]))\n    naremo &lt;- which(is.na(datamQP[(dim(datamQP)[1]-1),]))\n    nare &lt;- c(nare,naremo)\n    nare &lt;- unique(nare)\n    if(length(nare) != 0){\n      carteram &lt;- datamQP[, - nare]\n    }else{\n      carteram &lt;- datamQP\n    }\n    \n    # Extre las previsiones\n    returnm &lt;- carteram[dim(carteram)[1], -1] |&gt;\n      as.matrix() |&gt;\n      t()\n    \n    # Calcula la matriz de covarianza\n    covmm &lt;- cov(carteram[, -1], use = \"complete.obs\")\n    npcovmm &lt;- nearPD(covmm)$mat |&gt; \n      as.matrix()\n    # Extrae el número de empresas\n    n &lt;- ncol(npcovmm)\n    \n    # Halla la composición de la cartera\n    qp_outm &lt;- solve.QP(\n      Dmat = 2*npcovmm,\n      dvec = rep(0,n),\n      Amat = cbind(-1, diag(n)),\n      bvec = c(-1, rep(0,n)),\n      meq = 1)\n    qp_outm &lt;- qp_outm$solution\n    qp_outm &lt;- floor(qp_outm*100)/100\n    for(j in 1:length(qp_outm)){\n      if(qp_outm[j] &lt; 0.001){\n        qp_outm[j] &lt;- 0\n      }else{}\n    }\n    \n    # Guarda la composición de la cartera\n    names(qp_outm) &lt;- names(carteram[, -1])\n    weightsm &lt;- bind_rows(weightsm, qp_outm)\n  }\n  \n  setTxtProgressBar(pb,i)\n}\n\nclose(pb)\n\n# Sustituir los pesos y observaciones reales con valores faltantes con cero\npvtReal[is.na(pvtReal)] &lt;- 0\nweightsm[is.na(weightsm)] &lt;- 0\n\nLuego para hallar la rentabilidad de la cartera se multiplicaron las composiciones por las rentabilidades reales, se asumio que se invertia uno en el primer periodo y se realizó una sumatoria acumulativa a los largo de los valores para obtener el comportamiento de la rentabilidad a lo largo del tiempo.\n\n# Hallando las rentabilidades de las carteras conformadas a partir de las predicciones de la media aritmetica\n\nreturn_CM &lt;-  weightsm * pvtReal[-1,-1]\nreturn_CM &lt;- rowSums(return_CM)\nreturn_CM &lt;- c(1,return_CM)\nreturn_CM &lt;- data.frame(\n  Date = pvtReal[,1],\n  Mean = return_CM\n)\n\nLos mismos pasos que se realizaron para hallar el comportamiento de la rentabilidad de las carteras a partir de las medias aritmeticas se realizaron para hallar el comportamiento a partir de las predicciones obtenidas por el modelo de RNA como se ve en el código a continuación.\n\n# A partir del data frame DATA se creo un data frame cuyas columnas son los datos obtenidos mediante el uso del model de RNA de cada una de las empresas para cada uno de los periodos de tiempo para los que se obtuvieron predicciones.\n\npvtRNA &lt;- DATA |&gt;\n  select(Date, RNA, ID) |&gt;\n  pivot_wider(\n    names_from = ID,\n    values_from = RNA\n  )\n\n# Se creo el data frame en el que se guardara la composición de las carteras para cada uno de los periodos para los que se obtuvo predicción\nweightse &lt;- data.frame()\n\n# Iteración mediante la cual se halla la composición de las carteras\n\npb &lt;- txtProgressBar(min = 0, max = length(unique(data$Date)[-1]), initial = 0, style = 3)\n\nfor (i in 1:length(unique(data$Date)[-1])) {\n  if(i&gt;1){\n    # Se crea el data frame que comprende los datos a utilizar para hallar la composición de la cartera, este esta creado por los datos reales hasta la fecha y la previsión del siguiente periodo\n    dataeQP &lt;- pvtReal |&gt;\n      filter(Date &lt; unique(data$Date)[-1][i]) |&gt;\n      rbind(pvtRNA |&gt;\n              filter(Date == unique(data$Date)[-1][-1][i])\n            )\n    # Elimina aquellas empresas que no tengan ni datos reales o de previsión\n    nare &lt;- which(is.na(dataeQP[dim(dataeQP)[1],]))\n    naremo &lt;- which(is.na(dataeQP[(dim(dataeQP)[1]-1),]))\n    nare &lt;- c(nare,naremo)\n    nare &lt;- unique(nare)\n    if(length(nare) != 0){\n      carterae &lt;- dataeQP[, - nare]\n    }else{\n      carterae &lt;- dataeQP\n    }\n    \n    # Extre las previsiones\n    returne &lt;- carterae[dim(carterae)[1], -1] |&gt;\n      as.matrix() |&gt;\n      t()\n    \n    # Calcula la matriz de covarianza\n    covme &lt;- cov(carterae[, -1], use = \"complete.obs\")\n    npcovme &lt;- nearPD(covme)$mat |&gt; \n      as.matrix()\n    # Extrae el número de empresas\n    n &lt;- ncol(npcovme)\n    \n    # Halla la composición de la cartera\n    qp_oute &lt;- solve.QP(\n      Dmat = 2*npcovme,\n      dvec = rep(0,n),\n      Amat = cbind(-1, diag(n)),\n      bvec = c(-1, rep(0,n)),\n      meq = 1)\n    qp_oute &lt;- qp_oute$solution\n    qp_oute &lt;- floor(qp_oute*100)/100\n    for(j in 1:length(qp_oute)){\n      if(qp_oute[j] &lt; 0.001){\n        qp_oute[j] &lt;- 0\n      }else{}\n    }\n    \n    # Guarda la composición de la cartera\n    names(qp_oute) &lt;- names(carterae[, -1])\n    weightse &lt;- bind_rows(weightse, qp_oute)\n  }\n  \n  setTxtProgressBar(pb,i)\n}\n\nclose(pb)\n\n# Sustituir los pesos con valores faltantes con cero\nweightse[is.na(weightse)] &lt;- 0\n\nLuego para hallar la rentabilidad de la cartera se multiplicaron las composiciones por las rentabilidades reales, se asumio que se invertia uno en el primer periodo y se realizó una sumatoria acumulativa a los largo de los valores para obtener el comportamiento de la rentabilidad a lo largo del tiempo.\n\n# Hallando las rentabilidades de las carteras conformadas a partir de las predicciones del modelo RNA\n\nreturn_CRNA &lt;-  weightse * pvtReal[-1,-1]\nreturn_CRNA &lt;- rowSums(return_CRNA)\nreturn_CRNA &lt;- c(1,return_CRNA)\nreturn_CRNA &lt;- data.frame(\n  Date = pvtReal[,1],\n  RNA = return_CRNA\n)\n\nLuego, al igual que con los indicadores se creo un lista list_ret_RNA en la que se guardaron los data frames de los distintos modelos construidos con cada una de las estructuras. Después se ejecutó el siguiente código para obtener la gráfica.\n\n# Hallando el comportamiento de las rentabilidades del IBEX para el periodo\n\nIBEXvals &lt;- IBEXsel |&gt;\n    filter(Date &gt; unique(data$Date)[2]) |&gt;\n    select(2) |&gt;\n    pull()\nIBEXvals &lt;- c(1, IBEXvals)\n\ndata_rent_RNA &lt;- do.call(cbind,list_ret_RNA)\ndata_rent_RNA &lt;- data_rent_RNA |&gt;\n  mutate(\n    Date = RNA1.Date,\n    IBEX = IBEXvals,\n    Means = return_CM$Mean) |&gt;\n  mutate_at(vars(contains(\".RNA\")), ~ cumsum(.)) |&gt;\n  mutate(\n    IBEX = cumsum(IBEX),\n    Means = cumsum(Means)) |&gt;\n  group_by(Date) |&gt;\n  summarize(\n    meanRNA = mean(c_across(contains(\".RNA\"))),\n    max_y = max(c_across(contains(\".RNA\"))),\n    min_y = min(c_across(contains(\".RNA\"))),\n    min_5 = unname(quantile(c_across(contains(\".RNA\")),0.05)),\n    max_95 = unname(quantile(c_across(contains(\".RNA\")),0.95)),\n    IBEX = IBEX,\n    Means = Means)\ndata_rent_RNA |&gt;\n  mutate(\n    Date = as.Date(Date)) |&gt;\nggplot(aes(x = Date)) +\n  geom_ribbon(aes(ymin = min_y, ymax = min_5), fill = \"blue\", alpha = 0.3) +\n  geom_ribbon(aes(ymin = max_y, ymax = max_95), fill = \"blue\", alpha = 0.3) +\n  geom_ribbon(aes(ymin = min_5, ymax = max_95), fill = \"blue\", alpha = 0.6) +\n  geom_line(\n    aes(y = meanRNA, color = \"Media RNA1\"),\n    linetype = \"dashed\") +\n  geom_line(aes(y = max_y), color = \"blue\") +\n  geom_line(aes(y = min_y), color = \"blue\") +\n  geom_line(aes(y = max_95), color = \"blue\") +\n  geom_line(aes(y = min_5, color = \"RNA1\")) +\n  geom_line(aes(y = IBEX, color = \"IBEX\")) +\n  geom_line(aes(y = Means, color = \"Medias\")) +\n  scale_color_manual(\n    values = c(\n      \"Media RNA1\"=\"blue\",\n      \"RNA1\" = \"blue\",\n      \"IBEX\" = \"red\",\n      \"Medias\" = \"green\")) +\n  guides(\n    color = guide_legend(\n      override.aes = list(\n        linetype = c(\"solid\",\"dashed\",\"solid\",\"solid\"))))+\n  labs(x = \"Fecha\",\n       y = \"Rentabilidades\",\n       color = \"Leyenda\")+\n  theme_minimal()\n\n\n\n\n\nBerwin A. Turlach R port by Andreas Weingessel &lt;Andreas.Weingessel@ci.tuwien.ac.at&gt; Fortran contributions from Cleve Moler dpodi/LINPACK), S original by. 2019. quadprog: Functions to Solve Quadratic Programming Problems. https://CRAN.R-project.org/package=quadprog.\n\n\nIannone, Richard. 2023. DiagrammeR: Graph/Network Visualization. https://CRAN.R-project.org/package=DiagrammeR.\n\n\nRyan, Jeffrey A., y Joshua M. Ulrich. 2023. quantmod: Quantitative Financial Modelling Framework. https://CRAN.R-project.org/package=quantmod."
  }
]