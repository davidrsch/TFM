<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Final Master’s Project - 2.2 Artificial neural networks in the forecast of time series</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./PC.html" rel="next">
<link href="./FSandP.html" rel="prev">
<link href="./../icologo.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner {
        background: #D60D8C;
      }
</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../my_style.css">
</head>

<body class="nav-sidebar floating">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./body.html">2 Work development</a></li><li class="breadcrumb-item"><a href="./ANNinTSF.html">2.2 Artificial neural networks in the forecast of time series</a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">2.2 Artificial neural networks in the forecast of time series</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Final Master’s Project</a> 
        <div class="sidebar-tools-main tools-wide">
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label=""><i class="bi bi-translate"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./../index.html">
            Español
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="./../gal/index.html">
            Galego
            </a>
          </li>
      </ul>
    </div>
    <a href="https://github.com/davidrsch/TFM" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="https://twitter.com/intent/tweet?url=|url|" title="Twitter" class="quarto-navigation-tool px-1" aria-label="Twitter"><i class="bi bi-twitter"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Description</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./greetings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Thanks</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summaryen.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Abstract</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summaryes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Resumen</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summarygal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Resumo</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">1 Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./body.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2 Work development</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./FSandP.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.1 Characterization of financial time series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ANNinTSF.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">2.2 Artificial neural networks in the forecast of time series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./PC.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.3 Portfolio composition</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.4 Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./MandT.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.5 Modeling and training</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Results.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">2.6 Result</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./conclusions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">3 Conclusions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">Annex</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Annex1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Annex. 1 Figures</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Annex2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Annex. 2 Graphics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Annex3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Annex. 3 Tables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Annex4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Annex. 4 Codes</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#background-on-the-use-of-artificial-neural-networks-in-time-series-forecasting" id="toc-background-on-the-use-of-artificial-neural-networks-in-time-series-forecasting" class="nav-link active" data-scroll-target="#background-on-the-use-of-artificial-neural-networks-in-time-series-forecasting">2.2.1 Background on the use of artificial neural networks in time series forecasting</a></li>
  <li><a href="#convolutional-neural-networks" id="toc-convolutional-neural-networks" class="nav-link" data-scroll-target="#convolutional-neural-networks">2.2.2 Convolutional Neural Networks</a></li>
  <li><a href="#long-short-term-memory" id="toc-long-short-term-memory" class="nav-link" data-scroll-target="#long-short-term-memory">2.2.3 Long short-term memory</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/davidrsch/TFM/edit/main/ANNinTSF.qmd" class="toc-action">Edit this page</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>This epigraph is divided into three sub-headings. The first deals with the background to the use of artificial neural networks for working with time series, more specifically in forecasting. In the second and third sub-headings, the operation of two of the RNA layer structures used in this work are exposed, these being the CNN and the LSTM.</p>
<section id="background-on-the-use-of-artificial-neural-networks-in-time-series-forecasting" class="level2">
<h2 class="anchored" data-anchor-id="background-on-the-use-of-artificial-neural-networks-in-time-series-forecasting">2.2.1 Background on the use of artificial neural networks in time series forecasting</h2>
<p>In <span class="citation" data-cites="chollet2018deep">Chollet and Allaire (<a href="#ref-chollet2018deep" role="doc-biblioref">2018</a>)</span> it is stated that the ANN environment is made up of artificial intelligence (hereinafter IA), machine learning or automated learning (hereinafter ML) and deep learning or deep learning (hereinafter DL), <span class="citation" data-cites="fig">(<a href="#ref-fig" role="doc-biblioref"><strong>fig?</strong></a>)</span> -DLenv. Therefore, it is of vital importance to know the aspects of these fields that are closely related to ANN and that are briefly explained below.</p>
<p>“Making a machine behave in such a way that a human would be called intelligent” (<span class="citation" data-cites="McCarthy_Minsky_Rochester_Shannon_2006">McCarthy et al. (<a href="#ref-McCarthy_Minsky_Rochester_Shannon_2006" role="doc-biblioref">2006</a>)</span>, p.11) is the first definition given to the AI ​​problem. With the aim of solving this problem, the first AI emerged, the so-called symbolic AI.</p>
<p>As explained by <span class="citation" data-cites="haykin1998neural">Haykin (<a href="#ref-haykin1998neural" role="doc-biblioref">1998</a>)</span>, <span class="citation" data-cites="banda2014">Banda (<a href="#ref-banda2014" role="doc-biblioref">2014</a>)</span> and <span class="citation" data-cites="chollet2018deep">Chollet and Allaire (<a href="#ref-chollet2018deep" role="doc-biblioref">2018</a>)</span>, these early AIs involved hardcoded rules created by programmers. With the aim of achieving that these rules were automatically learned by the machines when observing the data, a new stage emerged in the development of AI, the so-called ML. This new stage gives rise to the emergence of a new form of programming, differentiating from the classic, in that, in this, the programmers introduce the data and the expected responses to them, and the computers are capable of generating the rules, <a href="#fig-MLprog">Figure&nbsp;<span class="quarto-unresolved-ref">fig-MLprog</span></a>.</p>
<p>So it is understood that ML models try to find appropriate representations for your input data: transformations of the data that make it more amenable to the task at hand. In DL, which is a specific sub-field of ML, these data representations are modeled through architectures composed of successive layers, which are called RNA <span class="citation" data-cites="chollet2018deep">Chollet and Allaire (<a href="#ref-chollet2018deep" role="doc-biblioref">2018</a>)</span>.</p>
<p>After studying what was exposed in <span class="citation" data-cites="haykin1998neural">Haykin (<a href="#ref-haykin1998neural" role="doc-biblioref">1998</a>)</span>, <span class="citation" data-cites="Larranaga07">Larrañaga (<a href="#ref-Larranaga07" role="doc-biblioref">2007</a>)</span>, <span class="citation" data-cites="banda2014">Banda (<a href="#ref-banda2014" role="doc-biblioref">2014</a>)</span> and <span class="citation" data-cites="chollet2018deep">Chollet and Allaire (<a href="#ref-chollet2018deep" role="doc-biblioref">2018</a>)</span> about ANN, it can be affirmed that they are inspired by the functioning of the human brain, these texts confirm and agree that three types of ANN can be distinguished layers: input, output and hidden. An input layer is composed of neurons that receive the input vectors. An output layer is made up of neurons that, during training, receive the output vectors and then generate the response. A hidden layer is connected to the environment through the input and output layers, this type of hidden layer processes the received input to obtain the corresponding output, <a href="#fig-RNAstruct">Figure&nbsp;<span class="quarto-unresolved-ref">fig-RNAstruct</span></a>.</p>
<p>One of the applications of ANN is the forecasting of time series. whose objective is to predict the future values ​​of variables based on their past observations. As discussed previously, financial time series are often nonlinear, noisy, chaotic, and nonstationary, making them difficult to model and forecast. ANNs have the advantage of being able to capture complex nonlinear relationships and adapt to changing conditions without requiring prior assumptions about the distribution or structure of the data.</p>
<p>The history of ANNs in financial time series forecasting dates back to the late 1980s and early 1990s, when researchers began to explore the potential of ANNs as an alternative to traditional statistical methods, such as the integrated autoregressive moving average model, better known as ARIMA (Autoregressive Integrated Moving Average) and generalized autoregressive models with conditional heteroskedasticity, better known as GARCH (Generalized Autoregressive Conditional Heteroskedasticity). ANNs were shown to have several advantages over these methods, such as the ability to capture non-linear and dynamic relationships, handle noisy and incomplete data, and adapt to changing market conditions (<span class="citation" data-cites="ZHANG199835">B. Eddy Patuwo &amp; Michael Y. Hu (<a href="#ref-ZHANG199835" role="doc-biblioref">1998</a>)</span>).</p>
<p>However, ANNs also face some limitations and challenges in financial time series forecasting, such as the difficulty of choosing a suitable network architecture, training algorithm, activation function, and input variables; the risk of overfitting and generalization problems; the lack of interpretability and transparency; and the high computational cost and time (<span class="citation" data-cites="TEALAB2018334">Tealab (<a href="#ref-TEALAB2018334" role="doc-biblioref">2018</a>)</span>).</p>
<p>To overcome these limitations and challenges, researchers have proposed several enhancements and extensions to ANN for financial time series forecasting in recent decades. Some of the major developments include:</p>
<ul>
<li><p>The use of hybrid models that combine ANN with other techniques such as fuzzy logic, genetic algorithms, wavelet analysis, support vector machines, and deep learning to improve ANN performance and robustness (<span class="citation" data-cites="wongguo2010">Wong and Guo (<a href="#ref-wongguo2010" role="doc-biblioref">2010</a>)</span>).</p></li>
<li><p>The use of recurrent neural networks (hereinafter RNR) or bidirectional, which are a special type of ANN that can process sequential data and capture temporal dependencies. RNRs have been shown to outperform unidirectional neural networks in complex and non-linear time series (<span class="citation" data-cites="GURESEN201110389">Guresen, Kayakutlu, and Daim (<a href="#ref-GURESEN201110389" role="doc-biblioref">2011</a>)</span>).</p></li>
<li><p>The use of more complex RNA models by combining different layers, such as convolutional neural networks (hereinafter, CNN), long short-term memory (hereinafter, LSTM), gated recurrent units (hereinafter GRU) have been applied to financial time series forecasting with promising results (<span class="citation" data-cites="SEZER2020106181">Sezer, Gudelek, and Ozbayoglu (<a href="#ref-SEZER2020106181" role="doc-biblioref">2020</a>)</span>).</p></li>
</ul>
<p>The history of ANNs in financial time series forecasting shows that ANNs have evolved and improved over time to cope with the complexity and uncertainty of financial markets. However, some of the previously mentioned challenges and limitations still persist, such as overfitting, generalization, interpretability, robustness, and computational cost.</p>
</section>
<section id="convolutional-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="convolutional-neural-networks">2.2.2 Convolutional Neural Networks</h2>
<p>The RNA model used in this work is composed of several layers, the most important being the Conv1D layer, a specific type of CNN, and the LSTM layer, both mentioned in the previous subsection when the ANN structures that most used today. This subsection focuses on the Conv1D Layer, so the fundamental concepts to understand its operation are explored, explaining convolution, convolutional neural networks and Conv1D and their use for time series analysis. An overview of convolution and how it can be applied to time series data is provided. Then, CNNs and their architecture, which allows them to automatically learn features from time series data, are discussed. Finally, Conv1D, a specific type of convolutional neural network layer that is particularly effective for processing time series data, is explained.</p>
<p>As discussed in <span class="citation" data-cites="rafid23">Siddiqui (<a href="#ref-rafid23" role="doc-biblioref">2023</a>)</span>, convolution is a mathematical operation that is commonly used in signal processing and image analysis. It involves taking two functions and producing a third function that represents how one of the original functions modifies the other. In the context of time series data, convolution can be used to extract features from the data by applying a filter to the time series.</p>
<p>In addition to extracting features from time series data, convolution can also be used for other tasks such as noise reduction, anomaly detection, and prediction. For example, a CNN can be trained to predict future values of a time series by learning the underlying patterns in the data. In general, convolution is a powerful tool for analyzing time series data and its applications are numerous <span class="citation" data-cites="rafid23">Siddiqui (<a href="#ref-rafid23" role="doc-biblioref">2023</a>)</span>.</p>
<p>CNNs were first introduced at <span class="citation" data-cites="cnn">Lecun et al. (<a href="#ref-cnn" role="doc-biblioref">1998</a>)</span> and are a type of deep learning model that is commonly used for image analysis. However, as previously mentioned, they can also be used for time series analysis, as they are well-suited for learning features from data that have a spatial or temporal structure.</p>
<p>The architecture of a CNN consists of one or more convolutional layers, which apply filters to the input data to extract features. Each filter is a set of weights that are learned during the training process. By sliding the filter over the input data, the convolutional layer computes a dot product at each position, producing a new <span class="citation" data-cites="cnn">Lecun et al. (<a href="#ref-cnn" role="doc-biblioref">1998</a>)</span> feature map.</p>
<p>In a time series context, a CNN can learn to automatically extract features from data at different scales and time intervals, making it a powerful tool for time series analysis. A key advantage of using a CNN for time series analysis is that it reduces the need for manual feature engineering. Instead of designing filters by hand, CNN learns to automatically extract features from the data, making it more flexible and adaptable to different types of time series data.</p>
<p>In general, the architecture of a CNN allows it to automatically learn features from time series data, making it a powerful tool for time series analysis, with Conv1D being one of the most widely used CNN structures for this task.</p>
<p>As explained in <span class="citation" data-cites="hongj20">Jing (<a href="#ref-hongj20" role="doc-biblioref">2020</a>)</span> Conv1D is a specific type of CNN layer that is designed to process one-dimensional data, such as time series data. While traditional CNNs are designed to process two-dimensional data, Conv1D is specifically optimized for one-dimensional data, making it more efficient and effective for time series analysis.</p>
<p>The architecture of a Conv1D layer is similar to that of a traditional CNN, but with some key differences. Instead of using two-dimensional filters, Conv1D uses one-dimensional filters, which are applied to the input time series to extract features. The features that are extracted from the string will depend on the different configurations used for the filter configuration and the number of filters used, being the following formula to calculate the amount of feature that each filter extracts: <a href="#eq-cnn-lout">Equation&nbsp;<span class="quarto-unresolved-ref">eq-cnn-lout</span></a> (<span class="citation" data-cites="hongj20">Jing (<a href="#ref-hongj20" role="doc-biblioref">2020</a>)</span>):</p>
<p><span id="eq-cnn-lout"><span class="math display">\[
\begin{aligned}
L_{out} &amp;= \frac{L_{in} + 2*padding - dilation*(kerenel\_size - 1)-1}{stride} + 1 \\
\end{aligned}
\tag{1}\]</span></span></p>
<p>Where:</p>
<div class="margin">
<p><em>Lout</em>: is the length of the output of the filtering process or the number of features.</p>
</div>
<div class="margin">
<p><em>Lin</em>: the length of the input vector, corresponding in time series analysis to the number of observations that contain the samples of the time series that are passed to the filter.</p>
</div>
<div class="margin">
<p><em>kernel_size</em>: is the size of the filter, which defines how many observations of the input vector are passed to the filter each time. <a href="#fig-HJks">Figure&nbsp;<span class="quarto-unresolved-ref">fig-HJks</span></a> represents how the size of the filter can affect the length of the output vector.</p>
</div>
<div class="margin">
<p><em>stride</em>: represents the number of steps or observations by which the selection of observations passed to the filter is moved. <a href="#fig-HJstride">Figure&nbsp;<span class="quarto-unresolved-ref">fig-HJstride</span></a> represents how the stride parameter can affect the length of the output vector.</p>
</div>
<div class="margin">
<p><em>dilation</em>: is the distance of the observations that pass the filter. <a href="#fig-HJdilation">Figure&nbsp;<span class="quarto-unresolved-ref">fig-HJdilation</span></a> represents how the dilation parameter can affect the length of the output vector.</p>
</div>
<div class="margin">
<p><em>padding</em>: represents the number of zeros to add to each end of the vector. <a href="#fig-HJpadding">Figure&nbsp;<span class="quarto-unresolved-ref">fig-HJpadding</span></a> represents how the padding parameter can affect the length of the output vector.</p>
</div>
<p>Overall, Conv1D is a powerful tool for processing time series data, and its advantages include computational efficiency and the ability to capture time dependencies in the data. Its use cases are numerous and span different fields, making it a valuable tool for time series analysis.</p>
</section>
<section id="long-short-term-memory" class="level2">
<h2 class="anchored" data-anchor-id="long-short-term-memory">2.2.3 Long short-term memory</h2>
<p>This subsection explains why LSTMs are one of the most widely used ANN structures in time series forecasting, based on a brief explanation of RNRs and why they are useful in solving series forecasting problems. of time, delving into why LSTMs differ from the rest of the RNNs, and the operation of each of the layers that make up the structure of an LSTM layer.</p>
<p><span class="citation" data-cites="COlah15">Olah (<a href="#ref-COlah15" role="doc-biblioref">2015</a>)</span> explains that an RNN can be considered as multiple copies of the same network, <a href="#fig-CORNRstruct">Figure&nbsp;<span class="quarto-unresolved-ref">fig-CORNRstruct</span></a>, states that this aspect reveals that RNRs are intimately related to sequences and lists, which makes this type of RNA the one that naturally used for work with time series.</p>
<p>Conventional RNRs present a problem in relation to the ability to retain information, as explained by <span class="citation" data-cites="COlah15">Olah (<a href="#ref-COlah15" role="doc-biblioref">2015</a>)</span>, standard RNNs perform with great capacity only if the information relevant to the current situation is recent, that is, where the gap between the relevant information and where it is needed is small, <a href="#fig-CORInclose">Figure&nbsp;<span class="quarto-unresolved-ref">fig-CORInclose</span></a>; further exposes that as the gap grows, standard RNNs are unable to access the relevant information, <a href="#fig-CORInaway">Figure&nbsp;<span class="quarto-unresolved-ref">fig-CORInaway</span></a>.</p>
<p>As previously mentioned, LSTMs are a type of RNR that can learn long-term dependencies on sequential data. These were proposed in <span class="citation" data-cites="SeppJur97">Hochreiter (<a href="#ref-SeppJur97" role="doc-biblioref">1997</a>)</span> and have been widely used for various tasks such as language modeling, speech recognition, machine translation, image description, and time series forecasting.</p>
<p>The main idea of LSTM is to introduce a memory cell that can store and update information over long steps of time. The memory cell is controlled by three gates: an entry gate, a forget gate, and an exit gate. These gates are neural networks that learn to regulate the flow of information in and out of the cell <a href="#fig-CODrnrlstm">Figure&nbsp;<span class="quarto-unresolved-ref">fig-CODrnrlstm</span></a>.</p>
<p>The input gate decides how much of the new input to add to the cell state. The forget gate decides which part of the previous cell state to keep or delete. The output gate decides which part of the current cell state is to be sent to the next layer. <span class="citation" data-cites="COlah15">Olah (<a href="#ref-COlah15" role="doc-biblioref">2015</a>)</span> based on what was exposed in <span class="citation" data-cites="SeppJur97">Hochreiter (<a href="#ref-SeppJur97" role="doc-biblioref">1997</a>)</span>, describes the operation of the doors in four steps:</p>
<ol type="1">
<li>Deciding which cell state information is forgotten through the gate, forget gate layer <span class="math inline">\(f_t\)</span>. This gate looks at <span class="math inline">\(h_{t-1}\)</span>, hidden state from the previous time period, and <span class="math inline">\(x_{t}\)</span>, input from the current time instant, and outputs a number between 0 (undo) and 1 (hold). for each number in cell state <span class="math inline">\(C_{t-1}\)</span>, <a href="#fig-COLSTMstep1">Figure&nbsp;<span class="quarto-unresolved-ref">fig-COLSTMstep1</span></a>, <a href="#eq-lstm-fstep">Equation&nbsp;<span class="quarto-unresolved-ref">eq-lstm-fstep</span></a>.</li>
</ol>
<p><span id="eq-lstm-fstep"><span class="math display">\[
\begin{aligned}
f_t &amp;= \sigma(W_f [h_{t-1}, x_t] + b_f) \\
\end{aligned}
\tag{2}\]</span></span></p>
<ol start="2" type="1">
<li>Decide what new information is stored in the cell state. For this first the input gate layer decides which values to update and then a tanh (hyperbolic tangent) layer creates a vector of new candidate values (<span class="math inline">\(\tilde{C}_t\)</span>) that could be added to the state, <a href="#fig-COLSTMstep2">Figure&nbsp;<span class="quarto-unresolved-ref">fig-COLSTMstep2</span></a>, <a href="#eq-lstm-sstepf">Equation&nbsp;<span class="quarto-unresolved-ref">eq-lstm-sstepf</span></a> y <a href="#eq-lstm-ssteps">Equation&nbsp;<span class="quarto-unresolved-ref">eq-lstm-ssteps</span></a>.</li>
</ol>
<p><span id="eq-lstm-sstepf"><span class="math display">\[
\begin{aligned}
i_t &amp;= \sigma(W_i [h_{t-1}, x_t] + b_i) \\
\end{aligned}
\tag{3}\]</span></span></p>
<p><span id="eq-lstm-ssteps"><span class="math display">\[
\begin{aligned}
\tilde{C}_t &amp;= tanh(W_c [h_{t-1}, x_t] + b_c) \\
\end{aligned}
\tag{4}\]</span></span></p>
<ol start="3" type="1">
<li>The state of the old cell, <span class="math inline">\(C_{t-1}\)</span>, is updated to the new state of cell <span class="math inline">\(C_{t}\)</span>. Multiply the previous state by <span class="math inline">\(f_{t}\)</span>, forgetting what is necessary, then add <span class="math inline">\(i_{t} * \tilde{C}_{t}\)</span>. These are the new candidate values, scaled by how much each status value needs to be updated, <a href="#fig-COLSTMstep3">Figure&nbsp;<span class="quarto-unresolved-ref">fig-COLSTMstep3</span></a>, <a href="#eq-lstm-tstep">Equation&nbsp;<span class="quarto-unresolved-ref">eq-lstm-tstep</span></a>.</li>
</ol>
<p><span id="eq-lstm-tstep"><span class="math display">\[
\begin{aligned}
C_t &amp;= f_t * C_{t-1} + i_t * \tilde{C}_t  \\
\end{aligned}
\tag{5}\]</span></span></p>
<ol start="4" type="1">
<li>An output is generated based on the cell state. Running first a sigmoid layer that decides what parts of the cell state is the output; then the state of the cell is passed through a tanh function (scaling the values between −1 and 1) and multiplied by the output gate, output gate, <a href="#fig-COLSTMstep4">Figure&nbsp;<span class="quarto-unresolved-ref">fig-COLSTMstep4</span></a>, <a href="#eq-lstm-fstepf">Equation&nbsp;<span class="quarto-unresolved-ref">eq-lstm-fstepf</span></a> y <a href="#eq-lstm-fsteps">Equation&nbsp;<span class="quarto-unresolved-ref">eq-lstm-fsteps</span></a>.</li>
</ol>
<p><span id="eq-lstm-fstepf"><span class="math display">\[
\begin{aligned}
o_t &amp;= \sigma(W_o [h_{t-1}, x_t] + b_o) \\
\end{aligned}
\tag{6}\]</span></span> <span id="eq-lstm-fsteps"><span class="math display">\[
\begin{aligned}
h_t &amp;= o_t * tanh(C_t) \\
\end{aligned}
\tag{7}\]</span></span></p>
<p>LSTMs can learn to capture long-term dependencies by tuning gate values through back propagation. For example, if a certain input is relevant to a later exit, the input gate will learn to let it in, and the forgotten gate will learn to hold it in the cell state until it is needed. Conversely, if an input is irrelevant or stale, the gateway will learn to ignore it, and the forgotten gate will learn to remove it from the cell state.</p>


<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-ZHANG199835" class="csl-entry" role="listitem">
B. Eddy Patuwo &amp; Michael Y. Hu, Guoqiang Zhang &amp;. 1998. <span>“Forecasting with Artificial Neural Networks:: The State of the Art.”</span> <em>International Journal of Forecasting</em> 14 (1): 35–62. https://doi.org/<a href="https://doi.org/10.1016/S0169-2070(97)00044-7">https://doi.org/10.1016/S0169-2070(97)00044-7</a>.
</div>
<div id="ref-banda2014" class="csl-entry" role="listitem">
Banda, Hugo. 2014. <em>Inteligencia Artificial: Principios y Aplicaciones</em>. Quito, Ecuador: Escuela Politécnica Nacional.
</div>
<div id="ref-chollet2018deep" class="csl-entry" role="listitem">
Chollet, F., and J. J. Allaire. 2018. <em>Deep Learning with r</em>. Manning Publications. <a href="https://books.google.es/books?id=xnIRtAEACAAJ">https://books.google.es/books?id=xnIRtAEACAAJ</a>.
</div>
<div id="ref-GURESEN201110389" class="csl-entry" role="listitem">
Guresen, Erkam, Gulgun Kayakutlu, and Tugrul U. Daim. 2011. <span>“Using Artificial Neural Network Models in Stock Market Index Prediction.”</span> <em>Expert Systems with Applications</em> 38 (8): 10389–97. https://doi.org/<a href="https://doi.org/10.1016/j.eswa.2011.02.068">https://doi.org/10.1016/j.eswa.2011.02.068</a>.
</div>
<div id="ref-haykin1998neural" class="csl-entry" role="listitem">
Haykin, Simon. 1998. <em>Neural Networks: A Comprehensive Foundation</em>. Prentice Hall PTR.
</div>
<div id="ref-SeppJur97" class="csl-entry" role="listitem">
Hochreiter, Jürgen, Sepp &amp; Schmidhuber. 1997. <span>“<span>Long Short-Term Memory</span>.”</span> <em>Neural Computation</em> 9 (8): 1735–80. <a href="https://doi.org/10.1162/neco.1997.9.8.1735">https://doi.org/10.1162/neco.1997.9.8.1735</a>.
</div>
<div id="ref-hongj20" class="csl-entry" role="listitem">
Jing, Hong. 2020. <span>“How Convolutional Layers Work in Deep Learning Neural Networks?”</span> Jingles, Github Blog. 2020. <a href="https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/">https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/</a>.
</div>
<div id="ref-Larranaga07" class="csl-entry" role="listitem">
Larrañaga, Iñaki &amp; Moujahid, Pedro &amp; Inza. 2007. <span>“Tema 14. Redes Neuronales.”</span> Departamento de Ciencias de la Computaci´on e Inteligencia Artificial, Universidad del Pa´ıs Vasco–Euskal Herriko Unibertsitatea. 2007. <a href="http://www.sc.ehu.es/ccwbayes/docencia/mmcc/docs/t14-neuronales.pdf">http://www.sc.ehu.es/ccwbayes/docencia/mmcc/docs/t14-neuronales.pdf</a>.
</div>
<div id="ref-cnn" class="csl-entry" role="listitem">
Lecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998. <span>“Gradient-Based Learning Applied to Document Recognition.”</span> <em>Proceedings of the IEEE</em> 86 (11): 2278–2324. <a href="https://doi.org/10.1109/5.726791">https://doi.org/10.1109/5.726791</a>.
</div>
<div id="ref-McCarthy_Minsky_Rochester_Shannon_2006" class="csl-entry" role="listitem">
McCarthy, John, Marvin L. Minsky, Nathaniel Rochester, and Claude E. Shannon. 2006. <span>“A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955.”</span> <em>AI Magazine</em> 27 (4): 12. <a href="https://doi.org/10.1609/aimag.v27i4.1904">https://doi.org/10.1609/aimag.v27i4.1904</a>.
</div>
<div id="ref-COlah15" class="csl-entry" role="listitem">
Olah, Christopher. 2015. <span>“Understanding LSTM Networks.”</span> Colah’s blog. 2015. <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>.
</div>
<div id="ref-SEZER2020106181" class="csl-entry" role="listitem">
Sezer, Omer Berat, Mehmet Ugur Gudelek, and Ahmet Murat Ozbayoglu. 2020. <span>“Financial Time Series Forecasting with Deep Learning : A Systematic Literature Review: 2005–2019.”</span> <em>Applied Soft Computing</em> 90: 106181. https://doi.org/<a href="https://doi.org/10.1016/j.asoc.2020.106181">https://doi.org/10.1016/j.asoc.2020.106181</a>.
</div>
<div id="ref-rafid23" class="csl-entry" role="listitem">
Siddiqui, J. Rafid. 2023. <span>“Why Convolve? Understanding Convolution and Feature Extraction in Deep Networks.”</span> Medium, Towards Data Science. 2023. <a href="https://towardsdatascience.com/why-convolve-understanding-convolution-and-feature-extraction-in-deep-networks-ee45d1fdd17c">https://towardsdatascience.com/why-convolve-understanding-convolution-and-feature-extraction-in-deep-networks-ee45d1fdd17c</a>.
</div>
<div id="ref-TEALAB2018334" class="csl-entry" role="listitem">
Tealab, Ahmed. 2018. <span>“Time Series Forecasting Using Artificial Neural Networks Methodologies: A Systematic Review.”</span> <em>Future Computing and Informatics Journal</em> 3 (2): 334–40. https://doi.org/<a href="https://doi.org/10.1016/j.fcij.2018.10.003">https://doi.org/10.1016/j.fcij.2018.10.003</a>.
</div>
<div id="ref-wongguo2010" class="csl-entry" role="listitem">
Wong, W. K., and Z. X. Guo. 2010. <span>“<span class="nocase">A hybrid intelligent model for medium-term sales forecasting in fashion retail supply chains using extreme learning machine and harmony search algorithm</span>.”</span> <em>International Journal of Production Economics</em> 128 (2): 614–24. <a href="https://ideas.repec.org/a/eee/proeco/v128y2010i2p614-624.html">https://ideas.repec.org/a/eee/proeco/v128y2010i2p614-624.html</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script type="text/javascript">

  const elementWithauthor = document.querySelector('.quarto-title-meta-author');
  
  if(elementWithauthor){
    
    //Ad more description in title banner
    let TFMtitle = document.getElementById('title-block-header');
    let TFMtitlefc = TFMtitle.firstElementChild;
    let TFMtitlefcfc = TFMtitlefc.firstElementChild;
    const sublead = document.createElement('p');
      sublead.className = 'subtitle lead';
      sublead.id = 'sublead';
      sublead.textContent = "Master's Degree in Banking and Finance Academic year 2022/2023";
    const sub_sublead = document.createElement('p');
      sub_sublead.className = 'subtitle lead';
      sub_sublead.id='sub_sublead';
      sub_sublead.textContent = "Final Master's Project presented at the Faculty of Economics and Business of the University of A Coruña to obtain the Master's Degree in Banking and Finance";
    const udclogo = document.createElement('p');
      udclogo.innerHTML='<img src="../udclogo.png" style="width: 25%;">'
    TFMtitlefcfc.appendChild(sublead);
    TFMtitlefcfc.appendChild(sub_sublead);
    TFMtitlefcfc.appendChild(udclogo);
    
    //Fix authoring info
    const authorsElement = document.querySelector('.quarto-title-meta-heading');
    authorsElement.textContent = "AUTHOR";
    const authorsInfo = authorsElement.parentElement;
    const tutorname = authorsElement.cloneNode(true);
    tutorname.textContent = "TUTOR";
    tutorname.style.marginTop = '0';
    const tutorafili = tutorname.cloneNode(true);
    tutorafili.textContent = "";
    authorsInfo.insertBefore(tutorafili, authorsInfo.children[4]);
    authorsInfo.insertBefore(tutorname, authorsInfo.children[4]);
    
  }else{
    let TFMtitle = document.getElementById('title-block-header');
    let TFMtitlefc = TFMtitle.firstElementChild;
    let TFMtitlefcfc = TFMtitlefc.firstElementChild;
    const sub_sublead = document.createElement('p');
      sub_sublead.className = 'subtitle lead';
      sub_sublead.id='sub_sublead';
      sub_sublead.textContent = 'Application of artificial neural networks and quadratic programming in portfolio management';
    TFMtitlefcfc.appendChild(sub_sublead);
    
    
    
  }
</script>

<script>

// Read the HTML content from a file
async function readHTMLFile(filePath) {
  try {
    const response = await fetch(filePath);
    const htmlContent = await response.text();
    return htmlContent;
  } catch (error) {
    console.error('Error reading HTML file:', error);
    return null;
  }
}

// Main function to process the HTML file
async function processHtmlFile(filePath, patternToMatch, patternToReplace, replacement) {
  const htmlContent = await readHTMLFile(filePath);
  const idsToSearch = [];

  if (htmlContent) {
    const parser = new DOMParser();
    const doc = parser.parseFromString(htmlContent, 'text/html');

    const elementsWithMatchingIds = Array.from(doc.querySelectorAll('[id^="' + patternToMatch + '"]'));
    elementsWithMatchingIds.forEach(element => {
      const id = element.getAttribute('id');
      idsToSearch.push(id);
    });
  }

  const anchorElements = Array.from(document.querySelectorAll('a'));

  anchorElements.forEach(anchorElement => {
    const href = anchorElement.getAttribute('href');
    if (idsToSearch.some(id => href && href.endsWith(id))) {
      anchorElement.innerHTML = anchorElement.innerHTML.replace(patternToReplace, replacement);
    }
  });
}


const filePath = 'Annex2.html'; 
const patternToMatch = 'fig-'; 
const patternToReplace = 'Figure&nbsp;';
const replacement = 'Graph ';

processHtmlFile(filePath, patternToMatch, patternToReplace, replacement);
  
</script>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./FSandP.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">2.1 Characterization of financial time series</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./PC.html" class="pagination-link">
        <span class="nav-page-text">2.3 Portfolio composition</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">Final Master’s Project presented at <a href="https://fee.udc.es/"><img src="../feelogo.png" class="img-fluid" alt="FEE" width="65"></a> <a href="https://www.udc.es/"><img src="../udclogo.png" class="img-fluid" alt="UDC" width="65"></a></div>   
    <div class="nav-footer-center">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 David Díaz Rodríguez
  </li>  
</ul>
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/david-d-6257951b8/">
      <i class="bi bi-linkedin" role="img" aria-label="Linkedin">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/davidrsch/TFM/">
      <i class="bi bi-github" role="img" aria-label="TFM GitHub">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>