[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Final Master’s Project",
    "section": "",
    "text": "Description\nThis is the website for “Application of artificial neural networks and quadratic programming in portfolio management”, a Master’s Thesis of the Master’s Degree in Banking and Finance from the University of A Coruña. The work was carried out by David Díaz Rodríguez and supervised by Xosé Manuel Martínez Filgueira. The site is built using Quarto.\nThis work has a repository that contains, in addition to the source code for this site, some of the data resulting from the procedure described throughout the work, as well as structures of the RNA models used to obtain the predictions. .\nThe code of the procedure exposed in the present work, Annex. 4 Codes, was developed using version 4.3.1 of R and version 2023.06.1-524 of the RStudio software. The packages necessary for the execution of the code are the following:\n\n\n\nPackage\nVersión\n\n\n\n\nabind\n1.4.5\n\n\nDiagrammeR\n1.0.11\n\n\ndplyr\n1.1.4\n\n\nforecast\n8.22.0\n\n\nggplot2\n3.5.1\n\n\ngridExtra\n2.3\n\n\ngt\n0.10.1\n\n\njsonlite\n1.8.8\n\n\nkableExtra\n1.4.0\n\n\nkeras\n2.15.0\n\n\nknitr\n1.46\n\n\nlubridate\n1.9.3\n\n\nMatrix\n1.7.0\n\n\nMetrics\n0.1.4\n\n\nquadprog\n1.5.8\n\n\nquantmod\n0.4.26\n\n\nreadr\n2.1.5\n\n\nreadxl\n1.4.3\n\n\nsimplermarkdown\n0.0.6\n\n\nstringr\n1.5.1\n\n\ntensorflow\n2.16.0\n\n\ntibble\n3.2.1\n\n\ntidyr\n1.3.1\n\n\nTTR\n0.24.4\n\n\nxml2\n1.3.6\n\n\nxts\n0.13.2\n\n\nzoo\n1.8.12\n\n\n\nTo cite use the following url: https://ruc.udc.es/dspace/handle/2183/36519. You can also consult:\n\n  \n    \n    Slides\n  \n  \n    \n    Dashboard",
    "crumbs": [
      "Description"
    ]
  },
  {
    "objectID": "greetings.html",
    "href": "greetings.html",
    "title": "Thanks",
    "section": "",
    "text": "I want to express my sincere gratitude to all the people who contributed significantly to the completion of this final master’s thesis. Without your support, guidance and encouragement, this achievement would not have been possible.\nI would like to thank the Xunta de Galica for providing me with the resources to carry out master’s studies as a beneficiary of the Excellence Juventud Exterior Scholarship.\nI would like to thank Universidade da Coruña for the knowledge acquired while studying the Master’s Degree in Banking and Finance. Its commitment to academic excellence has been a constant source of inspiration.\nI am deeply grateful to my tutor Xosé Manuel Martínez Filgueira for his expert guidance and valuable suggestions throughout this process. His knowledge and dedication were essential to give direction and quality to this work.\nMy thanks are extended to my classmates and friends who provided a space for rich discussions and valuable contributions that contributed to the development of this work.\nI also want to express my gratitude to my family for their constant emotional support and understanding during the challenging moments of this academic process.",
    "crumbs": [
      "Thanks"
    ]
  },
  {
    "objectID": "summaryen.html",
    "href": "summaryen.html",
    "title": "Abstract",
    "section": "",
    "text": "In the context of the financial world in constant change and complexity, this work deals with the application of artificial neural networks and quadratic programming in the management of financial portfolios. The importance of properly characterizing financial time series for more accurate forecasting is highlighted, and the potential of combining convolutional neural networks and LSTM to improve time series forecasting is examined. In the portfolio composition process, quadratic programming is applied as an efficient technique to achieve an optimal distribution of financial assets. In conclusion, the approach of combining artificial neural networks and quadratic programming shows promise in the management of financial portfolios, but a deeper and more exhaustive study is necessary to determine its optimal efficiency. This paper lays the groundwork for future research, highlighting the importance of using up-to-date data and properly configuring models to achieve more informed and effective portfolio management in an ever-evolving financial environment.\nKeywords: portfolio management, portfolios, artificial neural networks, quadratic programming, financial time series, price prediction, portfolio composition\nWords: 13027",
    "crumbs": [
      "Abstract"
    ]
  },
  {
    "objectID": "summaryes.html",
    "href": "summaryes.html",
    "title": "Resumen",
    "section": "",
    "text": "En el contexto del mundo financiero en constante cambio y complejidad, este trabajo aborda la aplicación de redes neuronales artificiales y programación cuadrática en la gestión de carteras financieras. Se destaca la importancia de caracterizar adecuadamente las series temporales financieras para realizar pronósticos más precisos y se examina el potencial de la combinación de las redes neuronales convolucionales y LSTM para mejorar la previsión de series de tiempo. En el proceso de composición de carteras, se aplica la programación cuadrática como una técnica eficiente para lograr una distribución óptima de activos financieros. En conclusión, el enfoque de combinar redes neuronales artificiales y programación cuadrática muestra promesa en la gestión de carteras financieras, pero es necesario un estudio más profundo y exhaustivo para determinar su eficiencia óptima. Este trabajo sienta las bases para futuras investigaciones, destacando la importancia de utilizar datos actualizados y configurar adecuadamente los modelos para lograr una gestión de carteras más informada y efectiva en un entorno financiero en constante evolución.\nPalabras clave: gestión de carteras, carteras, redes neuronales artificiales, programación cuadrática, series temporales financieras, predicción de precios, composición de carteras.",
    "crumbs": [
      "Resumen"
    ]
  },
  {
    "objectID": "summarygal.html",
    "href": "summarygal.html",
    "title": "Resumo",
    "section": "",
    "text": "No contexto do mundo financeiro en constante cambio e complexidade, este traballo trata sobre a aplicación das redes neuronais artificiais e da programación cuadrática na xestión de carteiras financeiras. Destaca a importancia de caracterizar adecuadamente as series temporales financeiras para unha previsión máis precisa e examínase o potencial de combinar redes neuronais convolucionais e LSTM para mellorar a previsión de series temporais. No proceso de composición da carteira aplícase a programación cuadrática como técnica eficiente para conseguir unha distribución óptima dos activos financeiros. En conclusión, o enfoque de combinar redes neuronais artificiais e programación cuadrática resulta prometedor na xestión de carteiras financeiras, pero é necesario un estudo máis profundo e exhaustivo para determinar a súa eficiencia óptima. Este traballo senta as bases para futuras investigacións, destacando a importancia de utilizar datos actualizados e de configurar adecuadamente os modelos para lograr unha xestión de carteira máis informada e eficaz nun entorno financeiro en constante evolución.\nPalabras clave: xestión de carteiras, carteiras, redes neuronais artificiais, programación cuadrática, series temporales financeiras, predición de prezos, composición da carteira.",
    "crumbs": [
      "Resumo"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1 Introduction",
    "section": "",
    "text": "In the financial field, efficient portfolio management is a crucial task for investors and asset managers, since it seeks to maximize returns and minimize the risks associated with investments. In recent years, the field of artificial intelligence and machine learning has experienced remarkable progress, which has allowed the application of innovative techniques to improve the financial decision-making process.\nThis work focuses on the integration of two powerful tools: artificial neural networks and quadratic programming, to address the challenge of portfolio management. The combination of these techniques offers a robust and promising approach to financial time series forecasting and portfolio composition in a highly dynamic and complex financial environment.\nThe development of the work is structured in several fundamental sections to comprehensively address the topic. First, a detailed characterization of the financial time series is carried out, examining its essential characteristics and properties to better understand the behavior of asset prices.\nNext, the potential of artificial neural networks in time series forecasting is explored. Background on the use of these networks in this context is presented and two widely used architectures are highlighted: convolutional neural networks and Long Short-Term Memory (LSTM) networks, both with the ability to capture complex patterns in financial data.\nThe section on portfolio composition addresses the problem and presents various techniques applied in asset management. It is here where quadratic programming is introduced as a relevant and efficient tool for the optimal construction of investment portfolios.\nObtaining accurate and relevant data is crucial for any financial analysis and work with Machine Learning algorithms. The methodology applied to obtain data is described, and how some of the most common indicators used in finance were computed to use as descriptive variables of the problem in conjunction with historical data. It also exposes how the vectors that will be used in the modeling and training of neural networks are structured.\nIn the last sections, the modeling and training process is discussed, which involves the proper configuration of the neural networks and the implementation of quadratic programming to obtain optimal results. Finally, the results obtained are presented, including the predictions generated by the neural networks and the composition of recommended portfolios, thus demonstrating the effectiveness of the proposed methodology in the management of financial portfolios.\nTaken together, this work seeks to provide a comprehensive and updated view of the use of artificial neural networks and quadratic programming in portfolio management, highlighting their potential as an option to improve financial decision-making and provide investors with a valuable tool for Optimize your investment strategies in a changing and competitive environment.",
    "crumbs": [
      "1 Introduction"
    ]
  },
  {
    "objectID": "body.html",
    "href": "body.html",
    "title": "2 Work development",
    "section": "",
    "text": "The present work delves into the field of machine learning and artificial intelligence, specifically in the combined use of artificial neural networks and quadratic programming. This powerful synergy seeks to offer an effective and sophisticated solution for forecasting financial time series and the optimal composition of investment portfolios. Through the application of these techniques, the aim is to improve financial decision-making and maximize returns, while minimizing the risks associated with investments. In the following sections, the different stages of the process will be explored in detail, from the characterization of time series and the functioning of neural networks, to the implementation of quadratic programming in the construction of efficient portfolios.",
    "crumbs": [
      "2 Work development"
    ]
  },
  {
    "objectID": "FSandP.html",
    "href": "FSandP.html",
    "title": "2.1 Characterization of financial time series",
    "section": "",
    "text": "2.1.1 Time series and their characteristics\nTime series are a type of stochastic process that is characterized by ordering random variables according to time. This means that each moment is associated with a value of the variable that depends on chance and that can change over time. According to Ruiz (2011), a stochastic process is “a collection or family of random variables, ordered according to a subscript that is usually time” (p.01). The analysis of the time series can have different purposes, such as describing the behavior of the variables or predicting or forecasting their future values, which is especially relevant for financial series.\nTime series analysis is a statistical tool that allows studying the behavior of a variable over time. However, there is no single consensus on the components that should be considered in this type of analysis. Some authors, such as Kocenda and Cerný (2017) and Anderson et al. (2017), propose that time series can be decomposed into three components: trend, seasonality, and noise. Other authors, such as Dodge (2008) and Espallargas and Solís (2012), suggest that a fourth component should be added: the cycle. Finally, there are authors who suggest that time series can have up to five components, these are the cases of IBM (2021) and Chirinos (2018).\nTrend: It is the long-term pattern of change that is observed in a series of data. It can be defined as the general and persistent direction of the variations of the series over time. It can be classified as positive (Figure 1), negative (Figure 2) or null (Figure 3), depending on whether the series increases, decreases or remains constant in the long term. The trend can be identified by graphical analysis or by statistical methods. This component is important to understand the historical behavior and project the future of a series of data, it is common in the different criteria mentioned.\nSeasonality: Also called regular cyclical variation: It refers to the variation corresponding to the movements of the series that occur every certain period of time, Figure 4. This component is, like the trend, common in the aforementioned criteria. Differentiating in that those authors who expose four and five components call seasonality the periodic variations corresponding to periods less than or equal to one year (such as daily, weekly, monthly, or annual periodicity), while the periodic variations corresponding to longer periods They contemplate a component called cyclical variations. Therefore, to determine the seasonality of a time series, it is necessary to analyze them in a period of no less than two years.\nOne component that cannot be explained by the other elements of the time series is the irregular variation or error. This component is also known as random variation, noise, or residual, and is shown in Figure 4. Irregular variation is common in all three criteria mentioned above. Some authors distinguish between irregular variation, which is occasional and random, and atypical variation, which is caused by isolated events that alter the behavior of the series. Atypical variation can be classified into several types: additive, innovation, level change, transient, additive seasonality, and local trend.\nOne way to categorize the time series is according to the degree of variability that they present over time. According to what was exposed in Villagarcía (2006), it is possible to distinguish between homoscedastic and heteroscedastic series. The homoscedastic series are those that maintain a constant range of variation, as shown in Figure 3. On the contrary, heteroscedastic series are those that change the range of variation, increasing or decreasing its amplitude, as illustrated in Figure 1 and Figure 2.\nA key concept in time series analysis is that of stationarity. A time series is stationary when its statistical properties, such as the mean, variance, and covariance, do not change with time. This implies that the series does not present a trend, cycles or seasonality. As Castillo and Varela (2010), Villavicencio (2010) and Ruiz (2011) point out, stationarity is a necessary condition to be able to predict the future behavior of a time series using statistical techniques. An example of a stationary time series is shown in Figure 3.\nFinancial time series present heteroscedasticity, that is, variances that change over time. This implies that they are not stationary and that their behavior depends on external factors. To verify the stationarity of a time series, different methods can be used, such as the correlogram, which shows the autocorrelation and partial autocorrelation functions of the series, or unit root tests, such as Dickey Fuller’s or Phillips Perron’s. , which test the null hypothesis that the series has a unit root. These methods are explained in more detail in Castillo and Varela (2010), Villavicencio (2010) and Ruiz (2011). The Figure 5 illustrates an example correlogram for a financial time series.",
    "crumbs": [
      "2 Work development",
      "2.1 Characterization of financial time series"
    ]
  },
  {
    "objectID": "FSandP.html#pricing-features",
    "href": "FSandP.html#pricing-features",
    "title": "2.1 Characterization of financial time series",
    "section": "2.1.2 Pricing Features",
    "text": "2.1.2 Pricing Features\nInvesting in stocks or any other asset listed on the stock market is a complex and challenging task, requiring a thorough understanding of market trends and fluctuations. At the core of this understanding is the ability to analyze and interpret stock market price data, providing key insights into the behavior of market participants and the factors that drive market movements. The purpose of this sub-section is to provide a comprehensive overview of the stock price environment and how they are commonly represented, pointing out the most important aspects for the application of the techniques that will be explored in the following sections.\nAs explained in CNMV (n.d.b), stock exchanges are organized markets where shares and other securities are traded, such as fixed income, warrants, certificates and exchange-traded funds. In BME (n.d.) it is stated that, in Spain, there are four traditional stock exchanges (Madrid, Barcelona, ​​Bilbao and Valencia) that are part of the holding Stocks and Spanish Market (hereinafter, BME, for its acronym in Spanish, Bolsas y Mercados Españoles), which also integrates other segments and trading, clearing and settlement systems values. Being, as explained in CNMV (n.d.c), the Spanish Stock Market Interconnection System (hereinafter, SIBE, for its acronym in Spanish, Sistema de Interconexión Bursátil Español) is the platform that allows continuous and electronic trading of all securities admitted to trading on the four Spanish stock\nAs CNMV (n.d.a) exposes, shares are transferable securities that represent a proportional part of the share capital of a public limited company, and their holders are proprietary partners of the same. Shares may be traded on stock exchanges or other authorized secondary markets.\nFrom what was stated in Mitchell (2020), Pinset (2021) and C. Team (2023) it can be concluded that, to explain the price of a company’s shares, the following factors can be considered:\n\nThe supply and demand of shares in the market: if there are more buyers than sellers, the price will rise and vice versa. This depends on the expectations and confidence of investors in the future of the company.\nChanges in the management or production of the company: if the company improves its efficiency, its profitability or its innovation, the price of its shares may increase. On the contrary, if the company has internal problems, loses competitiveness or is affected by external crises, the price may fall.\nThe company’s reputation: If the company has a good public image, is associated with successes or achievements, or receives good ratings from analysts, its share price may rise. Conversely, if the company is embroiled in scandals, lawsuits, or controversies, or receives poor ratings from analysts, the price may drop.\n\nIn the texts Pinset (2021), T. I. Team (2022) and C. Team (2023) also point out the importance of differentiating the price of a company or its share from its intrinsic value. Being able to summarize considering what is indicated in these texts and what has been previously stated that the price of a company or action is what buyers and sellers are willing to pay for it at a given moment, while the intrinsic value of a company or action depends largely on the methodology used to value the companies and the objectives of the evaluator.\nOnce the environment in which share prices are found has been contextualized in a general way and some of the factors that may affect them have been explained, the structure in which these data usually appear is explained below. Generally, the prices of the shares are registered periodically (daily, weekly, monthly, annually, etc.). registering for each period the opening price, the highest price, the lowest, the closing price, the volume and the adjusted closing price, see Table 1.\nFrom what was exposed in Barone (2022), Chen (2022), Downey (2022), Hayes (2021) and Ganti (2020) it can be understood that:\n\nThe opening price is the first price at which a financial asset trades in a trading session. This price may be different from the closing price of the previous session, as there may be changes in supply and demand during the period when the market is closed. The opening price usually indicates the tone or trend of the market for that day.\nThe highest price is the highest price at which a financial asset trades in a trading session. This price reflects the highest level of buyer interest for that asset on that day. The higher price can be an indicator of an asset’s strength or weakness, as well as its volatility.\nThe lowest price is the lowest price at which a financial asset trades in a trading session. This price reflects the minimum level of interest of sellers for that asset on that day. The lower price can be an indicator of an asset’s pressure or resistance, as well as its volatility.\nThe closing price is the last price at which a financial asset is traded in a trading session. This price is the one used to calculate the market value of that asset at the end of the day. The closing price is usually the most important for investors, as it summarizes the result of the day’s operations and shows the direction of the market.\nVolume is the number of units of a financial asset traded in a trading session. Volume shows the level of activity or liquidity of a market or an asset. Volume often accompanies price movements, as it indicates the degree of consensus or divergence among market participants.\nAdjusted closing price is the closing price of a financial asset that is changed to consider events such as dividends, splits, mergers or acquisitions that affect the value of the asset. The adjusted closing price allows you to compare the historical performance of an asset with greater precision and consistency.\n\nBased on what was stated in Hayes (2021) and Ganti (2020), it is understood that the difference between the closing price and the adjusted closing price is of great importance, since the former can give a distorted image of the performance of a share throughout the year. while the second reflects the actual value of the stock after adjusting for the factors that alter it.\n\nFor example, a company’s board of directors may decide to divide the company’s shares 3 by 1. Thus, the company’s outstanding shares increase by a multiple of three, while its share price is divided by three. . Let’s say a stock closed at $300 the day before your stock split. In this case, the closing price is adjusted to $100 ($300 divided by 3) per share to maintain a consistent standard of comparison. Similarly, all other previous closing prices for that company would be divided by three to get the adjusted closing prices. Ganti (2020)\n\nDue to this, the adjusted closing price is better for the application of time series analysis techniques, since it allows comparing the behavior of a stock over time without the distortions caused by corporate events. The time series most commonly used in market price analysis studies is that made up of returns calculated from the adjusted closing price.\n\n\n\n\nAnderson, D. R., D. J. Sweeney, T. A. Williams, D. J. Camm, and J. J Cochran. 2017. Statistics for Business & Economics. Boston: Cengage Learning.\n\n\nBarone, A. 2022. “Opening Price: Definition, Example, Trading Strategies.” 2022. https://www.investopedia.com/terms/o/openingprice.asp.\n\n\nBME. n.d. “¿Qué Es BME?” Accessed April 24, 2023. https://www.bolsasymercados.es/esp/Sobre-BME/Que-es.\n\n\nCastillo, R. A., and R. Varela. 2010. ECONOMETRÍA PRÁCTICA: Fundamentos de Series de Tiempo. México: Universidad Autónoma de Baja California.\n\n\nChen, J. 2022. “Today’s High.” 2022. https://www.investopedia.com/terms/t/todayshigh.asp.\n\n\nChirinos, S. 2018. “Series Cronológicas.” https://www.slideshare.net/SuedimarChirinos/series-cronologicas-119058959. 2018.\n\n\nCNMV. n.d.a. “Glosario Financiero: Acción.” Accessed April 24, 2023. https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&letra=A&idlng=1.\n\n\n———. n.d.b. “Glosario Financiero: Bolsa de Valores.” Accessed April 24, 2023. https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&letra=B&idlng=1.\n\n\n———. n.d.c. “Glosario Financiero: Servicio de Interconexión Bursátil Español, SIBE.” Accessed April 24, 2023. https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&letra=S&idlng=1.\n\n\nDodge, Y. 2008. “Time Series.” In The Concise Encyclopedia of Statistics, 536–39. New York, NY: Springer New York. https://doi.org/10.1007/978-0-387-32833-1_401.\n\n\nDowney, L. 2022. “Today’s Low.” 2022. https://www.investopedia.com/terms/t/todayslow.asp.\n\n\nEspallargas, S. D., and M. V. Solís. 2012. Econometría y Series Temporales: Aplicaciones. La Habana: Editorial Félix Varela.\n\n\nGanti, A. 2020. “Adjusted Closing Price.” 2020. https://www.investopedia.com/terms/a/adjusted_closing_price.asp.\n\n\nHayes, A. 2021. “What Is Closing Price? Definition, How It’s Used, and Example.” 2021. https://www.investopedia.com/terms/c/closingprice.asp.\n\n\nIBM. 2021. “Characteristics of Time Series.” https://www.ibm.com/docs/en/spss-modeler/saas?topic=data-characteristics-time-series. 2021.\n\n\nKocenda, E., and A. Cerný. 2017. Elements of Time Series Econometrics: An Applied Approach. Prague: Karolinum Press.\n\n\nMitchell, C. 2020. “Market Price: Definition, Meaning, How to Determine, and Example.” 2020. https://www.investopedia.com/terms/m/market-price.asp.\n\n\nPinset, W. 2021. “Understanding Stock Prices and Values.” 2021. https://www.investopedia.com/articles/stocks/08/stock-prices-fool.asp.\n\n\nRuiz, M. C. 2011. “Tema 5: Procesos Estocásticos.” http://www.dmae.upct.es/~mcruiz/Telem06/Teoria/apuntes_procesos.pdf; Departamento de Matemática y Estadística. Universidad Politécnica de Cartagena. 2011.\n\n\nTeam, CFI. 2023. “What Is Stock Price?” 2023. https://corporatefinanceinstitute.com/resources/capital-markets/stock-price/.\n\n\nTeam, The Investopedia. 2022. “Intrinsic Value Defined and How It’s Determined in Investing and Business.” 2022. https://www.investopedia.com/terms/i/intrinsicvalue.asp.\n\n\nVillagarcía, T. 2006. “Series Temporales.” https://halweb.uc3m.es/fjnm/estind/doc_grupo1/archivos/Apuntes%20de%20series.pdf. 2006.\n\n\nVillavicencio, J. 2010. “Introducción a Las Series de Tiempo.” http://www.estadisticas.gobierno.pr/iepr/LinkClick.aspx; Instituto de estadística de Puerto Rico. 2010.",
    "crumbs": [
      "2 Work development",
      "2.1 Characterization of financial time series"
    ]
  },
  {
    "objectID": "ANNinTSF.html",
    "href": "ANNinTSF.html",
    "title": "2.2 Artificial neural networks in the forecast of time series",
    "section": "",
    "text": "2.2.1 Background on the use of artificial neural networks in time series forecasting\nIn Chollet and Allaire (2018) it is stated that the ANN environment is made up of artificial intelligence (hereinafter IA), machine learning or automated learning (hereinafter ML) and deep learning or deep learning (hereinafter DL), Figure 1. Therefore, it is of vital importance to know the aspects of these fields that are closely related to ANN and that are briefly explained below.\n“Making a machine behave in such a way that a human would be called intelligent” (McCarthy et al. (2006), p.11) is the first definition given to the AI ​​problem. With the aim of solving this problem, the first AI emerged, the so-called symbolic AI.\nAs explained by Haykin (1998), Banda (2014) and Chollet and Allaire (2018), these early AIs involved hardcoded rules created by programmers. With the aim of achieving that these rules were automatically learned by the machines when observing the data, a new stage emerged in the development of AI, the so-called ML. This new stage gives rise to the emergence of a new form of programming, differentiating from the classic, in that, in this, the programmers introduce the data and the expected responses to them, and the computers are capable of generating the rules, Figure 2.\nSo, it is understood that ML models try to find appropriate representations for your input data: transformations of the data that make it more amenable to the task at hand. In DL, which is a specific sub-field of ML, these data representations are modeled through architectures composed of successive layers, which are called RNA Chollet and Allaire (2018).\nAfter studying what was exposed in Haykin (1998), Larrañaga (2007), Banda (2014) and Chollet and Allaire (2018) about ANN, it can be affirmed that they are inspired by the functioning of the human brain, these texts confirm and agree that three types of ANN can be distinguished layers: input, output and hidden. An input layer is composed of neurons that receive the input vectors. An output layer is made up of neurons that, during training, receive the output vectors and then generate the response. A hidden layer is connected to the environment through the input and output layers, this type of hidden layer processes the received input to obtain the corresponding output, Figure 3.\nOne of the applications of ANN is the forecasting of time series. whose objective is to predict the future values ​​of variables based on their past observations. As discussed previously, financial time series are often nonlinear, noisy, chaotic, and nonstationary, making them difficult to model and forecast. ANNs have the advantage of being able to capture complex nonlinear relationships and adapt to changing conditions without requiring prior assumptions about the distribution or structure of the data.\nThe history of ANNs in financial time series forecasting dates back to the late 1980s and early 1990s, when researchers began to explore the potential of ANNs as an alternative to traditional statistical methods, such as the integrated autoregressive moving average model, better known as ARIMA (Autoregressive Integrated Moving Average) and generalized autoregressive models with conditional heteroskedasticity, better known as GARCH (Generalized Autoregressive Conditional Heteroskedasticity). ANNs were shown to have several advantages over these methods, such as the ability to capture non-linear and dynamic relationships, handle noisy and incomplete data, and adapt to changing market conditions (B. Eddy Patuwo & Michael Y. Hu (1998)).\nHowever, ANNs also face some limitations and challenges in financial time series forecasting, such as the difficulty of choosing a suitable network architecture, training algorithm, activation function, and input variables; the risk of overfitting and generalization problems; the lack of interpretability and transparency; and the high computational cost and time (Tealab (2018)).\nTo overcome these limitations and challenges, researchers have proposed several enhancements and extensions to ANN for financial time series forecasting in recent decades. Some of the major developments include:\nThe history of ANNs in financial time series forecasting shows that ANNs have evolved and improved over time to cope with the complexity and uncertainty of financial markets. However, some of the previously mentioned challenges and limitations still persist, such as overfitting, generalization, interpretability, robustness, and computational cost.",
    "crumbs": [
      "2 Work development",
      "2.2 Artificial neural networks in the forecast of time series"
    ]
  },
  {
    "objectID": "ANNinTSF.html#background-on-the-use-of-artificial-neural-networks-in-time-series-forecasting",
    "href": "ANNinTSF.html#background-on-the-use-of-artificial-neural-networks-in-time-series-forecasting",
    "title": "2.2 Artificial neural networks in the forecast of time series",
    "section": "",
    "text": "The use of hybrid models that combine ANN with other techniques such as fuzzy logic, genetic algorithms, wavelet analysis, support vector machines, and deep learning to improve ANN performance and robustness (Wong and Guo (2010)).\nThe use of recurrent neural networks (hereinafter RNR) or bidirectional, which are a special type of ANN that can process sequential data and capture temporal dependencies. RNRs have been shown to outperform unidirectional neural networks in complex and non-linear time series (Guresen, Kayakutlu, and Daim (2011)).\nThe use of more complex RNA models by combining different layers, such as convolutional neural networks (hereinafter, CNN), long short-term memory (hereinafter, LSTM), gated recurrent units (hereinafter GRU) have been applied to financial time series forecasting with promising results (Sezer, Gudelek, and Ozbayoglu (2020)).",
    "crumbs": [
      "2 Work development",
      "2.2 Artificial neural networks in the forecast of time series"
    ]
  },
  {
    "objectID": "ANNinTSF.html#convolutional-neural-networks",
    "href": "ANNinTSF.html#convolutional-neural-networks",
    "title": "2.2 Artificial neural networks in the forecast of time series",
    "section": "2.2.2 Convolutional Neural Networks",
    "text": "2.2.2 Convolutional Neural Networks\nThe RNA model used in this work is composed of several layers, the most important being the Conv1D layer, a specific type of CNN, and the LSTM layer, both mentioned in the previous subsection when the ANN structures that most used today. This subsection focuses on the Conv1D Layer, so the fundamental concepts to understand its operation are explored, explaining convolution, convolutional neural networks and Conv1D and their use for time series analysis. An overview of convolution and how it can be applied to time series data is provided. Then, CNNs and their architecture, which allows them to automatically learn features from time series data, are discussed. Finally, Conv1D, a specific type of convolutional neural network layer that is particularly effective for processing time series data, is explained.\nAs discussed in Siddiqui (2023), convolution is a mathematical operation that is commonly used in signal processing and image analysis. It involves taking two functions and producing a third function that represents how one of the original functions modifies the other. In the context of time series data, convolution can be used to extract features from the data by applying a filter to the time series.\nIn addition to extracting features from time series data, convolution can also be used for other tasks such as noise reduction, anomaly detection, and prediction. For example, a CNN can be trained to predict future values of a time series by learning the underlying patterns in the data. In general, convolution is a powerful tool for analyzing time series data and its applications are numerous Siddiqui (2023).\nCNNs were first introduced at Lecun et al. (1998) and are a type of deep learning model that is commonly used for image analysis. However, as previously mentioned, they can also be used for time series analysis, as they are well-suited for learning features from data that have a spatial or temporal structure.\nThe architecture of a CNN consists of one or more convolutional layers, which apply filters to the input data to extract features. Each filter is a set of weights that are learned during the training process. By sliding the filter over the input data, the convolutional layer computes a dot product at each position, producing a new Lecun et al. (1998) feature map.\nIn a time series context, a CNN can learn to automatically extract features from data at different scales and time intervals, making it a powerful tool for time series analysis. A key advantage of using a CNN for time series analysis is that it reduces the need for manual feature engineering. Instead of designing filters by hand, CNN learns to automatically extract features from the data, making it more flexible and adaptable to different types of time series data.\nIn general, the architecture of a CNN allows it to automatically learn features from time series data, making it a powerful tool for time series analysis, with Conv1D being one of the most widely used CNN structures for this task.\nAs explained in Jing (2020) Conv1D is a specific type of CNN layer that is designed to process one-dimensional data, such as time series data. While traditional CNNs are designed to process two-dimensional data, Conv1D is specifically optimized for one-dimensional data, making it more efficient and effective for time series analysis.\nThe architecture of a Conv1D layer is similar to that of a traditional CNN, but with some key differences. Instead of using two-dimensional filters, Conv1D uses one-dimensional filters, which are applied to the input time series to extract features. The features that are extracted from the string will depend on the different configurations used for the filter configuration and the number of filters used, being the following formula to calculate the amount of feature that each filter extracts: Equation 1 (Jing (2020)):\n\\[\n\\begin{aligned}\nL_{out} &= \\frac{L_{in} + 2*padding - dilation*(kerenel\\_size - 1)-1}{stride} + 1 \\\\\n\\end{aligned}\n\\tag{1}\\]\nWhere:\n\nLout: is the length of the output of the filtering process or the number of features.\n\n\nLin: the length of the input vector, corresponding in time series analysis to the number of observations that contain the samples of the time series that are passed to the filter.\n\n\nkernel_size: is the size of the filter, which defines how many observations of the input vector are passed to the filter each time. Figure 4 represents how the size of the filter can affect the length of the output vector.\n\n\nstride: represents the number of steps or observations by which the selection of observations passed to the filter is moved. Figure 5 represents how the stride parameter can affect the length of the output vector.\n\n\ndilation: is the distance of the observations that pass the filter. Figure 6 represents how the dilation parameter can affect the length of the output vector.\n\n\npadding: represents the number of zeros to add to each end of the vector. Figure 7 represents how the padding parameter can affect the length of the output vector.\n\nOverall, Conv1D is a powerful tool for processing time series data, and its advantages include computational efficiency and the ability to capture time dependencies in the data. Its use cases are numerous and span different fields, making it a valuable tool for time series analysis.",
    "crumbs": [
      "2 Work development",
      "2.2 Artificial neural networks in the forecast of time series"
    ]
  },
  {
    "objectID": "ANNinTSF.html#long-short-term-memory",
    "href": "ANNinTSF.html#long-short-term-memory",
    "title": "2.2 Artificial neural networks in the forecast of time series",
    "section": "2.2.3 Long short-term memory",
    "text": "2.2.3 Long short-term memory\nThis subsection explains why LSTMs are one of the most widely used ANN structures in time series forecasting, based on a brief explanation of RNRs and why they are useful in solving series forecasting problems. of time, delving into why LSTMs differ from the rest of the RNNs, and the operation of each of the layers that make up the structure of an LSTM layer.\nOlah (2015) explains that an RNN can be considered as multiple copies of the same network, Figure 8, states that this aspect reveals that RNRs are intimately related to sequences and lists, which makes this type of RNA the one that naturally used for work with time series.\nConventional RNRs present a problem in relation to the ability to retain information, as explained by Olah (2015), standard RNNs perform with great capacity only if the information relevant to the current situation is recent, that is, where the gap between the relevant information and where it is needed is small, Figure 9; further exposes that as the gap grows, standard RNNs are unable to access the relevant information, Figure 10.\nAs previously mentioned, LSTMs are a type of RNR that can learn long-term dependencies on sequential data. These were proposed in Hochreiter (1997) and have been widely used for various tasks such as language modeling, speech recognition, machine translation, image description, and time series forecasting.\nThe main idea of LSTM is to introduce a memory cell that can store and update information over long steps of time. The memory cell is controlled by three gates: an entry gate, a forget gate, and an exit gate. These gates are neural networks that learn to regulate the flow of information in and out of the cell Figure 11.\nThe input gate decides how much of the new input to add to the cell state. The forget gate decides which part of the previous cell state to keep or delete. The output gate decides which part of the current cell state is to be sent to the next layer. Olah (2015) based on what was exposed in Hochreiter (1997), describes the operation of the doors in four steps:\n\nDeciding which cell state information is forgotten through the gate, forget gate layer \\(f_t\\). This gate looks at \\(h_{t-1}\\), hidden state from the previous time period, and \\(x_{t}\\), input from the current time instant, and outputs a number between 0 (undo) and 1 (hold). for each number in cell state \\(C_{t-1}\\), Figure 12, Equation 2.\n\n\\[\n\\begin{aligned}\nf_t &= \\sigma(W_f [h_{t-1}, x_t] + b_f) \\\\\n\\end{aligned}\n\\tag{2}\\]\n\nDecide what new information is stored in the cell state. For this first the input gate layer decides which values to update and then a tanh (hyperbolic tangent) layer creates a vector of new candidate values (\\(\\tilde{C}_t\\)) that could be added to the state, Figure 13, Equation 3 y Equation 4.\n\n\\[\n\\begin{aligned}\ni_t &= \\sigma(W_i [h_{t-1}, x_t] + b_i) \\\\\n\\end{aligned}\n\\tag{3}\\]\n\\[\n\\begin{aligned}\n\\tilde{C}_t &= tanh(W_c [h_{t-1}, x_t] + b_c) \\\\\n\\end{aligned}\n\\tag{4}\\]\n\nThe state of the old cell, \\(C_{t-1}\\), is updated to the new state of cell \\(C_{t}\\). Multiply the previous state by \\(f_{t}\\), forgetting what is necessary, then add \\(i_{t} * \\tilde{C}_{t}\\). These are the new candidate values, scaled by how much each status value needs to be updated, Figure 14, Equation 5.\n\n\\[\n\\begin{aligned}\nC_t &= f_t * C_{t-1} + i_t * \\tilde{C}_t  \\\\\n\\end{aligned}\n\\tag{5}\\]\n\nAn output is generated based on the cell state. Running first a sigmoid layer that decides what parts of the cell state is the output; then the state of the cell is passed through a tanh function (scaling the values between −1 and 1) and multiplied by the output gate, output gate, Figure 15, Equation 6 y Equation 7.\n\n\\[\n\\begin{aligned}\no_t &= \\sigma(W_o [h_{t-1}, x_t] + b_o) \\\\\n\\end{aligned}\n\\tag{6}\\] \\[\n\\begin{aligned}\nh_t &= o_t * tanh(C_t) \\\\\n\\end{aligned}\n\\tag{7}\\]\nLSTMs can learn to capture long-term dependencies by tuning gate values through back propagation. For example, if a certain input is relevant to a later exit, the input gate will learn to let it in, and the forgotten gate will learn to hold it in the cell state until it is needed. Conversely, if an input is irrelevant or stale, the gateway will learn to ignore it, and the forgotten gate will learn to remove it from the cell state.\n\n\n\n\nB. Eddy Patuwo & Michael Y. Hu, Guoqiang Zhang &. 1998. “Forecasting with Artificial Neural Networks:: The State of the Art.” International Journal of Forecasting 14 (1): 35–62. https://doi.org/https://doi.org/10.1016/S0169-2070(97)00044-7.\n\n\nBanda, Hugo. 2014. Inteligencia Artificial: Principios y Aplicaciones. Quito, Ecuador: Escuela Politécnica Nacional.\n\n\nChollet, F., and J. J. Allaire. 2018. Deep Learning with r. Manning Publications. https://books.google.es/books?id=xnIRtAEACAAJ.\n\n\nGuresen, Erkam, Gulgun Kayakutlu, and Tugrul U. Daim. 2011. “Using Artificial Neural Network Models in Stock Market Index Prediction.” Expert Systems with Applications 38 (8): 10389–97. https://doi.org/https://doi.org/10.1016/j.eswa.2011.02.068.\n\n\nHaykin, Simon. 1998. Neural Networks: A Comprehensive Foundation. Prentice Hall PTR.\n\n\nHochreiter, Jürgen, Sepp & Schmidhuber. 1997. “Long Short-Term Memory.” Neural Computation 9 (8): 1735–80. https://doi.org/10.1162/neco.1997.9.8.1735.\n\n\nJing, Hong. 2020. “How Convolutional Layers Work in Deep Learning Neural Networks?” Jingles, Github Blog. 2020. https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/.\n\n\nLarrañaga, Iñaki & Moujahid, Pedro & Inza. 2007. “Tema 14. Redes Neuronales.” Departamento de Ciencias de la Computaci´on e Inteligencia Artificial, Universidad del Pa´ıs Vasco–Euskal Herriko Unibertsitatea. 2007. http://www.sc.ehu.es/ccwbayes/docencia/mmcc/docs/t14-neuronales.pdf.\n\n\nLecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998. “Gradient-Based Learning Applied to Document Recognition.” Proceedings of the IEEE 86 (11): 2278–2324. https://doi.org/10.1109/5.726791.\n\n\nMcCarthy, John, Marvin L. Minsky, Nathaniel Rochester, and Claude E. Shannon. 2006. “A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955.” AI Magazine 27 (4): 12. https://doi.org/10.1609/aimag.v27i4.1904.\n\n\nOlah, Christopher. 2015. “Understanding LSTM Networks.” Colah’s blog. 2015. https://colah.github.io/posts/2015-08-Understanding-LSTMs/.\n\n\nSezer, Omer Berat, Mehmet Ugur Gudelek, and Ahmet Murat Ozbayoglu. 2020. “Financial Time Series Forecasting with Deep Learning : A Systematic Literature Review: 2005–2019.” Applied Soft Computing 90: 106181. https://doi.org/https://doi.org/10.1016/j.asoc.2020.106181.\n\n\nSiddiqui, J. Rafid. 2023. “Why Convolve? Understanding Convolution and Feature Extraction in Deep Networks.” Medium, Towards Data Science. 2023. https://towardsdatascience.com/why-convolve-understanding-convolution-and-feature-extraction-in-deep-networks-ee45d1fdd17c.\n\n\nTealab, Ahmed. 2018. “Time Series Forecasting Using Artificial Neural Networks Methodologies: A Systematic Review.” Future Computing and Informatics Journal 3 (2): 334–40. https://doi.org/https://doi.org/10.1016/j.fcij.2018.10.003.\n\n\nWong, W. K., and Z. X. Guo. 2010. “A hybrid intelligent model for medium-term sales forecasting in fashion retail supply chains using extreme learning machine and harmony search algorithm.” International Journal of Production Economics 128 (2): 614–24. https://ideas.repec.org/a/eee/proeco/v128y2010i2p614-624.html.",
    "crumbs": [
      "2 Work development",
      "2.2 Artificial neural networks in the forecast of time series"
    ]
  },
  {
    "objectID": "PC.html",
    "href": "PC.html",
    "title": "2.3 Portfolio composition",
    "section": "",
    "text": "2.3.1 Problem and techniques\nAs Gunjan (2023) explains, portfolio optimization is the process of selecting the best mix of assets to hold in a portfolio based on pre-defined objectives. The objectives can be the maximization of return or the minimization of risk, or both. Portfolio optimization involves finding the optimal weights for each asset in the portfolio so that the overall portfolio meets the desired objectives. This can be a challenging problem due to the large number of assets to choose from and the complex relationships between them.\nPortfolio optimization is an important process for investors as it helps them minimize risk and maximize return on their investments. By carefully selecting the assets to hold in their portfolio, investors can achieve their desired level of risk and return while diversifying their investments to reduce overall risk. Portfolio optimization is a crucial mechanism used to reduce investment risk.\nThere are various techniques that can be used to solve the portfolio optimization problem. In Gunjan (2023) these techniques are classified into two categories: classical approaches and intelligent approaches. Below is a general explanation of some of the techniques belonging to each approach.\nClassical approaches:\nSmart approaches:",
    "crumbs": [
      "2 Work development",
      "2.3 Portfolio composition"
    ]
  },
  {
    "objectID": "PC.html#problem-and-techniques",
    "href": "PC.html#problem-and-techniques",
    "title": "2.3 Portfolio composition",
    "section": "",
    "text": "Mean-variance: This technique, proposed in Markowitz and Markowitz (1967), is based on the idea of minimizing the variance for a given expected return or maximizing the expected return for a given variance. It is a parametric quadratic programming (hereinafter PQP) technique that can be used to solve quadratic optimization problems that arise in portfolio optimization (Aijun Zhang & Chun-hung Li & Agus Sudjianto (2008)). The mean variance approach assumes that investors are risk averse and prefer portfolios with lower variance. The technique consists of constructing a portfolio frontier that represents the set of portfolios that offer the highest expected return for a given level of risk. The optimal portfolio from this frontier is then selected based on the investor’s risk preferences.\nSkewed variance: This technique extends the mean-variance approach by accounting for skewed distribution. It was proposed in Samuelson (1970) and can be used when the distribution function is not quadratic in nature. Skewness measures the skewness of a distribution and can provide additional information about the potential risks and returns of a portfolio. By incorporating asymmetry into the portfolio optimization process, investors can better understand potential downside risks and make more informed decisions.\nValue at Risk (VaR): This statistical approach measures the potential loss in value of a portfolio over a defined period for a given confidence interval. It was introduced in the first edition of Jorion (2007) in 1997 and requires the determination of three parameters: time period, confidence level, and value-at-risk unit. VaR provides a measure of the maximum potential loss that could occur with a given probability in a specified time horizon. It is commonly used by financial institutions to manage their risk exposure and comply with regulatory requirements.\nConditional Value-at-Risk (CVaR): This approach widens the VaR by considering the expected loss that exceeds the VaR. It was introduced in Rockafellar and Uryasev (2002) and can handle extreme losses by using dynamic weights derived from historical data. CVaR provides a measure of the expected loss that could occur beyond the VaR threshold. It is also known as Expected Shortfall (ES) or Tail Value-at-Risk (TVaR) and is considered a more consistent measure of risk than VaR.\nMean-Absolute Deviation (MAD): This technique can be used for large-scale and highly diversified portfolio selection problems. It was introduced in Konno and Yamazaki (1991) and penalizes both positive and negative deviations. MAD provides a measure of the average absolute deviation of portfolio returns from their mean value. It is considered more robust than variance-based measures, as it is less sensitive to outliers.\nMinimax: This technique uses the minimum return as a measure of risk. It was introduced in Cai et al. (2004) and has certain advantages when the returns are not normally distributed. Minimax provides a worst-case measure for a portfolio by minimizing the maximum potential loss that could occur. It can be useful for investors who are particularly concerned about downside risks.\n\n\n\nBayesian Networks: These probabilistic graphical models can be used to model risk and return. They were featured on Shenoy and Shenoy (2000) and can be used to visualize the relationship between different variables in a model. Bayesian networks provide a way to represent complex dependencies between variables using directed acyclic graphs (DAGs). They can be used to model uncertain relationships between variables and to make probabilistic predictions about future events. In the context of portfolio management, Bayesian networks can be used to model relationships between different assets and make predictions about their future returns based on historical data and other relevant information.\nSupport Vector Regression (SVR): This machine learning technique can be used to determine the amount to buy and sell. It was introduced by Drucker et al. (1996) and has certain advantages over statistical-based techniques, such as its ability to learn from historical data. SVR involves building a hyperplane that separates data points with different labels while maximizing the margin between them. It can be used for regression tasks where the goal is to predict continuous values ​​instead of discrete labels. In the context of portfolio management, SVR can be used to predict future asset prices based on historical data and other relevant information.\nArtificial neural networks: As explained previously, these computational models can be used to solve complex computational and learning problems. In the context of portfolio management, neural networks can be used to predict future asset prices or returns based on historical data and other relevant information, which is what they are used for in this paper.\nReinforcement Learning: This type of machine learning involves an agent or model interacting with its environment to learn from its actions. It was featured in Sutton and Barto (2018) and works to maximize agent reward. Reinforcement learning involves learning through trial and error interactions with an environment. The agent takes actions based on its current state and receives rewards or penalties based on the results of those actions. Over time, the agent learns to take actions that maximize his accumulated reward. In the context of portfolio management, reinforcement learning can be used to develop trading strategies that maximize returns while managing risk.",
    "crumbs": [
      "2 Work development",
      "2.3 Portfolio composition"
    ]
  },
  {
    "objectID": "PC.html#quadratic-programming",
    "href": "PC.html#quadratic-programming",
    "title": "2.3 Portfolio composition",
    "section": "2.3.1 Quadratic programming",
    "text": "2.3.1 Quadratic programming\nThis sub-heading explains what quadratic programming is. What are some of the techniques that exist within this discipline of mathematical optimization. It also exposes how the portfolio optimization problem can be described as a quadratic programming problem and briefly explains how one of the most used techniques in this discipline works, specifically the so-called Dual Active Set Method, which is used in the later chapters.\nQuadratic programming can be chosen from among the techniques listed in the previous subheading for several reasons. First, it is a well-established technique that has been widely used in portfolio optimization. It can handle complex optimization problems with multiple constraints and can provide an efficient and effective way to solve the portfolio optimization problem. This makes it a useful tool for investors looking to minimize risk while achieving the desired level of return. Finally, quadratic programming has a solid theoretical foundation and has been widely studied in the literature. This makes it a reliable and well-understood technique that can be used with confidence in portfolio optimization.\nThere are several quadratic programming techniques, among the most used are:\n\nInterior Point: This is a linear or nonlinear programming method that achieves optimization by going through the center of the solid defined by the problem instead of around its surface. A polynomial time linear programming algorithm using an interior point method was found by Karmarkar (1984).\nActive Set: This is an algorithm used to identify the active constraints in a set of inequality constraints. The active constraints are then expressed as equality constraints, thus transforming an inequality constrained problem into a simpler equality constrained subproblem. The active set method was first introduced in an article by Beale (1959) and developed by Fletcher (1971) and Bunch and Kaufman (1977).\nDual Active Set: The method, as exposed by Goldfarb and Idnani (1982) and Goldfarb and Idnani (1983), is an efficient and numerically stable dual algorithm for positive definite quadratic programming that takes advantage of the fact that the unrestricted minimum of the objective function can be used as a point of departure.\nAugmented Lagrangian: It was independently introduced in Magnus R. Hestenes (1969) and Powell (1969). It is used to solve constrained optimization problems by adding a penalty term to the objective function that penalizes any violation of the constraints. The penalty term is typically a multiple of a constraint violation measure, such as the sum of squared constraint violations.\nConjugate Gradient: This is an iterative method for solving systems of linear equations with a symmetric positive definite matrix. It can also be used to solve unrestricted optimization problems by finding the minimum of a quadratic function. The method generates a sequence of search addresses that are conjugated with respect to the matrix that defines the system of equations or quadratic function. The conjugate gradient method was originally introduced in an article by Magnus R. Hestenes and Stiefel (1952).\nGradient Projection: The gradient projection method was introduced in J. B. Rosen (1960) and J. Rosen (1961). This is an iterative method for solving constrained optimization problems by projecting the gradient into the feasible region at each iteration. The projected gradient is then used as the search direction, and a line search is performed along this direction to find a new iteration that satisfies the constraints and reduces the objective function.\n\nFrom the previously mentioned techniques, the Dual Active Set Method algorithm (hereinafter, DASM) was selected, which, as previously mentioned, was introduced in Goldfarb and Idnani (1982) and Goldfarb and Idnani (1983), it is an optimization algorithm to solve quadratic programming problems. The algorithm predicts the active set of constraints that are equally satisfied in the solution of the problem. Computes a sequence of optimal solutions of QP problems involving some of the constraints of the original problem, called a sequence of dual feasible points.\nBelow is a general example of how the DASM algorithm could work using what-if values for a 2-asset portfolio optimization problem, the example was built from Goswami, Mondal, and Paruya (2012) and Walker (2014):\nUnder the assumption that it is about finding the best composition of a portfolio in which, for simplicity, we have 2 assets, the quadratic problem would be posed as follows Equation 1:\n\\[\n\\begin{aligned}\nmin~~Q(\\vec{w}) &= \\vec{w}^TC\\vec{w}\\\\\nsubject~to:\\\\\nw_{1}+w_{2}=1\\\\\n0\\leq{w_{i}}\\leq{1}\\\\\nw_{1}\\mathbb{E} + w_{2}\\mathbb{E} \\geq{0.005}\n\\end{aligned}\n\\tag{1}\\]\nAssuming that they have average monthly returns \\(r=\\begin{bmatrix} 0.02 & 0.03 \\end{bmatrix}\\) and covariance matrix \\(C=\\begin{bmatrix} 0.001 & 0.0008 \\\\ 0.0008 & 0.002 \\end{bmatrix}\\) . The vectors and matrices needed for the DASM algorithm can be constructed as follows:\n\nThe average monthly return vector would be \\(r=\\begin{bmatrix} 0.02 & 0.03 \\end{bmatrix}\\).\nThe covariance matrix C would be used as the matrix D in DASM.\nThe constraint \\(w_{1}+w_{2}=1\\) can be written in matrix form as \\(\\begin{bmatrix} 1 & 1 \\end{bmatrix}\\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}=1\\). This would be the first row of the \\(A\\) array in DASM.\nThe minimum return requirement \\(w_{1}\\mathbb{E} + w_{2}\\mathbb{E} \\geq{0.005}\\) can be written in matrix form as \\(\\begin{bmatrix} 0.02 & 0.03 \\end{bmatrix}\\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}\\geq{0.005}\\). This would be another row of the \\(A\\) array in DASM.\nThe constraints \\(0\\leq{w_i}\\leq{1}\\) can be written in matrix form as \\(\\begin{bmatrix} 1 & 0 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\geq{0}\\) and \\(\\begin{bmatrix} 0 & 1 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\geq{0}\\) for lower bounds and \\(\\begin{bmatrix} -1 & 0 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\geq{-1}\\) and \\(\\begin{bmatrix} 0 & -1 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix} \\geq{-1}\\) for upper bounds.\nThe matrix \\(A\\) would look like this: \\(A=\\begin{bmatrix} 1 & 1 \\\\ 0.02 & 0.03 \\\\ 1 & 0 \\\\ 0 & 1 \\\\ -1 & 0 \\\\ 0 & -1 \\end{bmatrix}\\)\n\nThe corresponding vector \\(b\\) would be \\(\\begin{bmatrix} 1 & 0.005 & 0 & 0 & -1 & -1\\end{bmatrix}\\). We can then use the DASM algorithm to solve this quadratic programming problem and determine the optimal asset allocation in our portfolio.\nStep 0: Find the unrestricted minimum by solving the unrestricted quadratic programming problem. Set the number of elements in the active set A (empty set) to zero.\nStep 1: Choose a violated constraint, if any. In this case, suppose that the constraint \\(w_{1}+w_{2}=1\\) is violated.\nStep 2: Calculate the primary and dual step directions and the step length \\(t=min(t_{1},t_{2})\\). Suppose \\(t=t_{2}\\).\nStep 3: Step up and update the active set A and the solution (\\(S\\)) for pair (x, A). Since \\(t=t_{2}\\) , we add the pth constraint (in this case \\(w_1+w_2=1\\)) to \\(\\bar{N}\\) and update \\(H\\) and \\(N^{*}\\) in Equation 2.\n\\[\n\\begin{aligned}\nN^{*}=(\\bar{N}^{T}Q^{-1}\\bar{N})\\bar{N}^{T}Q^{-1}\\\\\nH=Q^{-1}(I-\\bar{N}N^{*})\n\\end{aligned}\n\\tag{2}\\]\nWhere:\n\n\\(N^{*}\\) is the pseudo-inverse or generalized Moore-Penrose inverse of \\(\\bar{N}\\).\n\n\n\\(\\bar{N}\\) is the matrix of the normal vectors of the constraints in the active set \\(A\\).\n\n\n\\(H\\) is the reduced inverse Hessian operator of \\(Q\\).\n\nThese steps are repeated iteratively until all constraints are satisfied and the optimal asset allocation has been determined.\n\n\n\n\nAijun Zhang & Chun-hung Li & Agus Sudjianto, Zhi-li Wu &. 2008. “Trace Solution Paths for SVMs via Parametric Quadratic Programming.” Researchgate. 2008. https://www.researchgate.net/publication/228577955_Trace_solution_paths_for_SVMs_via_parametric_quadratic_programming.\n\n\nBeale, EML. 1959. “On Quadratic Proramming.” Naval Research Logistics Quarterly 6 (3): 227–43.\n\n\nBunch, James R, and Linda Kaufman. 1977. “Some Stable Methods for Calculating Inertia and Solving Symmetric Linear Systems.” Mathematics of Computation 31 (137): 163–79.\n\n\nCai, Xiaoqiang, Kok Lay Teo, XQ Yang, and Xun Yu Zhou. 2004. “Minimax Portfolio Optimization: Empirical Numerical Study.” Journal of the Operational Research Society 55 (1): 65–72.\n\n\nDrucker, Harris, Christopher Burges, Linda Kaufman, Alex Smola, and Vladimir Vapnik. 1996. “Linear Support Vector Regression Machines.” Advances in Neural Information Processing Systems 9 (9): 155–61.\n\n\nFletcher, Roger. 1971. “A General Quadratic Programming Algorithm.” IMA Journal of Applied Mathematics 7 (1): 76–91.\n\n\nGoldfarb, Donald, and Ashok U. Idnani. 1982. “Dual and Primal-Dual Methods for Solving Strictly Convex Quadratic Programs.” In Numerical Analysis, edited by J. P. Hennart, 226–39. Berlin, Heidelberg: Springer Berlin Heidelberg.\n\n\n———. 1983. “A Numerically Stable Dual Method for Solving Strictly Convex Quadratic Programs.” Mathematical Programming 27: 1–33.\n\n\nGoswami, Nababithi, Supriyo K. Mondal, and Swapan Paruya. 2012. “A Comparative Study of Dual Active-Set and Primal-Dual Interior-Point Method.” IFAC Proceedings Volumes 45 (15): 620–25. https://doi.org/https://doi.org/10.3182/20120710-4-SG-2026.00029.\n\n\nGunjan, Siddhartha, Abhishek & Bhattacharyya. 2023. “A brief review of portfolio optimization techniques.” Artificial Intelligence Review 56 (5): 3847–86. https://doi.org/10.1007/s10462-022-10273-7.\n\n\nHestenes, Magnus R. 1969. “Multiplier and Gradient Methods.” Journal of Optimization Theory and Applications 4 (5): 303–20.\n\n\nHestenes, Magnus R., and Eduard Stiefel. 1952. “Methods of Conjugate Gradients for Solving Linear Systems.” Journal of Research of the National Bureau of Standards 49: 409–35.\n\n\nJorion, Philippe. 2007. Value at Risk: The New Benchmark for Managing Financial Risk. The McGraw-Hill Companies, Inc.\n\n\nKarmarkar, Narendra. 1984. “A New Polynomial-Time Algorithm for Linear Programming.” In Proceedings of the Sixteenth Annual ACM Symposium on Theory of Computing, 302–11.\n\n\nKonno, Hiroshi, and Hiroaki Yamazaki. 1991. “Mean-Absolute Deviation Portfolio Optimization Model and Its Applications to Tokyo Stock Market.” Management Science 37 (5): 519–31.\n\n\nMarkowitz, Harry M, and Harry M Markowitz. 1967. Portfolio Selection: Efficient Diversification of Investments. J. Wiley.\n\n\nPowell, Michael JD. 1969. “A Method for Nonlinear Constraints in Minimization Problems.” Optimization, 283–98.\n\n\nRockafellar, R Tyrrell, and Stanislav Uryasev. 2002. “Conditional Value-at-Risk for General Loss Distributions.” Journal of Banking & Finance 26 (7): 1443–71.\n\n\nRosen, JB. 1961. “The Gradient Projection Method for Nonlinear Programming. Part II. Nonlinear Constraints.” Journal of the Society for Industrial and Applied Mathematics 9 (4): 514–32.\n\n\nRosen, Jo Bo. 1960. “The Gradient Projection Method for Nonlinear Programming. Part i. Linear Constraints.” Journal of the Society for Industrial and Applied Mathematics 8 (1): 181–217.\n\n\nSamuelson, Paul A. 1970. “The Fundamental Approximation Theorem of Portfolio Analysis in Terms of Means, Variances and Higher Moments.” The Review of Economic Studies 37 (4): 537–42.\n\n\nShenoy, Catherine, and Prakash P Shenoy. 2000. “Bayesian Network Models of Portfolio Risk and Return.” In. The MIT Press.\n\n\nSutton, Richard S, and Andrew G Barto. 2018. Reinforcement Learning: An Introduction. MIT press.\n\n\nWalker, Ryan. 2014. “Solving Quadratic Progams with r’s Quadprog Package.” rwalk. 2014. https://rwalk.xyz/solving-quadratic-progams-with-rs-quadprog-package/.",
    "crumbs": [
      "2 Work development",
      "2.3 Portfolio composition"
    ]
  },
  {
    "objectID": "Data.html",
    "href": "Data.html",
    "title": "2.4 Data",
    "section": "",
    "text": "2.4.1 Data Collection\nA more detailed explanation regarding the code used to carry out the procedure described in this sub-heading can be found in Annex. 4 - Data Collection.\nIn order to exemplify how artificial neural networks and quadratic programming can be used in a portfolio management strategy, it was decided in this paper to use data from the Spanish market. Therefore, it was decided to work with the information corresponding to the companies that are in the list of listed companies that is exposed in “Empresas Cotizadas” (n.d.) and can be seen in Table 2.\nThe Table 2 collects the data of r nrow(empresas) companies. The data collected being the name, ticker, sector and subsector, market, index of each of the companies and whether or not they were selected to carry out the rest of the procedure after carrying out the steps set out in this sub-heading.\nIn order to obtain the data from the companies and analyze them to select those with which we worked in the rest of the procedure,  (n.d.a) was used as a source. Next, the process carried out to obtain and select the data is explained.\nIt was decided to download the monthly data of each of the companies collected in Table 2. Obtaining all the data between January 31, 2000 to February 28, 2023 of each of the entities.\nAfter obtaining the data, the quality of the data was evaluated. The evaluation began with a visual exploratory analysis of adjusted prices since, as explained in the previous chapter, these are the ideal ones to use in any historical analysis methodology.\nDuring the aforementioned visual exploratory analysis, it was detected that there were irregularities in the adjusted prices of some of the series. The irregularities detected consisted of the incorrect registration of the adjusted prices, as well as errors in their calculation. These errors were easily detected by looking at the graphs of the constant trend adjusted closing price values over long periods of time, as seen in Figure 6, which indicates misrecording of price changes; as well as sudden changes of up to more than 100% in them in a single period of time, which may indicate a miscalculation in the adjusted price, as seen in Figure 7, in this last case it was verified with other sources like  (n.d.b), to verify that the prices were indeed miscalculated.\nGiven the time available to carry out the study described in the procedure and the extensive amount of time that the investigation would require to be carried out to replace the erroneous values ​​in the series, it was decided to eliminate these irregularities by using only the values after January 2005, which no longer presented inconsistencies in the calculation of the adjusted price, subsequently those series that still contained missing values ​​and that presented irregularities in the registration of variations were eliminated. For the latter, those series in which the variations of unrecorded prices are in more than 10 observations.\nRemaining after the adjustments made r length(returns_emps3) companies, as seen in the selected column of the Table 2, some of these companies have different numbers of observations, because not all of them existed or had gone on the market stock market before January 2005.\nOnce the companies with which we worked were selected, their returns were computed from the adjusted prices. In addition to the returns corresponding to the selected companies, the returns of the adjusted closing price of the IBEX 35 were used, as well as other variables that serve as indicators of the behavior of the returns, and their relationship with those of the index, in this case those of the IBEX 35. These variables include the volatilities of the companies and the index, the correlation between the values of the series and the IBEX, and the beta of the companies in relation to the IBEX.",
    "crumbs": [
      "2 Work development",
      "2.4 Data"
    ]
  },
  {
    "objectID": "Data.html#indicators",
    "href": "Data.html#indicators",
    "title": "2.4 Data",
    "section": "2.4.2 Indicators",
    "text": "2.4.2 Indicators\nThis sub-heading presents a brief explanation of the computed variables to be used as input variables in conjunction with the historical values of the companies’ returns. A more detailed explanation regarding the code used to carry out the procedure described in this sub-heading can be found in Annex. 4 - Indicators.\n\n2.4.2.1 Volatility\nBuilding on Hargrave (2023) and Hayes (2023), standard deviation and volatility are two related concepts that measure how much the price of a stock or other asset fluctuates over time. Standard deviation is a statistical term that quantifies the spread of a set of data points around its mean value. Volatility is a financial term that describes the degree of variation in the returns of an asset over a given period of time.\nStandard deviation and volatility are important in stock market analysis because they indicate the risk and uncertainty associated with investing in a particular asset. A high standard deviation or volatility means that the price of the asset can change significantly in either direction, implying greater potential for profit or loss. A low standard deviation or volatility means that the asset’s price is relatively stable and predictable, which means less potential for profit or loss Hayes (2023).\nTo calculate the volatility of a stock or index, the standard deviation of returns is calculated. Therefore, the necessary calculations are those shown below in the Equation 1.\n\\[R_i = \\frac{P_i - P_{i-1}}{P_{i-1}}\\]\n\\[\\sigma = \\sqrt{\\frac{\\sum_{i=1}^N (R_i - \\bar{R})^2}{N} } \\tag{1}\\]\nwhere:\n\n\\(R_i\\) is the return of the stock in the period \\(i\\)\n\\(P_i\\) y \\(P_{i-1}\\) are the prices of a share in time periods \\(i\\) and \\(i-1\\), respectively.\n\\(\\sigma\\) is the standard deviation - \\(N\\) is the number of observations\n\\(\\bar{R}\\) is the average return on the stock.\n\nStandard deviation and volatility are useful tools for investors and analysts to assess the risk-reward balance of different assets and portfolios. They can also help compare the performance of different assets and portfolios over time and under different market conditions.\n\n\n2.4.2.2 Correlation\nAs Edwards (2022) explains, correlation is a statistical measure that determines how two variables move relative to each other. In stock market analysis, correlation can help understand the behavior of different stocks or market indicators over time. Taking the data used in this paper as an example, if the prices of one of the selected companies tend to go up and down together with the IBEX 35, these prices have a positive correlation. If, on the contrary, the company’s prices tend to rise when the IBEX 35 indicator falls, they have a negative correlation. A correlation coefficient of zero means that there is no linear relationship between the variables, being in this case the values ​​of the IBEX 35 and the prices of one of the determined companies.\nAs exposed by Ross (2022) the correlation between two variables is calculated using the following equation, Equation 2:\n\\[\\rho_{xy} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^n (y_i - \\bar{y})^2}} \\tag{2}\\]\nwhere:\n\n\\(\\rho_{xy}\\) is the correlation coefficient\n\\(n\\) is the number of observations\n\\(x_i\\) y \\(y_i\\) are the values of the two variables for the \\(i\\) observation\n\\(\\bar{x}\\) y \\(\\bar{y}\\) are the means of the two variables.\n\nAs also explained by Edwards (2022), the correlation coefficient ranges from -1 to 1, where -1 indicates perfect negative correlation, 1 indicates perfect positive correlation, and 0 indicates no correlation at all. Being able to understand that the closer the correlation coefficient is to both -1 and 1, the stronger the linear relationship between the variables analyzed.\nAs previously explained, the correlation coefficient, in this paper, can be used to analyze how similarly the returns of a company move compared to those of the IBEX 35. The correlation can also be used to diversify a portfolio by choosing stocks that have a low or negative correlation with each other, as explained by Boyte-White (2022). This can help reduce overall portfolio risk, as losses from one stock can be offset by gains from another. However, the correlation is not constant and can change over time due to various factors, such as market conditions, economic events, or company news. Therefore, it is important to monitor the stock correlation regularly and adjust the portfolio accordingly Boyte-White (2022).\nCorrelation is a valuable tool in stock market analysis, but it does not imply causation. Having a high or low correlation between two variables does not imply that one variable causes change in the other. Correlation simply measures the strength and direction of the linear relationship between two variables, without considering other factors that may influence them.\nAs also exposed in Edwards (2022), the correlation is closely related to the volatility of the market and of the shares, being able to see that, during periods of greater volatility, such as the financial crisis of 2008, the shares can tend to be more correlated, even if they are in different sectors. International markets can also become highly correlated during times of instability. Investors may want to include assets in their portfolios that have low market correlation to equity markets to help manage their risk.\n\n\n2.4.2.3 Beta\nAs explained by Kenton (2022) Beta is a measure of how sensitive a stock’s returns are to changes in market returns. It is calculated as the slope of the regression line that fits historical stock and market returns. A beta of 1 means the stock is moving in sync with the market, a beta greater than 1 means the stock is more volatile than the market, and a beta less than 1 means the stock is less volatile than the market.\nBeta is important in stock market analysis because, as Kenton (2022) explains, it helps investors assess the risk and return of a portfolio. By knowing the beta of each stock in a portfolio, investors can estimate how much the portfolio will fluctuate with market movements and adjust their asset allocation accordingly. For example, if an investor wants to reduce the risk in their portfolio, they might choose stocks with low or negative beta values ​​that tend to move in the opposite direction of the market.\nAs explained by Monaghan (2019) Beta is related to mapping, but they are not the same. As explained above, correlation is a measure of how linearly related two variables are, Beta, on the other hand, is a measure of how strongly related two variables are, indicating how much one variable changes when another variable change by one unit. Beta can be calculated from the correlation using the following equation, Equation 3:\n\\[\\beta = \\frac{\\rho_{xy} \\sigma_x}{\\sigma_y} \\tag{3}\\]\nwhere:\n\n\\(\\rho_{xy}\\) is the correlation coefficient between \\(x\\) and \\(y\\)\n\\(\\sigma_x\\) is the volatility of x\n\\(\\sigma_y\\) is the volatility of y",
    "crumbs": [
      "2 Work development",
      "2.4 Data"
    ]
  },
  {
    "objectID": "Data.html#vectors",
    "href": "Data.html#vectors",
    "title": "2.4 Data",
    "section": "2.4.3 Vectors",
    "text": "2.4.3 Vectors\nIn this sub-section, the procedure carried out to create the input and output vectors from the data resulting from the procedure described in the previous sub-section is explained. A more detailed explanation regarding the code used to carry out the procedure described in this sub-heading can be found in Annex. 4 - Vectors.\nThe structure of the set of input and output vectors is of vital importance in the modeling of ML techniques, having a significant impact on its effectiveness. The set of vectors must be created in a representative way of the problem to be solved, so the steps described below explain in detail the aspects of the problem to be answered in this work and how to shape the set of input and output vectors to it.\nAs previously mentioned, the objective of this paper is to present a procedure for the use of RNA models and quadratic programming in an investment strategy. The modeling addresses the need to obtain the most accurate predictions possible so that later, based on the predictions and historical data, find the ideal portfolio composition. Therefore, the problem to be represented with the sets of input and output vectors is how to explain the behavior of the profitability of a company at an instant of time \\(i+1\\) with the values of several variables at the instant of time \\(i\\).\nTo represent this problem, three-dimensional vectors were created, following what was exposed in Chollet and Allaire (2018). The dimensions of these vectors are explained as follows:\n\nThe first dimension is comprised of the number of samples obtained by sectioning the observations of the different series into consecutive two-dimensional vectors.\nThe second dimension is comprised of the number of observations, of the different series, collected in each two-dimensional vector.\nThe third dimension is the amount of series in each two-dimensional vector.\n\nTherefore, in order to correctly obtain these samples, it is necessary to first define which series will be used for the input and output vectors. The series used in the input vectors were defined in the previous section, being these: the historical returns of the company and the IBEX, the historical volatilities of the company and the IBEX, the historical correlation of the company and the IBEX, and the Historical beta of the company and the IBEX. The series used for the output vectors is the historical profitability of the company.\nSubsequently, the time horizon to be foreseen was defined, this is a key aspect in the creation of the sets of inputs and outputs. The number of observations defined as the time horizon determines the observations of the output vectors, in the present work an observation was determined as the time horizon since it is desired to predict the profitability of the next month of the different selected companies.\nAnd the last aspect to define is how many observations the model must observe to infer the desired output. This defines the number of observations that will be taken from each time series to form the input vectors. To determine this aspect, an iterative process must be carried out, testing different quantities and evaluating the results obtained by the models that are trained with them. To simplify the process, in the present work it was determined to test different input sizes, these being 1, 2 and 3 observations. Thus, testing in a certain way how the size of the inputs affects the prediction obtained.\nIf we have an array for the input vectors contains some r dim(returns_indc[[1]])[1] observations, we can calculate the number of samples obtained from this array following the following equation, Equation 4:\n\\[\nm = n - (i-1+o)\n\\tag{4}\\]\nwhere:\n\n\\(m\\) the number of samples\n\\(n\\) the number of observations in the series\n\\(i\\) y \\(o\\) the number of observations in the input and output vecotres respectively.\n\nThe Table 3 shows the number of samples obtained for the different sizes of input vectors proposed, for which the different numbers of observations of the selected r length(returns_indc) were considered. The Figure 16 shows what the input and output vectors look like, in the case in which the input vector has 3 observations.\n\n\n\n\nn.d.a. Yahoo Finance. https://finance.yahoo.com/.\n\n\n———. n.d.b. Investing. https://www.investing.com/.\n\n\nBoyte-White, C. 2022. “How Does Correlation Affect the Stock Market?” 2022. https://www.investopedia.com/ask/answers/021716/how-does-correlation-affect-stock-market.asp.\n\n\nChollet, F., and J. J. Allaire. 2018. Deep Learning with r. Manning Publications. https://books.google.es/books?id=xnIRtAEACAAJ.\n\n\nEdwards, J. 2022. “Why Market Correlation Matters?” 2022. https://www.investopedia.com/articles/financial-advisors/022516/4-reasons-why-market-correlation-matters.asp.\n\n\n“Empresas Cotizadas.” n.d. BME Exchange. Accessed May 21, 2023. https://www.bolsasymercados.es/bme-exchange/es/Mercados-y-Cotizaciones/Acciones/Mercado-Continuo/Empresas-Cotizadas.\n\n\nHargrave, M. 2023. “Standard Deviation Formula and Uses Vs. Variance.” 2023. https://www.investopedia.com/terms/s/standarddeviation.asp.\n\n\nHayes, A. 2023. “Volatility: Meaning in Finance and How It Works with Stocks.” 2023. https://www.investopedia.com/terms/v/volatility.asp.\n\n\nKenton, W. 2022. “Beta: Definition, Calculation, and Explanation for Investors.” 2022. https://www.investopedia.com/terms/b/beta.asp.\n\n\nMonaghan, B. 2019. “Correlation Vs. Beta: What Is the Difference and Why Does It Matter?” 2019. https://www.mackenzieinvestments.com/content/dam/final/corporate/mackenzie/docs/investment-teams/multi-asset-team/en/Correlation%20vs.%20Beta_%20What%20is%20The%20Difference%20and%20Why%20Does%20It%20Matter_%20_%20Mackenzie%20Investments.pdf.\n\n\nRoss, S. 2022. “How Do i Calculate Correlation Between Market Indicators and Specific Stocks?” 2022. https://www.investopedia.com/ask/answers/032315/how-do-i-calculate-correlation-between-market-indicators-and-specific-stocks.asp.",
    "crumbs": [
      "2 Work development",
      "2.4 Data"
    ]
  },
  {
    "objectID": "MandT.html",
    "href": "MandT.html",
    "title": "2.5 Modeling and training",
    "section": "",
    "text": "2.5.1 Modeling\nAs previously explained, the main elements of the artificial neural network models used are a CNN layer and an LSTM layer. In addition to this, an input layer and an output layer were used, which are in charge of supplying the models with the information of the vectors previously constituted. A more detailed explanation regarding the code used to carry out the procedure described in this sub-heading can be found in Annex. 4 - Modeling.\nSince three different sizes of observations were defined to consider making a prediction, it was necessary to build three different model structures that would adapt to the dimensions of the different input vectors, the different structures can be observed in the Figure 17.\nThe first notable difference between the structures are the outputs of the input layers, this difference is due to the sample sizes if you have chosen to use 1, 2 or 3 observations to build the model. As can be seen, the size of the input layer output therefore modifies the size of the inputs and outputs of the CNN layer.\nAs previously mentioned, the variations in the second dimension in the outputs of the CNN layer can be explained by the different sizes of the input vectors. But as can be seen, the size of the third dimension of the output of this layer is the same in all the structures, 64, which indicates the number of filters chosen to use, one of the main parameters to consider when configuring these layers. The latter means that the observations corresponding to the 6 variables used were divided into 64 variables that allow the model a better understanding of the relationship between the variables.\nAnother aspect that was modified in the CNN layer of the structures was the activation function that by default is called ReLU (for its acronym in English, Rectified Linear Unit) was changed to Leaky ReLU because as explained in OmG (2021) , ReLU is a nonlinear activation function that generates zero for negative inputs, which can cause some neurons to stop learning if many of their inputs are negative, since their gradients will be zero.\nGiven what was previously explained and that some of the variables used in the input values have a high number of negative observations, as is the case of returns or the correlation of some of the series in certain periods of time, the use of the ReLU activation function did not seem like a good option. Therefore, it was decided to use Leaky Relu as activation function, which as explained in OmG (2021), this is a variant that allows a small constant gradient, non-zero, for negative inputs. This means that this activation function allows some neurons to continue learning from negative inputs.\nIn the Figure 18 the domain of the ReLU and Leaky ReLU function is observed, which will allow you a better understanding of what was previously exposed.\nThe CNN layer in all the structures is linked to an LSTM layer, which in all cases had 64 neurons. The output of this layer was linked to the output layer which returns a single value.\nTo conclude with the construction of the models, it was determined to use the mean square error (hereinafter MSE) as the function used to evaluate a candidate solution, the results of the model and the SGD optimizer (for its acronym in English, Stochastic Gradient Descent) with an Alpha of 0.0005.",
    "crumbs": [
      "2 Work development",
      "2.5 Modeling and training"
    ]
  },
  {
    "objectID": "MandT.html#sec-entrenamiento",
    "href": "MandT.html#sec-entrenamiento",
    "title": "2.5 Modeling and training",
    "section": "2.5.2 Training",
    "text": "2.5.2 Training\nA more detailed explanation of the code used during the procedure described in this subsection can be found in Annex. 4 - Training.\nThe training of Machine Learning algorithms in the forecast of time series has its peculiarities to how models are trained with the aim of solving other types of problems. Therefore, in this sub-section the training methodology used is briefly covered, which is the so-called walk forward validation or advance validation.\nAs already mentioned, feedforward validation is a method used to evaluate machine learning models on time series data. This is because as explained by Brownlee (2019) it provides the most realistic evaluation of machine learning models on time series data. Traditional model evaluation methods from machine learning, such as k-fold cross-validation or splitting into training and validation data, do not work for time series data because they ignore the time components inherent in the problem. Walk-forward validation takes these temporal components into account and provides a more realistic assessment of how the model will perform when used operationally.\nWhen evaluating a model, we are interested in how the model performs on data that was not used to train it. In machine learning, this is called unseen or out-of-sample data. Commonly, for the resolution of other problems, the data is divided into different subsets: training, testing and validation, whose objective is to train and validate the model. With the walk forward validation methodology, the data is divided by time periods and the model is trained and validated consecutively, which allows evaluating how the model understands the temporal dependence of the data.\nBy dividing the data by time periods, it allows us to evaluate the actual functioning of the model if it had been applied from the first period, as well as to analyze its behavior throughout all the periods, observing whether its performance improves or not.\nFrom what is stated in this sub-section, it is understood that the models were trained using the corresponding sample sets, passing all the available samples in a certain period of time before continuing with the next period. Obtaining as a result of the above a prediction corresponding to each period of time contemplated, with the exception of the first two that would be used to train the model for the first time, as seen in the following diagram of the Figure 19.\n\n\n\n\nBrownlee, J. 2019. “How to Backtest Machine Learning Models for Time Series Forecasting.” 2019. https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/.\n\n\nOmG. 2021. “Difference Between ReLU, ELU and Leaky ReLU. Their Pros and Cons Majorly.” 2021. https://datascience.stackexchange.com/a/102485.",
    "crumbs": [
      "2 Work development",
      "2.5 Modeling and training"
    ]
  },
  {
    "objectID": "Results.html",
    "href": "Results.html",
    "title": "2.6 Result",
    "section": "",
    "text": "2.6.1 Predictions\nA more detailed explanation of the code used during the procedure described in this subsection can be found in Annex. 4 - Predictions.\nAs explained previously, while the model was being trained, the predictions were obtained. As was done with the artificial neural network models, the predictions were computed for the different observations using the arithmetic mean of the observations. The arithmetic mean was used because it is one of the most frequently used measures as an indicator of possible future behavior in the study of financial time series.\nThe predictions will be evaluated by computing the \\(MSE\\) and the real values and the \\(R^2\\) of the results obtained by the artificial neural network models and the arithmetic means.\nAs explained by Glen (2023) the \\(MSE\\) tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them. Squaring is necessary to remove any negative signs. It also gives more weight to larger differences. It is called the root mean square error since you are finding the average of a set of errors. The lower the \\(MSE\\), the better the forecast, shown by Equation 1 as calculated.\n\\[\n\\begin{aligned}\nMSE &= \\frac{1}{n} * \\sum_{i=1}^{n}{(Y_i-\\hat{Y_i})^2} \\\\\n\\end{aligned}\n\\tag{1}\\]\nWhere:\nAs explained by Nandakumar (2020) \\(R^2\\) is commonly used to explain how well a model does compared to the total mean of the observations, Equation 2:\n\\[\n\\begin{aligned}\nR^2 &= 1-\\frac{SSR}{SST}\\\\\nR^2 &= 1-\\frac{\\sum_{i=1}^{n}{(Y_i-\\hat{Y_i})^2}}{\\sum_{i=1}^{n}{(Y_i-\\tilde{Y})^2}}\\\\\n\\end{aligned}\n\\tag{2}\\]\nWhere:\nBut this can be an unfair indicator of the performance of a regression model since it is assumed that all observations over which a mean is computed are known, and as mentioned above this is not the case for neural network models. artificial animals trained using the walk forward validation methodology. Due to this, the calculation of \\(R^2\\) was modified, as has been done in other investigations such as Gu, Kelly, and Xiu (2018), so that the model with which the results obtained by the ANNs used are compared is that comprised by the arithmetic means of the observations earlier than the one predicted.\nNext, the different results obtained by the different models built will be briefly described. It should be noted that, although 3 different models of each of them were proposed, 10 were built, with the aim of standardizing the results obtained, since the process of building and training neural networks contains a random factor. Therefore, the results described below are the average results obtained by the various models built.",
    "crumbs": [
      "2 Work development",
      "2.6 Result"
    ]
  },
  {
    "objectID": "Results.html#sec-predicciones",
    "href": "Results.html#sec-predicciones",
    "title": "2.6 Result",
    "section": "",
    "text": "\\(n\\): number of observations\n\n\n\\(Y_i\\): real value\n\n\n\\(\\hat{Y_i}\\): expected value\n\n\n\n\n\n\\(\\tilde{Y}\\): arithmetic mean of all observations\n\n\n\n\n2.6.1.1 An observation\nThe results obtained by those models that were trained with input vectors that had one observation from each series showed, as seen in Figure 8, that in the first periods the models presented better predictions than those obtained by the arithmetic mean. . It can be observed that the effectiveness of the models in comparison with the means decays as the model progresses in time and learns from the new observations. It is also clearly seen that in most periods the \\(R^2\\) of this model is negative. In addition, a peak is observed in the \\(MSE\\) of the model at the beginning of 2020, which is understood as a loss of effectiveness of the model. This loss of effectiveness of the model could be related to sudden market movements resulting from the economic effects of Covid-19.\nThe previous analysis of the behavior of the indicators of these models by period gives us an overview of how these models performed but given that the results obtained in the companies are essential for the composition of the portfolio, we will now analyze the behavior observed in the results obtained by the 20 companies that presented the best and worst results, based on the \\(R^2\\) obtained as a criterion.\nObserving the results of the indicators exposed in the Table 4, those companies that presented a worse \\(R^2\\) also present a low \\(MSE\\), which indicates that it is less likely that the composition of the portfolio will be altered by the results obtained by these companies. companies. On the other hand, among the companies that obtained a better \\(R^2\\) there are some that obtained a high \\(MSE\\) accompanied by a \\(R^2\\) greater than 5%. This indicates that differences could be generated between the compositions of the portfolios due to the differences in the predictions and that these are companies that do not have a good \\(MSE\\).\nThe results described in the previous paragraph are similar for the cases of the models built with two and three observations, respectively.\n\n\n2.6.1.2 Two observations\nThe results obtained by those models that were trained with input vectors that had two observations from each series, it was found, as seen in Figure 9, that in the first periods the models presented better predictions than those obtained by the mean. arithmetic. It can be seen that the effectiveness of the models compared to the means decays as the model progresses in time, but they decay at a slower rate than those models trained with input vectors with one observation. It is also clearly seen that the \\(R^2\\) of these models has less variation than the \\(R^2\\) of the previously analyzed models, seeing that for these models the \\(R^2\\) is positive in most of the periods. In addition, as in the case of the models analyzed previously, a peak is also observed in the \\(MSE\\) of the model at the beginning of 2020.\n\n\n2.6.1.3 Three observations\nThe results obtained by those models that were trained with input vectors that had three observations of each series, it was found, as seen in Figure 10, that in the first periods the models presented better predictions than those obtained by the mean. arithmetic. It can be seen that the effectiveness of the models compared to the means decays as the model progresses in time, but they decay at a slower rate than those models trained with input vectors with one observation. It is clearly seen that the R2 of these models has a greater variation than the R2 of the previously analyzed models, observing how this variation decreases for those predictions after 2015. These models, like the first ones, presented a negative R2 in most of them. of the periods. In addition, as in the previous cases, a peak is also observed in the \\(MSE\\) of the model at the beginning of 2020.",
    "crumbs": [
      "2 Work development",
      "2.6 Result"
    ]
  },
  {
    "objectID": "Results.html#sec-cc",
    "href": "Results.html#sec-cc",
    "title": "2.6 Result",
    "section": "2.6.2 Portfolios composition",
    "text": "2.6.2 Portfolios composition\nA more detailed explanation of the code used during the procedure described in this subsection can be found in Annex. 4 - Portfolios composition.\nThis sub-heading describes the results obtained after applying quadratic programming to determine the composition of the portfolio. This, as well as the predictions, was made period by period with the aim of emulating a real situation in which the techniques were applied as a whole. Therefore, the present analysis focuses on the behavior observed when using the various models and the comparison of these results with those obtained with the use of means.\nAs can be seen in Figure 11, the portfolios made from the predictions obtained by the neural network models that had one observation generally obtained better results than the portfolios made from the predictions using the mean. It is observed that both portfolio groups presented a lower return than the index, IBEX, in the period between 2009 and 2016.\nWhen carrying out the analysis of the behavior of the returns obtained by the models with two input observations, Figure 12, it is observed: the behavior of the returns obtained by the different models varies less than those previously analyzed; In this case and contrary to the previous case, the returns remain similar in the period between 2009 and 2016; and although the final result is far from the result obtained by the averages, it is lower than that obtained by the previous models, the latter being due to the fact that the evaluation of the models in this case begins in a period prior to those of the previously analyzed models.\nObserving the results obtained by the latest models, Figure 13, it is observed: a distribution of returns higher than those trained with two observations but lower than those trained with one observation; it is observed that the returns begin to exceed those of the index after 2013 instead of 2016 as in previous years; and it is also observed that the returns of the RNA models are higher than those of the averages and also constitute the maximum returns obtained between the different structures of RNA models.\n\n\n\n\nGlen, S. 2023. “Mean Squared Error: Definition and Example.” 2023. https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-squared-error/.\n\n\nGu, Shihao, Bryan Kelly, and Dacheng Xiu. 2018. “Empirical Asset Pricing via Machine Learning.” Working Paper 25398. Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w25398.\n\n\nNandakumar, S. 2020. “How Can r-Squared Be Negative When the Correlation Between Prediction and Truth Is Positive?” 2020. https://stackoverflow.com/a/63311778/12660035.",
    "crumbs": [
      "2 Work development",
      "2.6 Result"
    ]
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "3 Conclusions",
    "section": "",
    "text": "During the development of this work, the application of artificial neural networks and quadratic programming in the management of financial portfolios has been addressed. Through a careful characterization of financial time series, it was possible to understand the importance of analyzing their characteristics and patterns to make more accurate forecasts.\nDifferent configurations of artificial neural network models, made up of the combination of convolutional layers and LSTM, were tested, which differed in the number of historical observations they would use as inputs before making a prediction. The predictions obtained from the aforementioned models were compared with predictions obtained by using the arithmetic mean, which is one of the most commonly used indicators. As a result of the aforementioned comparison, it was obtained that the models depending on the number of observations that they used as inputs: 1, 2 or 3; they obtained an R2 of: -0.00287, 0.0611 and 0.0179 respectively.\nThe predictions obtained, both with the RNA models and with the arithmetic means together with the historical behaviors were used to, through the use of quadratic programming, search for the composition of lower risk portfolios. After carrying out a portfolio management simulation, it was obtained that the portfolios made up from the predictions of the ANN models obtained at the end of the period studied, compared to those made up of the predictions using the arithmetic mean, returns: 5.63% higher , for models that used 1 observation as input; 35.67% higher for those who used 2; and 25.51% for those who used 3. In addition, it was observed that the portfolios made up of the RNA models obtained returns higher than the index, IBEX, by 40.86%, 39.78% and 60.54%, for the models that used 1, 2 and 3 observations as inputs respectively.\nThe aforementioned results show that the combined use of these tools, ANN and quadratic programming, can offer companies and organizations a significant competitive advantage in the management of their financial assets, allowing more effective decision-making, optimizing the composition of portfolios and maximizing returns.\nHowever, it is important to highlight that the results presented in this paper need a more in-depth study to analyze, among other aspects, the weight that the results of the predictions of the different companies have in the composition of portfolios. For this reason, this work is considered the beginning of a more exhaustive investigation in which: higher quality data must be obtained, and the use of various techniques will be contrasted, both to obtain predictions and to find the composition of the portfolio adequate.",
    "crumbs": [
      "3 Conclusions"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliography",
    "section": "",
    "text": "n.d.b. Yahoo Finance. https://finance.yahoo.com/.\n\n\n———. n.d.a. Investing. https://www.investing.com/.\n\n\nAijun Zhang & Chun-hung Li & Agus Sudjianto, Zhi-li Wu &.\n2008. “Trace Solution Paths for SVMs via Parametric Quadratic\nProgramming.” Researchgate. 2008. https://www.researchgate.net/publication/228577955_Trace_solution_paths_for_SVMs_via_parametric_quadratic_programming.\n\n\nAnderson, D. R., D. J. Sweeney, T. A. Williams, D. J. Camm, and J. J\nCochran. 2017. Statistics for Business & Economics. Boston:\nCengage Learning.\n\n\nB. Eddy Patuwo & Michael Y. Hu, Guoqiang Zhang &. 1998.\n“Forecasting with Artificial Neural Networks:: The State of the\nArt.” International Journal of Forecasting 14 (1):\n35–62. https://doi.org/https://doi.org/10.1016/S0169-2070(97)00044-7.\n\n\nBanda, Hugo. 2014. Inteligencia Artificial: Principios y\nAplicaciones. Quito, Ecuador: Escuela Politécnica Nacional.\n\n\nBarone, A. 2022. “Opening Price: Definition, Example, Trading\nStrategies.” 2022. https://www.investopedia.com/terms/o/openingprice.asp.\n\n\nBeale, EML. 1959. “On Quadratic Proramming.” Naval\nResearch Logistics Quarterly 6 (3): 227–43.\n\n\nBerwin A. Turlach R port by Andreas Weingessel\n&lt;Andreas.Weingessel@ci.tuwien.ac.at&gt; Fortran contributions from\nCleve Moler dpodi/LINPACK), S original by. 2019. Quadprog: Functions\nto Solve Quadratic Programming Problems. https://CRAN.R-project.org/package=quadprog.\n\n\nBME. n.d. “¿Qué Es BME?” Accessed April 24, 2023. https://www.bolsasymercados.es/esp/Sobre-BME/Que-es.\n\n\nBoyte-White, C. 2022. “How Does Correlation Affect the Stock\nMarket?” 2022. https://www.investopedia.com/ask/answers/021716/how-does-correlation-affect-stock-market.asp.\n\n\nBrownlee, J. 2019. “How to Backtest Machine Learning Models for\nTime Series Forecasting.” 2019. https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/.\n\n\nBunch, James R, and Linda Kaufman. 1977. “Some Stable Methods for\nCalculating Inertia and Solving Symmetric Linear Systems.”\nMathematics of Computation 31 (137): 163–79.\n\n\nCai, Xiaoqiang, Kok Lay Teo, XQ Yang, and Xun Yu Zhou. 2004.\n“Minimax Portfolio Optimization: Empirical Numerical\nStudy.” Journal of the Operational Research Society 55\n(1): 65–72.\n\n\nCastillo, R. A., and R. Varela. 2010. ECONOMETRÍA PRÁCTICA:\nFundamentos de Series de Tiempo. México: Universidad Autónoma de\nBaja California.\n\n\nChen, J. 2022. “Today’s High.” 2022. https://www.investopedia.com/terms/t/todayshigh.asp.\n\n\nChirinos, S. 2018. “Series Cronológicas.” https://www.slideshare.net/SuedimarChirinos/series-cronologicas-119058959.\n2018.\n\n\nChollet, F., and J. J. Allaire. 2018. Deep Learning with r.\nManning Publications. https://books.google.es/books?id=xnIRtAEACAAJ.\n\n\nCNMV. n.d.a. “Glosario Financiero: Acción.” Accessed April\n24, 2023. https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&letra=A&idlng=1.\n\n\n———. n.d.b. “Glosario Financiero: Bolsa de Valores.”\nAccessed April 24, 2023. https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&letra=B&idlng=1.\n\n\n———. n.d.c. “Glosario Financiero: Servicio de Interconexión\nBursátil Español, SIBE.” Accessed April 24, 2023. https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&letra=S&idlng=1.\n\n\nDodge, Y. 2008. “Time Series.” In The Concise\nEncyclopedia of Statistics, 536–39. New York, NY: Springer New\nYork. https://doi.org/10.1007/978-0-387-32833-1_401.\n\n\nDowney, L. 2022. “Today’s Low.” 2022. https://www.investopedia.com/terms/t/todayslow.asp.\n\n\nDrucker, Harris, Christopher Burges, Linda Kaufman, Alex Smola, and\nVladimir Vapnik. 1996. “Linear Support Vector Regression\nMachines.” Advances in Neural Information Processing\nSystems 9 (9): 155–61.\n\n\nEdwards, J. 2022. “Why Market Correlation Matters?” 2022.\nhttps://www.investopedia.com/articles/financial-advisors/022516/4-reasons-why-market-correlation-matters.asp.\n\n\n“Empresas Cotizadas.” n.d. BME Exchange. Accessed May 21,\n2023. https://www.bolsasymercados.es/bme-exchange/es/Mercados-y-Cotizaciones/Acciones/Mercado-Continuo/Empresas-Cotizadas.\n\n\nEspallargas, S. D., and M. V. Solís. 2012. Econometría y Series\nTemporales: Aplicaciones. La Habana: Editorial Félix Varela.\n\n\nFletcher, Roger. 1971. “A General Quadratic Programming\nAlgorithm.” IMA Journal of Applied Mathematics 7 (1):\n76–91.\n\n\nGanti, A. 2020. “Adjusted Closing Price.” 2020. https://www.investopedia.com/terms/a/adjusted_closing_price.asp.\n\n\nGlen, S. 2023. “Mean Squared Error: Definition and\nExample.” 2023. https://www.statisticshowto.com/probability-and-statistics/statistics-definitions/mean-squared-error/.\n\n\nGoldfarb, Donald, and Ashok U. Idnani. 1982. “Dual and Primal-Dual\nMethods for Solving Strictly Convex Quadratic Programs.” In\nNumerical Analysis, edited by J. P. Hennart, 226–39. Berlin,\nHeidelberg: Springer Berlin Heidelberg.\n\n\n———. 1983. “A Numerically Stable Dual Method for Solving Strictly\nConvex Quadratic Programs.” Mathematical Programming 27:\n1–33.\n\n\nGoswami, Nababithi, Supriyo K. Mondal, and Swapan Paruya. 2012. “A\nComparative Study of Dual Active-Set and Primal-Dual Interior-Point\nMethod.” IFAC Proceedings Volumes 45 (15): 620–25.\nhttps://doi.org/https://doi.org/10.3182/20120710-4-SG-2026.00029.\n\n\nGu, Shihao, Bryan Kelly, and Dacheng Xiu. 2018. “Empirical Asset\nPricing via Machine Learning.” Working Paper 25398. Working Paper\nSeries. National Bureau of Economic Research. https://doi.org/10.3386/w25398.\n\n\nGunjan, Siddhartha, Abhishek & Bhattacharyya. 2023. “A brief review of portfolio optimization\ntechniques.” Artificial Intelligence Review 56\n(5): 3847–86. https://doi.org/10.1007/s10462-022-10273-7.\n\n\nGuresen, Erkam, Gulgun Kayakutlu, and Tugrul U. Daim. 2011. “Using\nArtificial Neural Network Models in Stock Market Index\nPrediction.” Expert Systems with Applications 38 (8):\n10389–97. https://doi.org/https://doi.org/10.1016/j.eswa.2011.02.068.\n\n\nHargrave, M. 2023. “Standard Deviation Formula and Uses Vs.\nVariance.” 2023. https://www.investopedia.com/terms/s/standarddeviation.asp.\n\n\nHayes, A. 2021. “What Is Closing Price? Definition, How It’s Used,\nand Example.” 2021. https://www.investopedia.com/terms/c/closingprice.asp.\n\n\n———. 2023. “Volatility: Meaning in Finance and How It Works with\nStocks.” 2023. https://www.investopedia.com/terms/v/volatility.asp.\n\n\nHaykin, Simon. 1998. Neural Networks: A Comprehensive\nFoundation. Prentice Hall PTR.\n\n\nHestenes, Magnus R. 1969. “Multiplier and Gradient\nMethods.” Journal of Optimization Theory and\nApplications 4 (5): 303–20.\n\n\nHestenes, Magnus R., and Eduard Stiefel. 1952. “Methods of\nConjugate Gradients for Solving Linear Systems.” Journal of\nResearch of the National Bureau of Standards 49: 409–35.\n\n\nHochreiter, Jürgen, Sepp & Schmidhuber. 1997. “Long\nShort-Term Memory.” Neural Computation 9 (8):\n1735–80. https://doi.org/10.1162/neco.1997.9.8.1735.\n\n\nIannone, Richard. 2023. DiagrammeR: Graph/Network\nVisualization. https://CRAN.R-project.org/package=DiagrammeR.\n\n\nIBM. 2021. “Characteristics of Time Series.” https://www.ibm.com/docs/en/spss-modeler/saas?topic=data-characteristics-time-series.\n2021.\n\n\nJing, Hong. 2020. “How Convolutional Layers Work in Deep Learning\nNeural Networks?” Jingles, Github Blog. 2020. https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/.\n\n\nJorion, Philippe. 2007. Value at Risk: The New Benchmark for\nManaging Financial Risk. The McGraw-Hill Companies, Inc.\n\n\nKarmarkar, Narendra. 1984. “A New Polynomial-Time Algorithm for\nLinear Programming.” In Proceedings of the Sixteenth Annual\nACM Symposium on Theory of Computing, 302–11.\n\n\nKenton, W. 2022. “Beta: Definition, Calculation, and Explanation\nfor Investors.” 2022. https://www.investopedia.com/terms/b/beta.asp.\n\n\nKocenda, E., and A. Cerný. 2017. Elements of Time Series\nEconometrics: An Applied Approach. Prague: Karolinum Press.\n\n\nKonno, Hiroshi, and Hiroaki Yamazaki. 1991. “Mean-Absolute\nDeviation Portfolio Optimization Model and Its Applications to Tokyo\nStock Market.” Management Science 37 (5): 519–31.\n\n\nLarrañaga, Iñaki & Moujahid, Pedro & Inza. 2007. “Tema 14.\nRedes Neuronales.” Departamento de Ciencias de la Computaci´on e\nInteligencia Artificial, Universidad del Pa´ıs Vasco–Euskal Herriko\nUnibertsitatea. 2007. http://www.sc.ehu.es/ccwbayes/docencia/mmcc/docs/t14-neuronales.pdf.\n\n\nLecun, Y., L. Bottou, Y. Bengio, and P. Haffner. 1998.\n“Gradient-Based Learning Applied to Document Recognition.”\nProceedings of the IEEE 86 (11): 2278–2324. https://doi.org/10.1109/5.726791.\n\n\nMarkowitz, Harry M, and Harry M Markowitz. 1967. Portfolio\nSelection: Efficient Diversification of Investments. J. Wiley.\n\n\nMcCarthy, John, Marvin L. Minsky, Nathaniel Rochester, and Claude E.\nShannon. 2006. “A Proposal for the Dartmouth Summer Research\nProject on Artificial Intelligence, August 31, 1955.” AI\nMagazine 27 (4): 12. https://doi.org/10.1609/aimag.v27i4.1904.\n\n\nMitchell, C. 2020. “Market Price: Definition, Meaning, How to\nDetermine, and Example.” 2020. https://www.investopedia.com/terms/m/market-price.asp.\n\n\nMonaghan, B. 2019. “Correlation Vs. Beta: What Is the Difference\nand Why Does It Matter?” 2019. https://www.mackenzieinvestments.com/content/dam/final/corporate/mackenzie/docs/investment-teams/multi-asset-team/en/Correlation%20vs.%20Beta_%20What%20is%20The%20Difference%20and%20Why%20Does%20It%20Matter_%20_%20Mackenzie%20Investments.pdf.\n\n\nNandakumar, S. 2020. “How Can r-Squared Be Negative When the\nCorrelation Between Prediction and Truth Is Positive?” 2020. https://stackoverflow.com/a/63311778/12660035.\n\n\nOlah, Christopher. 2015. “Understanding LSTM Networks.”\nColah’s blog. 2015. https://colah.github.io/posts/2015-08-Understanding-LSTMs/.\n\n\nOmG. 2021. “Difference Between ReLU, ELU and Leaky ReLU. Their\nPros and Cons Majorly.” 2021. https://datascience.stackexchange.com/a/102485.\n\n\nPinset, W. 2021. “Understanding Stock Prices and Values.”\n2021. https://www.investopedia.com/articles/stocks/08/stock-prices-fool.asp.\n\n\nPowell, Michael JD. 1969. “A Method for Nonlinear Constraints in\nMinimization Problems.” Optimization, 283–98.\n\n\nRallabandi, S. 2023. “Activation Functions ReLU Vs. Leaky\nReLU.” 2023. https://medium.com/mlearning-ai/activation-functions-relu-vs-leaky-relu-b8272dc0b1be.\n\n\nRockafellar, R Tyrrell, and Stanislav Uryasev. 2002. “Conditional\nValue-at-Risk for General Loss Distributions.” Journal of\nBanking & Finance 26 (7): 1443–71.\n\n\nRosen, JB. 1961. “The Gradient Projection Method for Nonlinear\nProgramming. Part II. Nonlinear Constraints.” Journal of the\nSociety for Industrial and Applied Mathematics 9 (4): 514–32.\n\n\nRosen, Jo Bo. 1960. “The Gradient Projection Method for Nonlinear\nProgramming. Part i. Linear Constraints.” Journal of the\nSociety for Industrial and Applied Mathematics 8 (1): 181–217.\n\n\nRoss, S. 2022. “How Do i Calculate Correlation Between Market\nIndicators and Specific Stocks?” 2022. https://www.investopedia.com/ask/answers/032315/how-do-i-calculate-correlation-between-market-indicators-and-specific-stocks.asp.\n\n\nRuiz, M. C. 2011. “Tema 5: Procesos Estocásticos.” http://www.dmae.upct.es/~mcruiz/Telem06/Teoria/apuntes_procesos.pdf;\nDepartamento de Matemática y Estadística. Universidad Politécnica de\nCartagena. 2011.\n\n\nRyan, Jeffrey A., and Joshua M. Ulrich. 2023. Quantmod: Quantitative\nFinancial Modelling Framework. https://CRAN.R-project.org/package=quantmod.\n\n\nSamuelson, Paul A. 1970. “The Fundamental Approximation Theorem of\nPortfolio Analysis in Terms of Means, Variances and Higher\nMoments.” The Review of Economic Studies 37 (4): 537–42.\n\n\nSezer, Omer Berat, Mehmet Ugur Gudelek, and Ahmet Murat Ozbayoglu. 2020.\n“Financial Time Series Forecasting with Deep Learning : A\nSystematic Literature Review: 2005–2019.” Applied Soft\nComputing 90: 106181. https://doi.org/https://doi.org/10.1016/j.asoc.2020.106181.\n\n\nShenoy, Catherine, and Prakash P Shenoy. 2000. “Bayesian Network\nModels of Portfolio Risk and Return.” In. The MIT Press.\n\n\nSiddiqui, J. Rafid. 2023. “Why Convolve? Understanding Convolution\nand Feature Extraction in Deep Networks.” Medium, Towards Data\nScience. 2023. https://towardsdatascience.com/why-convolve-understanding-convolution-and-feature-extraction-in-deep-networks-ee45d1fdd17c.\n\n\nSutton, Richard S, and Andrew G Barto. 2018. Reinforcement Learning:\nAn Introduction. MIT press.\n\n\nTealab, Ahmed. 2018. “Time Series Forecasting Using Artificial\nNeural Networks Methodologies: A Systematic Review.” Future\nComputing and Informatics Journal 3 (2): 334–40. https://doi.org/https://doi.org/10.1016/j.fcij.2018.10.003.\n\n\nTeam, CFI. 2023. “What Is Stock Price?” 2023. https://corporatefinanceinstitute.com/resources/capital-markets/stock-price/.\n\n\nTeam, The Investopedia. 2022. “Intrinsic Value Defined and How\nIt’s Determined in Investing and Business.” 2022. https://www.investopedia.com/terms/i/intrinsicvalue.asp.\n\n\nVillagarcía, T. 2006. “Series Temporales.” https://halweb.uc3m.es/fjnm/estind/doc_grupo1/archivos/Apuntes%20de%20series.pdf.\n2006.\n\n\nVillavicencio, J. 2010. “Introducción a Las Series de\nTiempo.” http://www.estadisticas.gobierno.pr/iepr/LinkClick.aspx;\nInstituto de estadística de Puerto Rico. 2010.\n\n\nWalker, Ryan. 2014. “Solving Quadratic Progams with r’s Quadprog\nPackage.” rwalk. 2014. https://rwalk.xyz/solving-quadratic-progams-with-rs-quadprog-package/.\n\n\nWong, W. K., and Z. X. Guo. 2010. “A hybrid\nintelligent model for medium-term sales forecasting in fashion retail\nsupply chains using extreme learning machine and harmony search\nalgorithm.” International Journal of Production\nEconomics 128 (2): 614–24. https://ideas.repec.org/a/eee/proeco/v128y2010i2p614-624.html.",
    "crumbs": [
      "Bibliography"
    ]
  },
  {
    "objectID": "Annex1.html",
    "href": "Annex1.html",
    "title": "Annex. 1 Figures",
    "section": "",
    "text": "Figure 1: Relation between IA-ML-DL\n\n\n\n\n\n\n\nTaken from: Deep learning with R by Chollet and Allaire (2018).\n\n\n\n\nFigure 2: Classic programming and machine learning\n\n\n\n\n\n\n\nTaken from: Deep learning with R by Chollet and Allaire (2018).\n\n\n\n\nFigure 3: Basic structure of an artificial neural network\n\n\n\n\n\n\n\nTaken from: Topic 14: neural networks of Larrañaga (2007).\n\n\n\n\n\n\nFigure 4: How the size of the filter affects the output vector\n\n\n\n\n\n\n\n\n\nOwn elaboration: Elaborated from Jing (2020). Shows how the size of the output vector changes according to the size of the filter that is used.\n\n\n\n\n\n\nFigure 5: How stride affects the output vector\n\n\n\n\n\n\n\n\n\nOwn elaboration: Elaborated from Jing (2020). Shows how the stride parameter affects the size of the output vector.\n\n\n\n\n\n\nFigure 6: How dilation affects the output vector\n\n\n\n\n\n\n\n\n\nOwn elaboration: Elaborated from Jing (2020). Show how the dilation parameter affects the size of the output vector.\n\n\n\n\n\n\nFigure 7: How padding affects the output vector\n\n\n\n\n\n\n\n\n\nOwn elaboration: Elaborated from Jing (2020). Show how the padding parameter affects the size of the output vector.\n\n\n\n\nFigure 8: Deployment of the loop of a standard recurrent neural network\n\n\n\n\n\n\n\nTaken from: Understanding LSTM networks, Olah (2015).\n\n\n\n\nFigure 9: Nearby relevant information\n\n\n\n\n\n\n\nTaken from: Understanding LSTM networks, Olah (2015).\n\n\n\n\nFigure 10: Distant relevant information\n\n\n\n\n\n\n\nTaken from: Understanding LSTM networks, Olah (2015).\n\n\n\n\nFigure 11: Difference Between Repeat Modules\n\n\n\n\n\n\n\nTaken from: Understanding LSTM networks, Olah (2015).\n\n\n\n\nFigure 12: LSTM functionality: Representation of step 1\n\n\n\n\n\n\n\nTaken from: Understanding LSTM networks, Olah (2015).\n\n\n\n\nFigure 13: LSTM functionality: Representation of step 2\n\n\n\n\n\n\n\nTaken from: Understanding LSTM networks, Olah (2015).\n\n\n\n\nFigure 14: LSTM functionality: Representation of step 3\n\n\n\n\n\n\n\nTaken from: Understanding LSTM networks, Olah (2015).\n\n\n\n\nFigure 15: LSTM functionality: Representation of step 4\n\n\n\n\n\n\n\nTaken from: Understanding LSTM networks, Olah (2015).\n\n\n\n\n\n\nFigure 16: Input and output vector display\n\n\n\n\n\n\n\n\n\nOwn elaboration: Made from an image in Chollet and Allaire (2018). It shows what the three-dimensional input and output vectors for a company’s data look like, if three observations are used to create the input vector.\n\n\n\n\nFigure 17: Different structures depending on the different sizes of input vectors\n\n\n\n\n\n\n\nOwn elaboration: Elaborated from the different models built using the keras and tensorflow packages in R and were graphed using the Iannone (2023) package.\n\n\n\n\nFigure 18: ReLU and Leaky ReLU domain\n\n\n\n\n\n\n\nOwn elaboration: Elaborated from the images that can be seen in Rallabandi (2023).\n\n\n\n\nFigure 19: Flowchart of the Walk Forward Validation methodology\n\n\n\n\n\n\n\nOwn elaboration\n\n\n\n\n\nChollet, F., and J. J. Allaire. 2018. Deep Learning with r. Manning Publications. https://books.google.es/books?id=xnIRtAEACAAJ.\n\n\nIannone, Richard. 2023. DiagrammeR: Graph/Network Visualization. https://CRAN.R-project.org/package=DiagrammeR.\n\n\nJing, Hong. 2020. “How Convolutional Layers Work in Deep Learning Neural Networks?” Jingles, Github Blog. 2020. https://jinglescode.github.io/2020/11/01/how-convolutional-layers-work-deep-learning-neural-networks/.\n\n\nLarrañaga, Iñaki & Moujahid, Pedro & Inza. 2007. “Tema 14. Redes Neuronales.” Departamento de Ciencias de la Computaci´on e Inteligencia Artificial, Universidad del Pa´ıs Vasco–Euskal Herriko Unibertsitatea. 2007. http://www.sc.ehu.es/ccwbayes/docencia/mmcc/docs/t14-neuronales.pdf.\n\n\nOlah, Christopher. 2015. “Understanding LSTM Networks.” Colah’s blog. 2015. https://colah.github.io/posts/2015-08-Understanding-LSTMs/.\n\n\nRallabandi, S. 2023. “Activation Functions ReLU Vs. Leaky ReLU.” 2023. https://medium.com/mlearning-ai/activation-functions-relu-vs-leaky-relu-b8272dc0b1be.",
    "crumbs": [
      "Annex",
      "Annex. 1 Figures"
    ]
  },
  {
    "objectID": "Annex2.html",
    "href": "Annex2.html",
    "title": "Annex. 2 Graphics",
    "section": "",
    "text": "Figure 1: Bullish and heteroscedastic trend\n\n\n\n\n\n\n\n\n\nOwn elaboration: By using RStudio with the IBEX historical database, obtained from https://finance.yahoo.com/, in the period between 01-1995 to 01-1997.\n\n\n\n\n\n\nFigure 2: Bearish and heteroscedastic trend\n\n\n\n\n\n\n\n\n\nOwn elaboration: Through the use of RStudio with the IBEX historical database, obtained from https://finance.yahoo.com/, in the period between 01-2000 to 01-2003.\n\n\n\n\n\n\nFigure 3: No trend, homoscedastic and stationary\n\n\n\n\n\n\n\n\n\nOwn elaboration: By using RStudio with the IBEX historical database obtained from https://finance.yahoo.com/, in the period from 01-2000 to 01-2003, using the returns calculated from the closing price.\n\n\n\n\n\n\nFigure 4: Decomposition: seasonality and error\n\n\n\n\n\n\n\n\n\nOwn elaboration: By using RStudio with the IBEX historical database obtained from https://finance.yahoo.com/, decomposing the time series made up of the observations that cover the period from 01-2000 to 01-2023.\n\n\n\n\n\n\nFigure 5: Correlogram\n\n\n\n\n\n\n\n\n\nOwn elaboration: By using RStudio.\n\n\n\n\n\n\nFigure 6: Constant trend in the adjusted closing prices of the company “Nueva Expresión Textil S.A”\n\n\n\n\n\n\n\n\n\nOwn elaboration: Based on the data obtained from  (n.d.) corresponding to the company “Nueva Expresión Textil S.A” in the period from January 31, 2000 to February 28, 2023.\n\n\n\n\n\n\nFigure 7: Sudden price changes that reflect an erroneous calculation of adjusted closing prices, “BANKINTER,S.A.”\n\n\n\n\n\n\n\n\n\nOwn elaboration: Based on the data obtained from  (n.d.) corresponding to the company “Nueva Expresión Textil S.A” in the period from January 31, 2000 to February 28, 2023.\n\n\n\n\n\n\nFigure 8: Evolution of indicators – Entries with one observation\n\n\n\n\n\n\n\n\n\nOwn elaboration: Through the use of R and Rstudio.\n\n\n\n\n\n\nFigure 9: Evolution of the indicators – Entries with two observations\n\n\n\n\n\n\n\n\n\nOwn elaboration: Through the use of R and Rstudio.\n\n\n\n\n\n\nFigure 10: Evolution of the indicators – Entries with three observations\n\n\n\n\n\n\n\n\n\nOwn elaboration: Through the use of R and Rstudio.\n\n\n\n\n\n\nFigure 11: Evolution of portfolios and the IBEX – Entries with one observation\n\n\n\n\n\n\n\n\n\nOwn elaboration: Through the use of R and Rstudio.\n\n\n\n\n\n\nFigure 12: Evolution of the portfolios and the IBEX – Entries with two observations\n\n\n\n\n\n\n\n\n\nOwn elaboration: Through the use of R and Rstudio.\n\n\n\n\n\n\nFigure 13: Evolution of the portfolios and the IBEX – Entries with three observations\n\n\n\n\n\n\n\n\n\nOwn elaboration: Through the use of R and Rstudio.\n\n\n\n\n\nn.d. Yahoo Finance. https://finance.yahoo.com/.",
    "crumbs": [
      "Annex",
      "Annex. 2 Graphics"
    ]
  },
  {
    "objectID": "Annex3.html",
    "href": "Annex3.html",
    "title": "Annex. 3 Tables",
    "section": "",
    "text": "Table 1: Price data structure\n\n\n\n\n\nDate\nOpen\nHigh\nLow\nClose\nVolume\nAdjusted\n\n\n\n\n2001-05-24\n3.600\n3.620\n3.510\n3.608\n216270100\n-0.1317839\n\n\n2001-05-25\n3.600\n3.676\n3.580\n3.602\n50448300\n-0.1315648\n\n\n2001-05-28\n3.560\n3.604\n3.544\n3.580\n26118945\n-0.1307612\n\n\n2001-05-29\n3.562\n3.626\n3.562\n3.614\n26910070\n-0.1320031\n\n\n2001-05-30\n3.606\n3.648\n3.602\n3.620\n48229995\n-0.1322222\n\n\n2001-05-31\n3.620\n3.676\n3.610\n3.670\n24806710\n-0.1340484\n\n\n\n\n\n\n\nOwn elaboration: Through the use of RStudio with the historical database of “INDITEX”, obtained from https://finance.yahoo.com/, in the period between 05-24-2001 to 05-31-2001.\n\n\n\n\n\nTable 2: List of listed companies\n\n\n\n\n\n\n\n\n\n\n\n\n\nNAME\nTICKERS\nSECTOR-SUBSECTOR\nMARKET\nINDEX\nSELECTED\n\n\n\n\nACCIONA,S.A.\nANA.MC\nMat.Basicos, Industria y Construcción - Construcción\nMC\nIBEX 35®, IGBM\nX\n\n\nACERINOX, S.A.\nACX.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nACS,ACTIVIDADES DE CONST.Y SERVICIOS S.A\nACS.MC\nMat.Basicos, Industria y Construcción - Construcción\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nADOLFO DOMINGUEZ, S.A.\nADZ.MC\nBienes de Consumo - Textil, Vestido y Calzado\nMC\nIGBM\nX\n\n\nAEDAS HOMES, S.A.\nAEDAS.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nIGBM\nX\n\n\nAENA, S.M.E., S.A.\nAENA.MC\nServicios de Consumo - Transporte y Distribución\nMC\nIBEX 35®, IGBM\nX\n\n\nAIRBUS SE\nAIR.MC\nMat.Basicos, Industria y Construcción - Aerospacial\nMC\nIGBM\nX\n\n\nAIRTIFICIAL INTELLIGENCE STRUCTURES S.A.\nAI.MC\nMat.Basicos, Industria y Construcción - Ingeniería y Otros\nMC\nIGBM\n\n\n\nALANTRA PARTNERS, S.A.\nALNT.MC\nServicios Financieros - Cartera y Holding\nMC\nIGBM\n\n\n\nALMIRALL, S.A.\nALM.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIGBM\nX\n\n\nAMADEUS IT GROUP, S.A.\nAMS.MC\nTecnología y Telecomunicaciones - Electrónica y Software\nMC\nIBEX 35®, IGBM\nX\n\n\nAMPER, S.A.\nAMP.MC\nTecnología y Telecomunicaciones - Electrónica y Software\nMC\nIGBM\nX\n\n\nAMREST HOLDINGS, S.E.\nEAT.MC\nServicios de Consumo - Ocio, Turismo y Hostelería\nMC\nIGBM\nX\n\n\nAPERAM, SOCIETE ANONYME\nAPAM.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nNA\nX\n\n\nAPPLUS SERVICES, S.A.\nAPPS.MC\nMat.Basicos, Industria y Construcción - Ingeniería y Otros\nMC\nIGBM\nX\n\n\nARCELORMITTAL, S.A.\nMTS.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nIBEX 35®, IGBM\nX\n\n\nÁRIMA REAL ESTATE SOCIMI, S.A.\nARM.MC\nServicios Inmobiliarios - SOCIMI\nMC\nIGBM\nX\n\n\nATRESMEDIA CORP. DE MEDIOS DE COM. S.A.\nA3M.MC\nServicios de Consumo - Medios de Comunicación y Publicidad\nMC\nIGBM\nX\n\n\nATRYS HEALTH, S.A.\nATRY.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIGBM\nX\n\n\nAUDAX RENOVABLES, S.A.\nADX.MC\nPetróleo y Energía - Energías Renovables\nMC\nIGBM\n\n\n\nAZKOYEN S.A.\nAZK.MC\nMat.Basicos, Industria y Construcción - Fabric. y Montaje Bienes de Equipo\nMC\nIGBM\nX\n\n\nBANCO BILBAO VIZCAYA ARGENTARIA, S.A.\nBBVA.MC\nServicios Financieros - Bancos y Cajas de Ahorro\nMC\nIBEX 35®, IGBM\nX\n\n\nBANCO DE SABADELL, S.A.\nSAB.MC\nServicios Financieros - Bancos y Cajas de Ahorro\nMC\nIBEX 35®, IGBM\nX\n\n\nBANCO SANTANDER, S.A.\nSAN.MC\nServicios Financieros - Bancos y Cajas de Ahorro\nMC\nIBEX 35®, IGBM\nX\n\n\nBANKINTER,S.A.\nBKT.MC\nServicios Financieros - Bancos y Cajas de Ahorro\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nBERKELEY ENERGIA LIMITED\nBKY.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nIGBM\nX\n\n\nBODEGAS RIOJANAS, S.A.\nRIO.MC\nBienes de Consumo - Alimentación y Bebidas\nMC\nIGBM\nX\n\n\nBORGES AGRICULTURAL & INDUST. NUTS, S.A.\nBAIN.MC\nBienes de Consumo - Alimentación y Bebidas\nMC\nNA\n\n\n\nCAIXABANK, S.A.\nCABK.MC\nServicios Financieros - Bancos y Cajas de Ahorro\nMC\nIBEX 35®, IGBM\nX\n\n\nCAJA DE AHORROS DEL MEDITERRANEO\nCAM.MC\nServicios Financieros - Bancos y Cajas de Ahorro\nMC\nNA\n\n\n\nCASH, S.A.\nCASH.MC\nServicios de Consumo - Otros Servicios\nMC\nIGBM, IBEXTD®\nX\n\n\nCELLNEX TELECOM, S.A.\nCLNX.MC\nTecnología y Telecomunicaciones - Telecomunicaciones y Otros\nMC\nIBEX 35®, IGBM\nX\n\n\nCIA. DE DIST. INTEG. LOGISTA HOLDINGS\nLOG.MC\nServicios de Consumo - Transporte y Distribución\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nCIA. ESPAÑOLA VIVIENDAS EN ALQUILER,S.A\nCEV.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nNA\n\n\n\nCIA.LEVANTINA, EDIFICACION DE O.PUBLICAS\nCLEO.MC\nMat.Basicos, Industria y Construcción - Construcción\nMC\nNA\n\n\n\nCIE AUTOMOTIVE, S.A.\nCIE.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nIGBM\nX\n\n\nCLINICA BAVIERA, S.A.\nCBAV.MC\nServicios de Consumo - Otros Servicios\nMC\nIGBM\nX\n\n\nCOCA-COLA EUROPACIFIC PARTNERS PLC\nCCEP.MC\nBienes de Consumo - Alimentación y Bebidas\nMC\nIGBM\nX\n\n\nCONSTRUCC. Y AUX. DE FERROCARRILES, S.A.\nCAF.MC\nMat.Basicos, Industria y Construcción - Fabric. y Montaje Bienes de Equipo\nMC\nIGBM\nX\n\n\nCORP. ACCIONA ENERGÍAS RENOVABLES, S.A.\nANE.MC\nPetróleo y Energía - Energías Renovables\nMC\nIBEX 35®, IGBM\nX\n\n\nCORPORACION FINANCIERA ALBA, S.A.\nALB.MC\nServicios Financieros - Cartera y Holding\nMC\nIGBM\nX\n\n\nDEOLEO, S.A.\nOLE.MC\nBienes de Consumo - Alimentación y Bebidas\nMC\nIGBM\nX\n\n\nDESA\nDESA.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nNA\n\n\n\nDIA-DISTRIBUIDORA INT. DE ALIMENT. S.A.\nDIA.MC\nServicios de Consumo - Comercio\nMC\nIGBM\nX\n\n\nDURO FELGUERA, S.A.\nMDF.MC\nMat.Basicos, Industria y Construcción - Ingeniería y Otros\nMC\nIGBM\nX\n\n\nEBRO FOODS, S.A.\nEBRO.MC\nBienes de Consumo - Alimentación y Bebidas\nMC\nIGBM, IBEXTD®\nX\n\n\nEDREAMS ODIGEO, S.A.\nEDR.MC\nServicios de Consumo - Ocio, Turismo y Hostelería\nMC\nIGBM\nX\n\n\nELECNOR S. A.\nENO.MC\nMat.Basicos, Industria y Construcción - Fabric. y Montaje Bienes de Equipo\nMC\nIGBM, IBEXTD®\nX\n\n\nENAGAS, S.A.\nENG.MC\nPetróleo y Energía - Electricidad y Gas\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nENCE ENERGIA Y CELULOSA, S.A.\nENC.MC\nBienes de Consumo - Papel y Artes Gráficas\nMC\nIGBM\nX\n\n\nENDESA, SOCIEDAD ANONIMA\nELE.MC\nPetróleo y Energía - Electricidad y Gas\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nERCROS S.A.\nECR.MC\nMat.Basicos, Industria y Construcción - Industria Química\nMC\nIGBM\nX\n\n\nFAES FARMA, S.A.\nFAE.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIGBM, IBEXTD®\nX\n\n\nFERROVIAL, S.A.\nFER.MC\nMat.Basicos, Industria y Construcción - Construcción\nMC\nIBEX 35®, IGBM\nX\n\n\nFLUIDRA, S.A.\nFDR.MC\nMat.Basicos, Industria y Construcción - Ingeniería y Otros\nMC\nIBEX 35®, IGBM\nX\n\n\nFOMENTO DE CONSTR. Y CONTRATAS S.A.\nFCC.MC\nMat.Basicos, Industria y Construcción - Construcción\nMC\nIGBM, IBEXTD®\nX\n\n\nGENERAL DE ALQUILER DE MAQUINARIA, S.A.\nGAM.MC\nMat.Basicos, Industria y Construcción - Ingeniería y Otros\nMC\nIGBM\n\n\n\nGESTAMP AUTOMOCION, S.A.\nGEST.MC\nMat.Basicos, Industria y Construcción - Fabric. y Montaje Bienes de Equipo\nMC\nIGBM\nX\n\n\nGLOBAL DOMINION ACCESS, S.A.\nDOM.MC\nTecnología y Telecomunicaciones - Telecomunicaciones y Otros\nMC\nIGBM\nX\n\n\nGRENERGY RENOVABLES, S.A.\nGRE.MC\nPetróleo y Energía - Energías Renovables\nMC\nIGBM\nX\n\n\nGRIFOLS, S.A.\nGRF.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIBEX 35®, IGBM\nX\n\n\nGRUPO CATALANA OCCIDENTE, S.A.\nGCO.MC\nServicios Financieros - Seguros\nMC\nIGBM\nX\n\n\nGRUPO ECOENER, S.A.\nENER.MC\nPetróleo y Energía - Energías Renovables\nMC\nIGBM\nX\n\n\nGRUPO EMPRESARIAL SAN JOSE, S.A.\nGSJ.MC\nMat.Basicos, Industria y Construcción - Construcción\nMC\nIGBM\nX\n\n\nGRUPO EZENTIS, S.A.\nEZE.MC\nTecnología y Telecomunicaciones - Telecomunicaciones y Otros\nMC\nNA\nX\n\n\nIBERDROLA, S.A.\nIBE.MC\nPetróleo y Energía - Electricidad y Gas\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nIBERPAPEL GESTION, S.A.\nIBG.MC\nBienes de Consumo - Papel y Artes Gráficas\nMC\nIGBM\nX\n\n\nINDRA SISTEMAS, S.A., SERIE A\nIDR.MC\nTecnología y Telecomunicaciones - Electrónica y Software\nMC\nIBEX 35®, IGBM\nX\n\n\nINDUSTRIA DE DISEÑO TEXTIL, SA “INDITEX”\nITX.MC\nBienes de Consumo - Textil, Vestido y Calzado\nMC\nIBEX 35®, IGBM\nX\n\n\nINMOBILIARIA COLONIAL SOCIMI, S.A.\nCOL.MC\nServicios Inmobiliarios - SOCIMI\nMC\nIBEX 35®, IGBM\nX\n\n\nINMOBILIARIA DEL SUR, S.A.\nISUR.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nIGBM\nX\n\n\nINNOVATIVE SOLUTIONS ECOSYSTEM, S.A.\nISE.MC\nServicios de Consumo - Comercio\nMC\nNA\n\n\n\nINTERNATIONAL CONSOLIDAT. AIRLINES GROUP\nIAG.MC\nServicios de Consumo - Transporte y Distribución\nMC\nIBEX 35®, IGBM\nX\n\n\nLABORATORIO REIG JOFRE, S.A.\nRJF.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIGBM\nX\n\n\nLABORATORIOS FARMACEUTICOS ROVI, S.A.\nROVI.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIBEX 35®, IGBM\nX\n\n\nLAR ESPAÑA REAL ESTATE, SOCIMI, S.A.\nLRE.MC\nServicios Inmobiliarios - SOCIMI\nMC\nIGBM, IBEXTD®\nX\n\n\nLIBERTAS 7, S.A.\nLIB.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nNA\n\n\n\nLINEA DIRECTA ASEGURADORA, S.A.\nLDA.MC\nServicios Financieros - Seguros\nMC\nIGBM\nX\n\n\nLINGOTES ESPECIALES, S.A.\nLGT.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nNA\nX\n\n\nMAPFRE, S.A.\nMAP.MC\nServicios Financieros - Seguros\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nMELIA HOTELS INTERNATIONAL, S.A.\nMEL.MC\nServicios de Consumo - Ocio, Turismo y Hostelería\nMC\nIBEX 35®, IGBM\nX\n\n\nMERLIN PROPERTIES, SOCIMI, S.A.\nMRL.MC\nServicios Inmobiliarios - SOCIMI\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nMETROVACESA, S.A.\nMVC.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nIGBM\nX\n\n\nMIQUEL Y COSTAS & MIQUEL, S.A.\nMCM.MC\nBienes de Consumo - Papel y Artes Gráficas\nMC\nIGBM, IBEXTD®\nX\n\n\nMONTEBALITO, S.A.\nMTB.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nNA\nX\n\n\nNATURGY ENERGY GROUP, S.A.\nNTGY.MC\nPetróleo y Energía - Electricidad y Gas\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nNATURHOUSE HEALTH, S.A.\nNTH.MC\nBienes de Consumo - Alimentación y Bebidas\nMC\nIGBM\nX\n\n\nNEINOR HOMES, S.A.\nHOME.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nIGBM\nX\n\n\nNH HOTEL GROUP, S.A.\nNHH.MC\nServicios de Consumo - Ocio, Turismo y Hostelería\nMC\nIGBM\nX\n\n\nNICOLAS CORREA S.A.\nNEA.MC\nMat.Basicos, Industria y Construcción - Fabric. y Montaje Bienes de Equipo\nMC\nIGBM, IBEXTD®\nX\n\n\nNUEVA EXPRESION TEXTIL, S.A.\nNXT.MC\nBienes de Consumo - Textil, Vestido y Calzado\nMC\nIGBM\n\n\n\nNYESA VALORES CORPORACION, S.A.\nNYE.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nIGBM\n\n\n\nOBRASCON HUARTE LAIN, S.A.\nOHLA.MC\nMat.Basicos, Industria y Construcción - Construcción\nMC\nIGBM\nX\n\n\nOPDENERGY HOLDING, S.A.\nOPDE.MC\nPetróleo y Energía - Energías Renovables\nMC\nIGBM\nX\n\n\nORYZON GENOMICS, S.A.\nORY.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIGBM\nX\n\n\nPESCANOVA, S.A.\nPVA.MC\nBienes de Consumo - Alimentación y Bebidas\nMC\nIGBM\n\n\n\nPHARMA MAR, S.A.\nPHM.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIGBM\nX\n\n\nPRIM, S.A.\nPRM.MC\nBienes de Consumo - Productos farmaceúticos y Biotecnología\nMC\nIGBM\nX\n\n\nPROMOTORA DE INFORMACIONES,S.A.\nPRS.MC\nServicios de Consumo - Medios de Comunicación y Publicidad\nMC\nIGBM\nX\n\n\nPROSEGUR , CIA. DE SEGURIDAD, S.A.\nPSG.MC\nServicios de Consumo - Otros Servicios\nMC\nIGBM, IBEXTD®\nX\n\n\nREALIA BUSINESS, S.A.\nRLIA.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nIGBM\nX\n\n\nRED ELECTRICA CORPORACION, S.A.\nRED.MC\nPetróleo y Energía - Electricidad y Gas\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nRENTA 4 BANCO, S.A.\nR4.MC\nServicios Financieros - Servicios de Inversión\nMC\nIGBM\nX\n\n\nRENTA CORPORACION REAL ESTATE, S.A.\nREN.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nIGBM\n\n\n\nREPSOL, S.A.\nREP.MC\nPetróleo y Energía - Petróleo\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nSACYR, S.A.\nSCYR.MC\nMat.Basicos, Industria y Construcción - Construcción\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nSOLARIA ENERGIA Y MEDIO AMBIENTE, S.A.\nSLR.MC\nPetróleo y Energía - Energías Renovables\nMC\nIBEX 35®, IGBM\nX\n\n\nSOLTEC POWER HOLDINGS, S.A.\nSOL.MC\nPetróleo y Energía - Energías Renovables\nMC\nIGBM\nX\n\n\nSQUIRREL MEDIA, S.A\nSQRL.MC\nServicios de Consumo - Medios de Comunicación y Publicidad\nMC\nNA\n\n\n\nTALGO, S.A.\nTLGO.MC\nMat.Basicos, Industria y Construcción - Fabric. y Montaje Bienes de Equipo\nMC\nIGBM\nX\n\n\nTECNICAS REUNIDAS, S.A.\nTRE.MC\nMat.Basicos, Industria y Construcción - Ingeniería y Otros\nMC\nIGBM\nX\n\n\nTELEFONICA, S.A.\nTEF.MC\nTecnología y Telecomunicaciones - Telecomunicaciones y Otros\nMC\nIBEX 35®, IGBM, IBEXTD®\nX\n\n\nTUBACEX, S.A.\nTUB.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nIGBM\nX\n\n\nTUBOS REUNIDOS,S.A.\nTRG.MC\nMat.Basicos, Industria y Construcción - Mineral, Metales y Transformación\nMC\nIGBM\nX\n\n\nUNICAJA BANCO, S.A.\nUNI.MC\nServicios Financieros - Bancos y Cajas de Ahorro\nMC\nIBEX 35®, IGBM\nX\n\n\nURBAS GRUPO FINANCIERO, S.A.\nUBS.MC\nServicios Inmobiliarios - Inmobiliarias y Otros\nMC\nIGBM\n\n\n\nVIDRALA S.A.\nVID.MC\nBienes de Consumo - Otros Bienes de Consumo\nMC\nIGBM\nX\n\n\nVISCOFAN, S.A.\nVIS.MC\nBienes de Consumo - Alimentación y Bebidas\nMC\nIGBM, IBEXTD®\nX\n\n\nVOCENTO, S.A.\nVOC.MC\nServicios de Consumo - Medios de Comunicación y Publicidad\nMC\nIGBM\nX\n\n\n\n\n\n\n\n\nObtained from: The information displayed on the official site of the Spanish Stock Exchanges and Markets, “Empresas Cotizadas” (n.d.). Note: MC in MARKET means Continuous Market, IBEXTD in INDEX means IBEX TOP Dividend.\n\n\n\n\nTable 3: Amounts of samples used to train the models\n\n\n\n\n\nInputs\nTotal.samples\n\n\n\n\n1\n17347\n\n\n2\n17244\n\n\n3\n17141\n\n\n\n\n\n\n\nOwn elaboration\n\n\n\n\nTable 4: Best and best companies according to the results obtained from the calculations of the indicators\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n\nTICKER\nR2\nMSE\nTICKER\nR2\nMSE\nTICKER\nR2\nMSE\n\n\n\n\nOPDE.MC\n0.4976431\n0.0140459\nOPDE.MC\n0.3889262\n0.0119095\nOPDE.MC\n0.4861395\n0.0121918\n\n\nOLE.MC\n0.1408607\n0.2092006\nSOL.MC\n0.3037137\n0.0225800\nANE.MC\n0.3485550\n0.0063473\n\n\nANE.MC\n0.1073977\n0.0066219\nANE.MC\n0.1965812\n0.0064902\nSOL.MC\n0.3160949\n0.0207540\n\n\nDOM.MC\n0.1032658\n0.0055531\nBKY.MC\n0.1687609\n0.1873538\nBKY.MC\n0.1983894\n0.2024927\n\n\nMVC.MC\n0.1005222\n0.0095915\nTLGO.MC\n0.1390272\n0.0087595\nLOG.MC\n0.1928642\n0.0045066\n\n\nBKY.MC\n0.0612159\n0.2068808\nOLE.MC\n0.1077772\n0.2036521\nLDA.MC\n0.1502733\n0.0035126\n\n\nTLGO.MC\n0.0586490\n0.0091608\nAEDAS.MC\n0.1069500\n0.0069291\nTLGO.MC\n0.1165425\n0.0081263\n\n\nMRL.MC\n0.0558508\n0.0062275\nMVC.MC\n0.0990202\n0.0097977\nOLE.MC\n0.0676713\n0.2072154\n\n\nAEDAS.MC\n0.0504877\n0.0072116\nENER.MC\n0.0977418\n0.0098403\nCBAV.MC\n0.0590027\n0.0116354\n\n\nNTH.MC\n0.0496536\n0.0108446\nCCEP.MC\n0.0865251\n0.0041225\nENER.MC\n0.0523626\n0.0111998\n\n\nEBRO.MC\n-0.1131305\n0.0026464\nAENA.MC\n-0.0116025\n0.0059543\nBBVA.MC\n-0.1015487\n0.0106436\n\n\nVID.MC\n-0.1182183\n0.0054714\nPRS.MC\n-0.0147394\n0.0274017\nSAN.MC\n-0.1064919\n0.0101428\n\n\nAMS.MC\n-0.1195827\n0.0061822\nDIA.MC\n-0.0251285\n0.0218383\nEBRO.MC\n-0.1097231\n0.0026198\n\n\nATRY.MC\n-0.1372633\n0.0122878\nRIO.MC\n-0.0251470\n0.0029560\nRED.MC\n-0.1307882\n0.0038899\n\n\nENER.MC\n-0.1393435\n0.0124522\nVIS.MC\n-0.0288599\n0.0034482\nMVC.MC\n-0.1332527\n0.0125188\n\n\nPRM.MC\n-0.1685250\n0.0073096\nTEF.MC\n-0.0294417\n0.0054187\nRIO.MC\n-0.1363685\n0.0032897\n\n\nAPPS.MC\n-0.2295015\n0.0080938\nITX.MC\n-0.0311148\n0.0077844\nALB.MC\n-0.1487811\n0.0052084\n\n\nR4.MC\n-0.2788249\n0.0022699\nGRE.MC\n-0.0456889\n0.0285741\nIBE.MC\n-0.1519585\n0.0056253\n\n\nVIS.MC\n-0.2859081\n0.0042912\nATRY.MC\n-0.1516661\n0.0125130\nHOME.MC\n-0.1528376\n0.0073972\n\n\nLDA.MC\n-0.3181947\n0.0054316\nLDA.MC\n-0.1579126\n0.0042955\nVIS.MC\n-0.1825075\n0.0039782\n\n\n\n\n\n\n\n\n\n\nOwn elaboration.\n\n\n\n\n\n“Empresas Cotizadas.” n.d. BME Exchange. Accessed May 21, 2023. https://www.bolsasymercados.es/bme-exchange/es/Mercados-y-Cotizaciones/Acciones/Mercado-Continuo/Empresas-Cotizadas.",
    "crumbs": [
      "Annex",
      "Annex. 3 Tables"
    ]
  },
  {
    "objectID": "Annex4.html",
    "href": "Annex4.html",
    "title": "Annex. 4 Codes",
    "section": "",
    "text": "Data",
    "crumbs": [
      "Annex",
      "Annex. 4 Codes"
    ]
  },
  {
    "objectID": "Annex4.html#data",
    "href": "Annex4.html#data",
    "title": "Annex. 4 Codes",
    "section": "",
    "text": "Data Collection\nThe first thing that was done was to load the table of companies.\n\nempresas &lt;- read_excel(\"data/000_empresas.xlsx\")\n\nThen the ticks of the companies were extracted.\n\nticks &lt;- empresas |&gt; \n  select(TICKERS) |&gt; \n  pull()\n\nOnce the ticks of the companies were stored in the ticks variable, we proceeded to download the data corresponding to said companies from Yahoo Finance using the quantmod package from Ryan and Ulrich (2023).\n\nnombres_colum &lt;- c(\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\",\"Adjusted\")\nqmd_data &lt;- list()\nfor (i in 1:length(ticks)) {\n  tick &lt;- ticks[i]\n  value &lt;- getSymbols(\n    tick,\n    from = \"2000-01-02\",\n    to = \"2023-03-01\",\n    auto.assign = F,\n    periodicity = \"monthly\") |&gt;\n    as.data.frame()\n  dates &lt;- row.names(value)\n  row.names(value) &lt;- NULL\n  value &lt;- cbind(dates,value)\n  names(value) &lt;- nombres_colum\n  qmd_data[[tick]] &lt;-  value\n}\n\nWith the objective of carrying out an exploratory analysis of the data, it was decided to carry out a visual evaluation of the historical data of the adjusted price for what was executed:\n\nlapply(qmd_data, function(x){\n  x |&gt;\n    ggplot(aes(x=as.Date(Date), y=Adjusted))+\n             geom_line(color=\"#065AD8\")\n})\n\nAfter the visual analysis executed with the previous code fragment, the existence of constant prices was detected, as well as erroneous calculations in the adjusted price corresponding to the first years of some series. In order to eliminate these irregularities, only those observations after January 2005 were selected.\n\nreturns_emps &lt;- qmd_data |&gt;\n  lapply(function(x){\n    emps &lt;- x |&gt;\n      filter(Date &gt;= \"2005-01-31\")\n  })\n\nIn order to determine if the data that had been imported had missing values, the following code was executed:\n\nna_values &lt;- returns_emps |&gt;\n  sapply(function(x){\n    na &lt;- length(which(is.na(x)))\n  })\nemp_con_na &lt;- which(na_values &gt; 0)\n\nIn order to solve the problem regarding the incorrect recording of the data, it was decided to eliminate those that did not present price variations in more than 10 observations. For which, the returns were first computed by executing the following code, through which the series with missing values ​​were also eliminated.\n\nreturns_emps2 &lt;- returns_emps[-emp_con_na] |&gt;\n  lapply(function(x){\n    returns &lt;- x |&gt;\n      select(Date, Adjusted) |&gt;\n      mutate(Return_Ad = Delt(Adjusted)[,1]) |&gt;\n      na.omit() |&gt;\n      select(Date, Return_Ad)\n  })\n\nOnce the returns were computed, those series that presented 0 returns in more than 10 observations were eliminated, for which the following code was executed.\n\nzero_values &lt;- returns_emps2 |&gt;\n  sapply(function(x){\n    zeros &lt;- length(which(x[,2]==0))\n  })\nreturns_emps3 &lt;- returns_emps2[zero_values&lt;10]\n\n\n\nIndicators\nBelow is the code used during the process described in the indicators sub-heading of chapter 2.\nFirst, the IBEX data were downloaded, the returns of the adjusted price of the same were computed and the values ​​after January 2005 were selected.\n\n# Importing IBEX\nIBEXsel &lt;- getSymbols(\n  \"^IBEX\",\n  from = \"1990-01-01\",\n  to = \"2023-03-01\",\n  auto.assign = F,\n  periodicity = \"monthly\") |&gt;\n  as.data.frame()\ndates &lt;- row.names(IBEXsel)\nrow.names(IBEXsel) &lt;- NULL\nIBEXsel &lt;- cbind(dates,IBEXsel)\nnames(IBEXsel) &lt;- nombres_colum\n\n# Calculating profitability and selecting observations after January 2005.\n\nIBEXsel &lt;- IBEXsel |&gt;\n  mutate(Return_I = Delt(Adjusted)[,1]) |&gt;\n  na.omit() |&gt;\n  filter(Date &gt;= \"2005-01-31\") |&gt;\n  select(Date, Return_I)\n\nThen the values ​​of the IBEX returns were added to the tables of the returns of the shares of the selected companies, and the variables listed below were computed and added to each of the tables:\n\nCompany volatility\nIndex volatility\nCorrelation between the profitability of the company and the index\nThe Beta between the company and the index\n\n\nreturns_indc &lt;- returns_emps3 |&gt;\n  lapply(function(x, ind = IBEXsel){\n    emp &lt;- x |&gt;\n      left_join(ind) |&gt;\n      mutate(\n        VE = sqrt(cumsum((Return_Ad - cummean(Return_Ad))^2)/1:length(Return_Ad)),\n        VI = sqrt(cumsum((Return_I - cummean(Return_I))^2)/1:length(Return_I)),\n        Cor = cumsum((Return_Ad-cummean(Return_Ad))*(Return_I-cummean(Return_I)))/(sqrt(cumsum((Return_Ad-cummean(Return_Ad))^2))*sqrt(cumsum((Return_I-cummean(Return_I))^2)))\n      )|&gt;\n      na.omit() |&gt;\n      mutate(\n        Beta = (Cor*VE)/VI\n      )\n  })\n\n\n\nVectors\nBelow is the code used during the process described in the sub-heading vectors of the heading modeling in Chapter 2.\nThe first step carried out for the execution of the process explained in the sub-section in question was to create a function that allowed obtaining the consecutive samples for each series used. The function exposed below, as already mentioned, allows obtaining the consecutive samples of a series, for which the parameters mentioned in the sub-heading are used, number of input observations and number of output observations, as well as a parameter conditional with which it is indicated if the vector to be created is input or output.\n\nvector2dmaker &lt;- function(vec, ent, sal, eos=T){\n  if(eos==T){\n    emp &lt;- 1\n    term &lt;- (length(vec) - (ent+sal-1))\n    ob &lt;- ent\n  }else{\n    emp &lt;- ent + 1\n    term &lt;- (length(vec)-sal+1)\n    ob &lt;- sal\n  }\n  \n  vec2d &lt;- sapply(emp:term,\n               function(x) vec[x:(x + ob-1)]) |&gt;\n    matrix(nrow = ob) |&gt;\n    t()\n  \n  return(vec2d)\n}\n\nBelow is the code used to create the input vectors corresponding to each of the series. For which two functions were first created, one for the inputs and the other for the outputs.\n\n# Function that will be used to create the three-dimensional inputs\n\ninput3dmaker &lt;- function(x,inp,out){\n  empre &lt;- x\n  series &lt;- 2:dim(x)[2]\n  for (i in series) {\n    if(i==series[1]){\n      vec3d &lt;- vector2dmaker(empre[[i]],ent=inp,sal=out)\n    }else{\n      vec3d &lt;- abind(vec3d,vector2dmaker(empre[[i]],ent=inp,sal=out), along = 3)\n    }\n  }\n  return(vec3d)\n}\n\n# Function to be used to create the three-dimensional outputs\n\noutput3dmaker &lt;- function(x,inp,out){\n  empre &lt;- x[[\"Return_Ad\"]]\n  vec3d &lt;- vector2dmaker(empre,ent=inp,sal=out,F)\n  dim(vec3d) &lt;- c(dim(vec3d),1)\n  return(vec3d)\n}\n\nThen the lists of three-dimensional vectors of inputs and outputs per company were created, executing the following code another two times with the aim of creating the lists vecs3d2e and vecs3d3e that correspond to those cases in which 2 and 3 inputs were selected.\n\n# The time horizon is defined\nht &lt;- 1\n\n# The input observations are defined\noe &lt;- 1\n\n# 3d input vectors and 2d output vectors are created for input size 1\nvecs3d1e &lt;- list()\nfor(i in 1:length(returns_indc)){\n  emp &lt;- returns_indc[[i]]\n  inps &lt;- input3dmaker(emp, oe, ht)\n  outs &lt;- output3dmaker(emp, oe, ht)\n  dates &lt;- emp[(oe + ht):dim(emp)[1],1]\n  id &lt;- rep(names(returns_indc)[i],length(dates))\n  tibblex &lt;- tibble(\n    Date = dates,\n    ID = id,\n    inputs = inps,\n    outputs = outs\n  )\n  vecs3d1e[[names(returns_indc)[i]]] &lt;- tibblex\n}",
    "crumbs": [
      "Annex",
      "Annex. 4 Codes"
    ]
  },
  {
    "objectID": "Annex4.html#modeling-and-training",
    "href": "Annex4.html#modeling-and-training",
    "title": "Annex. 4 Codes",
    "section": "Modeling and training",
    "text": "Modeling and training\nThe code used during the process described in the different sub-sections of the Modeling and training section is presented below.\n\nModeling\nFor the creation of the models, the first step to execute is to obtain the information of the vectors for which the model is going to be built, which was done by executing the following code:\n\ndata &lt;- bind_rows(vecs3d1e)\ndata &lt;- data  |&gt;\n  arrange(Date)\ninputsinfo &lt;- data|&gt;\n  select(inputs) |&gt;\n  pull() |&gt;\n  dim()\noutputsinfo &lt;- data|&gt;\n  select(outputs) |&gt;\n  pull() |&gt;\n  dim()\n\n# Define parameters\nn_ob_pas &lt;- inputsinfo[2]\nn_variables &lt;- inputsinfo[3]\nn_ob_fut &lt;- outputsinfo[2]\n\nThen the structure of the models was constituted with the aspects described in 2.5.1 Modeling.\n\n# Input layer\ninp &lt;- layer_input(\n  shape = c(NULL,n_ob_pas,n_variables))\n\n# Hidden layers\n# - CNN\ncnn &lt;- inp |&gt;\n  layer_conv_1d(\n    filters = 64,\n    kernel_size = 1,\n    activation = layer_activation_leaky_relu())\n# - LSTM\nlstm &lt;- cnn |&gt;\n  layer_lstm(64)\n\n# Output layer\nout &lt;- lstm |&gt; \n  layer_dense(\n    n_ob_fut*1)\n\n# Join the layers to constitute the model \nmodel &lt;- keras_model(inp, out)\n# Setting learning parameters\nmodel |&gt; \n  compile(loss = \"mse\", optimizer = optimizer_sgd(0.0005))\n\n\n\n\n\n\n\nNote\n\n\n\nYou can find untrained models in the data folder of the repository where this work is located. The models were saved using the hdf5 extension and under the names model1e, model2e and model3e.\n\n\n\n\nTraining\nThe first step is to define the function to use for training the models. This function was built with the goal of using the training method described in 2.5.2 Training. As a result, this function will return a list that will contain the predictions obtained and the model after having been trained and will take as main inputs the tibble called data constituted in the first step that is exposed in this annex’s section Modeling and the model as well. of other arguments that allows the use of the function with some main inputs that are not used in the present work.\n\nwfv_train &lt;- function(x, modelo, seq_var_name, inp_var_name = \"inputs\", out_var_name = \"outputs\", progress_bar=T){\n  \n  predictions &lt;- c()\n  seq_val &lt;- unique(x[[seq_var_name]])\n  \n  if(progress_bar){\n    pb &lt;- txtProgressBar(min = 0, max = length(seq_val), initial = 0, style = 3)\n  }\n  \n  \n  # Iteration that will be executed for each unique value in the variable that defines the data sequence. For this reason it is of vital importance that the data in tibble x be ordered by the sequence variable whose name is passed to seq_var_name\n  \n  for (i in 1:length(seq_val)) {\n    val_seq &lt;- seq_val[i]\n    \n    # Extract inputs and outputs corresponding to the period in the current sequence variable\n    inputs &lt;- x |&gt;\n      filter(!!sym(seq_var_name) == val_seq) |&gt;\n      select(!!sym(inp_var_name)) |&gt;\n      pull()\n    outputs &lt;- x |&gt;\n      filter(!!sym(seq_var_name) == val_seq) |&gt;\n      select(!!sym(out_var_name)) |&gt;\n      pull()\n    outputs &lt;- outputs[,,1]\n    \n    # Use inputs to get forecasts for all periods in the sequence variable except for the first\n    if(i &gt; 1){\n      pred &lt;- modelo |&gt;\n        predict(inputs, verbose = 3)\n      predictions &lt;- rbind(predictions, pred)\n    }\n    \n    # Train the model\n    modelo |&gt;\n      fit(\n        inputs,\n        outputs,\n        epochs = 1,\n        batch_size = 10,\n        shuffle = F,\n        verbose = 0)\n    \n    if(progress_bar){\n      setTxtProgressBar(pb,i)\n      }\n    \n  }\n  \n  if(progress_bar){\n    close(pb)\n  }\n  \n  results &lt;- list()\n  results[['predicciones']] &lt;- predictions\n  results[['modelo']] &lt;- modelo\n  return(results)\n}\n\nOnce the function was created, the predictions were obtained using the following code:\n\nresultados &lt;- wfv_train(data,model,'Date')\npredicciones1e &lt;- resultados$predicciones\n\n\n\n\n\n\n\nNote\n\n\n\nYou can find trained models in the data folder of the repository where this work is located. The models were saved using the hdf5 extension and under the names model1etd, model2etd and model3etd.\n\n\nAs explained in 2.6.1 Predictions, in addition to the predictions obtained by the models, predictions obtained from the use of the arithmetic mean were computed, to compare with those obtained with the models. To compute these predictions, the following function was created:\n\nwfv_means &lt;- function(x, seq_var_name, inp_var_name = \"inputs\", out_var_name = \"outputs\", id_var_name, progress_bar=T){\n  \n  means &lt;- c()\n  seq_val &lt;- unique(x[[seq_var_name]])\n  \n  if(progress_bar){\n    pb &lt;- txtProgressBar(min = 0, max = length(seq_val), initial = 0, style = 3)\n  }\n  \n  for (i in 1:length(seq_val)) {\n    val_seq &lt;- seq_val[i]\n    inputs &lt;- x |&gt;\n      filter(!!sym(seq_var_name) == val_seq) |&gt;\n      select(!!sym(inp_var_name)) |&gt;\n      pull()\n    inputspred &lt;- x |&gt;\n      filter(!!sym(seq_var_name) == val_seq) |&gt;\n      select(!!sym(inp_var_name)) |&gt;\n      pull()\n    outputs &lt;- x |&gt;\n      filter(!!sym(seq_var_name) == val_seq) |&gt;\n      select(!!sym(out_var_name)) |&gt;\n      pull()\n    outputs &lt;- outputs[,,1]\n    \n    ids &lt;- x |&gt;\n      filter(!!sym(seq_var_name) == val_seq) |&gt;\n      select(!!sym(id_var_name)) |&gt;\n      pull()\n    \n    if(i==1){\n      dfmeans &lt;- inputs[,,1] |&gt;\n        as.data.frame() |&gt;\n        cbind(ID = ids)\n    }else{\n      dfmeansupd &lt;- inputs[,dim(inputs)[2],1] |&gt;\n        as.data.frame() |&gt;\n        cbind(ID = ids)\n      names(dfmeansupd)[1] &lt;- paste0(\"V\",(dim(dfmeans)[2]))\n      idsdf &lt;- unique(c(ids, dfmeans[[id_var_name]]))\n      idsdf &lt;- data.frame(ID = idsdf)\n      dfmeansupd &lt;- dplyr::left_join(idsdf, dfmeansupd, by = \"ID\")\n      ifelse(\n        dim(dfmeansupd)[1] &gt; dim(dfmeans)[1],\n        dfmeans &lt;- dplyr::left_join(dfmeansupd, dfmeans, by = \"ID\"),\n        dfmeans &lt;- dplyr::left_join(dfmeans, dfmeansupd, by = \"ID\")\n        )\n    }\n    \n    if(i &gt; 1){\n      MEANS &lt;-  dfmeans |&gt;\n        rowwise() |&gt;\n        mutate(\n          means = mean(c_across(-!!sym(id_var_name)), na.rm = T)) |&gt;\n        slice(match(ids,!!sym(id_var_name))) |&gt;\n        pull(means) |&gt;\n        as.matrix()\n      means &lt;- rbind(means, MEANS)\n    }\n    \n    if(progress_bar){\n      setTxtProgressBar(pb,i)\n    }\n    \n  }\n  \n  if(progress_bar){\n    close(pb)\n  }\n  \n  return(means)\n}\n\nOnce the function was created, the predictions were obtained using the following code:\n\nmeanse1 &lt;- wfv_means(data,'Date',id_var_name = \"ID\")\n\n\n\n\n\n\n\nNote\n\n\n\nIn addition to what was previously stated, two functions getconfig and plot_modelk were created, in the .Rprofile file of the repository in which this work is found, which allow graphing the structure of the models by using the Iannone (2023) package, as seen in the Figure 17. The code to use would be:\n\n# The functions are created to graph the structures used in this work.\n\nmodel |&gt;\n  getconfig() |&gt;\n  plot_modelk() |&gt;\n  grViz()\n\n\n\nThe procedure exposed in the sections Modeling and Training of this annex was repeated to build the 10 models made from each group of three-dimensional vectors, replacing the call to vecs3d1e with in the first code exposed. vecs3d2e and vecs3d3e, depending on the group of three-dimensional vectors used.",
    "crumbs": [
      "Annex",
      "Annex. 4 Codes"
    ]
  },
  {
    "objectID": "Annex4.html#result",
    "href": "Annex4.html#result",
    "title": "Annex. 4 Codes",
    "section": "Result",
    "text": "Result\nThe code used during the process described in the different sub-sections of the Result section is presented below.\n\nPredictions\nThe analysis exposed in 2.6.1 Predictions was carried out from graphs (see Figure 8, Figure 9 and Figure 10), which show the values of the \\(MSE\\) and \\(R^2\\) indicators. for each of the structures tested.\nThe first step to obtain these graphs was to compute the indicators, for each period of time, for each of the predictions obtained from the different models built with each structure. This is done using the following code.\n\n# Extract actual outputs\nsalidas &lt;- data |&gt;\n  filter(\n    Date &gt; data$Date[1]\n  ) |&gt;\n  select(outputs) |&gt;\n  pull()\nsalidas &lt;- salidas[,,1]\n\n# Compute MSE and R2 indicators\nindicadores &lt;- data |&gt;\n  filter(Date &gt; data$Date[1]) |&gt;\n  cbind(predicciones = predicciones1e[,1]) |&gt;\n  cbind(means = meanse1) |&gt;\n  mutate(salidas = salidas) |&gt;\n  select(Date, predicciones, means, salidas) |&gt;\n  group_by(Date) |&gt;\n  summarise(\n    r2 = 1 - (sum((salidas - predicciones)^2)/sum((salidas - means)^2)),\n    mse = mse(predicciones, salidas),\n  )\n\nThe different indicators computed for each of the 10 models trained with each of the structures were stored in a list called list_indicadores. This is done using the following code:\n\nlist_indicadores[[\"indicadores1\"]] &lt;-  indicadores\n\nOnce this is done, a list is obtained that contains 10 data frames (indicadores1,…,indicadores10), which in turn contain the values ​​of those of \\(MSE\\) and \\(R^2\\) of the predictions obtained by RNA models for each of the companies grouped by date. So, then the graph was built by using the following code.\n\n# Group the information of the different constructions in a single data frame\nindi_graf_data &lt;- do.call(cbind,list_indicadores)\n\n# Obtain the average results, for each period of time, using the different constructions\nindi_graf_data |&gt;\n  rowwise() |&gt;\n  mutate(\n    Date = `indicadores1.Date`,\n    meanmse = mean(c_across(contains(\"mse\"))),\n    meanr2 = mean(c_across(contains(\"r2\")))\n    ) |&gt;\n  select(\n    Date, meanmse,meanr2\n  )|&gt;\n  # Graph\n  mutate(\n    Date = as.Date(Date)) |&gt;\n  ggplot(aes(x = Date, group = 1)) +\n  geom_line(aes(y = meanmse, color = \"MSE\")) +\n  geom_line(aes(y = meanr2, color = \"R2\")) +\n  scale_color_manual(values = c(\"blue\", \"green\")) +\n  theme(axis.text.x = element_text(angle = 90)) +\n  labs(x = \"Date\", y = \"Indicators\", color = \"Indicators\")\n\nIn addition to the graphs, the Table 4 was also used in the analysis of the results, which contains the companies that obtained the best and worst indicators for each structure. To obtain these data, the following code was used:\n\nindicadores_X_emp &lt;- data |&gt;\n  filter(Date &gt; data$Date[1]) |&gt;\n  cbind(predicciones = predicciones1e[,1]) |&gt;\n  cbind(means = meanse1) |&gt;\n  mutate(salidas = salidas) |&gt;\n  select(Date, predicciones, means, salidas, ID) |&gt;\n  group_by(ID) |&gt;\n  summarise(\n    r2 = 1 - (sum((salidas - predicciones)^2)/sum((salidas - means)^2)),\n    mse = mse(predicciones, salidas)\n  ) |&gt;\n  select(ID, r2, mse)\n\nLike the indicators computed by date, to save the indicators computed by company, a list called list_indic_emp was created. After having stored the 10 indicator data frames per company in the list, the companies with the best and worst results were extracted using the following code:\n\n# Group the information of the different constructions in a single data frame\nind_emp_t &lt;- do.call(rbind, list_indic_emp)\n\n# Compute the average R2 and MSE by company\nind_emp_t &lt;- ind_emp_t |&gt;\n  group_by(ID) |&gt;\n  summarize(\n    r2 = mean(r2),\n    mse= mean(mse)) |&gt;\n  ungroup() |&gt;\n  arrange(desc(r2))\n\n# Obtain the 10 companies with the best and worst indicators\nmejores10 &lt;- head(ind_emp_t,10)\npeores10 &lt;- tail(ind_emp_t,10)\n\nAnd using the above variables and using the rbind() and cbind functions was how the Table 4 were created.\n\n\nPortfolio composition\nThis section explains how the analysis of the comparison of the results obtained by the different portfolios was carried out (see Figure 11, Figure 12 and Figure 13). For this, it is first necessary to obtain the composition of the portfolios, by date, from the predictions obtained by using the arithmetic means and the RNA models.\nTo calculate the composition of the portfolios, the R package Berwin A. Turlach R port by Andreas Weingessel &lt;Andreas.Weingessel@ci.tuwien.ac.at&gt; Fortran contributions from Cleve Moler dpodi/LINPACK) (2019) was used. Below is the code used to find the composition of the portfolios from the predictions of the mean:\n\n# A data frame was created in which all the information was stored:\n#   - IBEX values, as a reference index\n#   - Values ​​of the predictions, both those obtained by the RNA model and by the arithmetic means\n\nDATA &lt;- data |&gt;\n  left_join(IBEXsel, by =\"Date\") |&gt;\n  mutate(IBEX = Return_I) |&gt;\n  arrange(Date) |&gt;\n  filter(\n    Date &gt; data$Date[1]\n  ) |&gt;\n  mutate(predicciones = predicciones1e[,1]) |&gt;\n  mutate(\n    Real = salidas,\n    RNA = predicciones,\n    Means = meanse1\n  ) |&gt;\n  select(Date, Real, IBEX, RNA, Means, ID)\n\n# From the data frame DATA were created:\n#    - A data frame whose columns are the actual data of each of the companies for each of the time periods for which predictions were obtained.\n#    - A data frame whose columns are the data obtained by using the arithmetic means of each of the companies for each of the time periods for which predictions were obtained.\n\npvtReal &lt;- DATA |&gt;\n  select(Date, Real, ID) |&gt;\n  pivot_wider(\n    names_from = ID,\n    values_from = Real\n  )\n\npvtMeans &lt;- DATA |&gt;\n  select(Date, Means, ID) |&gt;\n  pivot_wider(\n    names_from = ID,\n    values_from = Means\n  )\n\n# The data frame was created in which the composition of the portfolios was stored for each of the periods for which the prediction was obtained.\nweightsm &lt;- data.frame()\n\n# Iteration by which the composition of the portfolios is found\n\npb &lt;- txtProgressBar(min = 0, max = length(unique(data$Date)[-1]), initial = 0, style = 3)\n\nfor (i in 1:length(unique(data$Date)[-1])) {\n  if(i&gt;1){\n    \n    # The data frame is created that includes the data to be used to find the composition of the portfolio, this is created by the actual data to date and the forecast for the next period.\n    datamQP &lt;- pvtReal |&gt;\n      filter(Date &lt; unique(data$Date)[-1][i]) |&gt;\n      rbind(pvtMeans |&gt;\n              filter(Date == unique(data$Date)[-1][i])\n      )\n    \n    # Eliminate those companies that do not have actual or forecast data\n    nare &lt;- which(is.na(datamQP[dim(datamQP)[1],]))\n    naremo &lt;- which(is.na(datamQP[(dim(datamQP)[1]-1),]))\n    nare &lt;- c(nare,naremo)\n    nare &lt;- unique(nare)\n    if(length(nare) != 0){\n      carteram &lt;- datamQP[, - nare]\n    }else{\n      carteram &lt;- datamQP\n    }\n    \n    # Extract the forecasts\n    returnm &lt;- carteram[dim(carteram)[1], -1] |&gt;\n      as.matrix() |&gt;\n      t()\n    \n    # Calculate the covariance matrix\n    covmm &lt;- cov(carteram[, -1], use = \"complete.obs\")\n    npcovmm &lt;- nearPD(covmm)$mat |&gt; \n      as.matrix()\n    # Extract the number of companies\n    n &lt;- ncol(npcovmm)\n    \n    # Find the composition of the portfolio\n    qp_outm &lt;- solve.QP(\n      Dmat = 2*npcovmm,\n      dvec = rep(0,n),\n      Amat = cbind(-1, diag(n)),\n      bvec = c(-1, rep(0,n)),\n      meq = 1)\n    qp_outm &lt;- qp_outm$solution\n    qp_outm &lt;- floor(qp_outm*100)/100\n    for(j in 1:length(qp_outm)){\n      if(qp_outm[j] &lt; 0.001){\n        qp_outm[j] &lt;- 0\n      }else{}\n    }\n    \n    # Save portfolio composition\n    names(qp_outm) &lt;- names(carteram[, -1])\n    weightsm &lt;- bind_rows(weightsm, qp_outm)\n  }\n  \n  setTxtProgressBar(pb,i)\n}\n\nclose(pb)\n\n# Replace actual weights and observations with missing values ​​with zero\npvtReal[is.na(pvtReal)] &lt;- 0\nweightsm[is.na(weightsm)] &lt;- 0\n\nThen, to find the profitability of the portfolio, the compositions were multiplied by the real returns, it was assumed that one was invested in the first period and a cumulative sum was made throughout the values ​​to obtain the behavior of the profitability throughout the period. time.\n\n# Finding the returns of the portfolios formed from the predictions of the arithmetic mean\n\nreturn_CM &lt;-  weightsm * pvtReal[-1,-1]\nreturn_CM &lt;- rowSums(return_CM)\nreturn_CM &lt;- c(1,return_CM)\nreturn_CM &lt;- data.frame(\n  Date = pvtReal[,1],\n  Mean = return_CM\n)\n\nThe same steps that were carried out to find the behavior of the profitability of the portfolios from the arithmetic means were carried out to find the behavior from the predictions obtained by the RNA model as seen in the code below.\n\n# From the DATA data frame, a data frame was created whose columns are the data obtained by using the RNA model of each of the companies for each of the time periods for which predictions were obtained.\n\npvtRNA &lt;- DATA |&gt;\n  select(Date, RNA, ID) |&gt;\n  pivot_wider(\n    names_from = ID,\n    values_from = RNA\n  )\n\n# The data frame was created in which the composition of the portfolios was stored for each of the periods for which the prediction was obtained.\nweightse &lt;- data.frame()\n\n# Iteration by which the composition of the portfolios is found\n\npb &lt;- txtProgressBar(min = 0, max = length(unique(data$Date)[-1]), initial = 0, style = 3)\n\nfor (i in 1:length(unique(data$Date)[-1])) {\n  if(i&gt;1){\n    # The data frame is created that includes the data to be used to find the composition of the portfolio, this is created by the actual data to date and the forecast for the next period.\n    dataeQP &lt;- pvtReal |&gt;\n      filter(Date &lt; unique(data$Date)[-1][i]) |&gt;\n      rbind(pvtRNA |&gt;\n              filter(Date == unique(data$Date)[-1][-1][i])\n            )\n    # Eliminate those companies that do not have actual or forecast data\n    nare &lt;- which(is.na(dataeQP[dim(dataeQP)[1],]))\n    naremo &lt;- which(is.na(dataeQP[(dim(dataeQP)[1]-1),]))\n    nare &lt;- c(nare,naremo)\n    nare &lt;- unique(nare)\n    if(length(nare) != 0){\n      carterae &lt;- dataeQP[, - nare]\n    }else{\n      carterae &lt;- dataeQP\n    }\n    \n    # Extract forecasts\n    returne &lt;- carterae[dim(carterae)[1], -1] |&gt;\n      as.matrix() |&gt;\n      t()\n    \n    # Calculate the covariance matrix\n    covme &lt;- cov(carterae[, -1], use = \"complete.obs\")\n    npcovme &lt;- nearPD(covme)$mat |&gt; \n      as.matrix()\n    # Extract the number of companies\n    n &lt;- ncol(npcovme)\n    \n    # Find the composition of the portfolio\n    qp_oute &lt;- solve.QP(\n      Dmat = 2*npcovme,\n      dvec = rep(0,n),\n      Amat = cbind(-1, diag(n)),\n      bvec = c(-1, rep(0,n)),\n      meq = 1)\n    qp_oute &lt;- qp_oute$solution\n    qp_oute &lt;- floor(qp_oute*100)/100\n    for(j in 1:length(qp_oute)){\n      if(qp_oute[j] &lt; 0.001){\n        qp_oute[j] &lt;- 0\n      }else{}\n    }\n    \n    # Save portfolio composition\n    names(qp_oute) &lt;- names(carterae[, -1])\n    weightse &lt;- bind_rows(weightse, qp_oute)\n  }\n  \n  setTxtProgressBar(pb,i)\n}\n\nclose(pb)\n\n# Replace weights with missing values with zero\nweightse[is.na(weightse)] &lt;- 0\n\nThen, to find the profitability of the portfolio, the compositions were multiplied by the real returns, it was assumed that one was invested in the first period and a cumulative sum was made throughout the values to obtain the behavior of the profitability throughout the period time.\n\n# Finding the returns of the portfolios formed from the predictions of the RNA model\n\nreturn_CRNA &lt;-  weightse * pvtReal[-1,-1]\nreturn_CRNA &lt;- rowSums(return_CRNA)\nreturn_CRNA &lt;- c(1,return_CRNA)\nreturn_CRNA &lt;- data.frame(\n  Date = pvtReal[,1],\n  RNA = return_CRNA\n)\n\nThen, as with the indicators, a list_ret_RNA list was created in which the data frames of the different models built with each of the structures were stored. Then the following code was executed to obtain the graph.\n\n# Finding the behavior of the returns of the IBEX for the period\n\nIBEXvals &lt;- IBEXsel |&gt;\n    filter(Date &gt; unique(data$Date)[2]) |&gt;\n    select(2) |&gt;\n    pull()\nIBEXvals &lt;- c(1, IBEXvals)\n\ndata_rent_RNA &lt;- do.call(cbind,list_ret_RNA)\ndata_rent_RNA &lt;- data_rent_RNA |&gt;\n  mutate(\n    Date = RNA1.Date,\n    IBEX = IBEXvals,\n    Means = return_CM$Mean) |&gt;\n  mutate_at(vars(contains(\".RNA\")), ~ cumsum(.)) |&gt;\n  mutate(\n    IBEX = cumsum(IBEX),\n    Means = cumsum(Means)) |&gt;\n  group_by(Date) |&gt;\n  summarize(\n    meanRNA = mean(c_across(contains(\".RNA\"))),\n    max_y = max(c_across(contains(\".RNA\"))),\n    min_y = min(c_across(contains(\".RNA\"))),\n    min_5 = unname(quantile(c_across(contains(\".RNA\")),0.05)),\n    max_95 = unname(quantile(c_across(contains(\".RNA\")),0.95)),\n    IBEX = IBEX,\n    Means = Means)\ndata_rent_RNA |&gt;\n  mutate(\n    Date = as.Date(Date)) |&gt;\nggplot(aes(x = Date)) +\n  geom_ribbon(aes(ymin = min_y, ymax = min_5), fill = \"blue\", alpha = 0.3) +\n  geom_ribbon(aes(ymin = max_y, ymax = max_95), fill = \"blue\", alpha = 0.3) +\n  geom_ribbon(aes(ymin = min_5, ymax = max_95), fill = \"blue\", alpha = 0.6) +\n  geom_line(\n    aes(y = meanRNA, color = \"Media RNA1\"),\n    linetype = \"dashed\") +\n  geom_line(aes(y = max_y), color = \"blue\") +\n  geom_line(aes(y = min_y), color = \"blue\") +\n  geom_line(aes(y = max_95), color = \"blue\") +\n  geom_line(aes(y = min_5, color = \"RNA1\")) +\n  geom_line(aes(y = IBEX, color = \"IBEX\")) +\n  geom_line(aes(y = Means, color = \"Medias\")) +\n  scale_color_manual(\n    values = c(\n      \"Media RNA1\"=\"blue\",\n      \"RNA1\" = \"blue\",\n      \"IBEX\" = \"red\",\n      \"Medias\" = \"green\")) +\n  guides(\n    color = guide_legend(\n      override.aes = list(\n        linetype = c(\"solid\",\"dashed\",\"solid\",\"solid\"))))+\n  labs(x = \"Date\",\n       y = \"Returns\",\n       color = \"Legenda\")+\n  theme_minimal()\n\n\n\n\n\nBerwin A. Turlach R port by Andreas Weingessel &lt;Andreas.Weingessel@ci.tuwien.ac.at&gt; Fortran contributions from Cleve Moler dpodi/LINPACK), S original by. 2019. Quadprog: Functions to Solve Quadratic Programming Problems. https://CRAN.R-project.org/package=quadprog.\n\n\nIannone, Richard. 2023. DiagrammeR: Graph/Network Visualization. https://CRAN.R-project.org/package=DiagrammeR.\n\n\nRyan, Jeffrey A., and Joshua M. Ulrich. 2023. Quantmod: Quantitative Financial Modelling Framework. https://CRAN.R-project.org/package=quantmod.",
    "crumbs": [
      "Annex",
      "Annex. 4 Codes"
    ]
  }
]