{
  "hash": "18500dd70c2706f9437c47a63eb980d4",
  "result": {
    "markdown": "# Anexo. 1 Figuras {.unnumbered}\n\n![Relación entre IA-ML-DL](../images/a1_DL-env.png \"Relación entre IA-ML-DL\"){#fig-DLenv fig-alt=\"IA engloba a ML y ML engloba a DL\"}\n\n::: figure-caption\nTomada de: Deep learning with R de @chollet2018deep.\n:::\n\n![Programación clásica y machine learning](../images/a1_ML-prog.png \"Programación clásica y machine learning\"){#fig-MLprog fig-alt=\"Muestra un esquema que explica de manera general el paradigma de la programación clasica y el uso de machine learning. Mostrandose en el primero como se introducen reglas y datos para obtener respuestas, mientras que en machine learning se introducen datos y respuestas para obtener reglas\"}\n\n::: figure-caption\nTomada de: Deep learning with R de @chollet2018deep.\n:::\n\n![Estructura básica de una red neuronal artificial](../images/a1_RNA-struct.png \"Estructura básica de una red neuronal artificial\"){#fig-RNAstruct fig-alt=\"Muestra un esquema el esquema de la estructura básica de una RNA. Se muestra como los diferentes las diferentes entradas se conectan a cada uno de los nodos de las capa oculta y a su vez como cada uno de los nodos de la capa oculta se conecta a los nodos de la capa de salida. Los nodos de la capa oculta y la capa de salida también están conectados a un parámetro bias \"}\n\n::: figure-caption\nTomada de: Tema 14: redes neuronales de @Larranaga07.\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Como el tamaño del filtro afecta el vector de salida](../images/a1_HJ-ks.gif){#fig-HJks}\n:::\n:::\n\n\n::: figure-caption\nElaboración propia: Elaborada a partir de @hongj20. Muestra como el tamaño del vector de salida cambia según el tamaño de filtro que se usa.\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Como stride afecta el vector de salida](../images/a1_HJ-stride.gif){#fig-HJstride}\n:::\n:::\n\n\n::: figure-caption\nElaboración propia: Elaborada a partir de @hongj20. Muestra como el parámetro stride afecta el tamaño del vector de salida.\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Como dilation afecta el vector de salida](../images/a1_HJ-dilation.gif){#fig-HJdilation}\n:::\n:::\n\n\n::: figure-caption\nElaboración propia: Elaborada a partir de @hongj20. Muestra como el parámetro dilation afecta el tamaño del vector de salida.\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Como padding afecta el vector de salida](../images/a1_HJ-padding.gif){#fig-HJpadding}\n:::\n:::\n\n\n::: figure-caption\nElaboración propia: Elaborada a partir de @hongj20. Muestra como el parámetro padding afecta el tamaño del vector de salida.\n:::\n\n![Despliegue del bucle de una red neuronal recurrente estándar](../images/a1_CO-RNRstruct.png \"Despliegue del bucle de una red neuronal recurrente estándar\"){#fig-CORNRstruct width=\"550\"}\n\n::: figure-caption\nTomada de: Understanding LSTM networks, @COlah15.\n:::\n\n![Información relevante cercana](../images/a1_CO-RInclose.png \"Información relevante cercana\"){#fig-CORInclose width=\"550\"}\n\n::: figure-caption\nTomada de: Understanding LSTM networks, @COlah15.\n:::\n\n![Información relevante lejana](../images/a1_CO-RInaway.png \"Información relevante lejana\"){#fig-CORInaway width=\"550\"}\n\n::: figure-caption\nTomada de: Understanding LSTM networks, @COlah15.\n:::\n\n![Diferencia entre los módulos de repetición](../images/a1_CO-Drnrlstm.png \"Diferencia entre los módulos de repetición\"){#fig-CODrnrlstm width=\"600\"}\n\n::: figure-caption\nTomada de: Understanding LSTM networks, @COlah15.\n:::\n\n![LSTM funcionalidad: Representación del paso 1](../images/a1_CO-LSTMstep1.png \"LSTM funcionalidad: Representación del paso 1\"){#fig-COLSTMstep1 height=\"200\"}\n\n::: figure-caption\nTomada de: Understanding LSTM networks, @COlah15.\n:::\n\n![LSTM funcionalidad: Representación del paso 2](../images/a1_CO-LSTMstep2.png \"LSTM funcionalidad: Representación del paso 2\"){#fig-COLSTMstep2 height=\"180\"}\n\n::: figure-caption\nTomada de: Understanding LSTM networks, @COlah15.\n:::\n\n![LSTM funcionalidad: Representación del paso 3](../images/a1_CO-LSTMstep3.png \"LSTM funcionalidad: Representación del paso 3\"){#fig-COLSTMstep3 height=\"250\"}\n\n::: figure-caption\nTomada de: Understanding LSTM networks, @COlah15.\n:::\n\n![LSTM funcionalidad: Representación del paso 4](../images/a1_CO-LSTMstep4.png \"LSTM funcionalidad: Representación del paso 4\"){#fig-COLSTMstep4 height=\"235\"}\n\n::: figure-caption\nTomada de: Understanding LSTM networks, @COlah15.\n:::\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Visualización de vectores de entrada y salida](../images/a2_muestras.gif){#fig-muestras}\n:::\n:::\n\n\n::: figure-caption\nElaboración propia: Elaborada a partir de imagen en @chollet2018deep. Muestra como lucen los vectores tridimensionales de entradas y salidas correspondientes a los datos de una empresa, en el caso de que se usen tres observaciones para crear el vector de entrada.\n:::\n\n![Distintas estructras según los distintos tamaños de vectores de entrada](../images/a2_estructuras.png){#fig-estructuras}\n\n::: figure-caption\nElaboración propia: Elaborada a partir de las distintos modelos construidos usando los paquetes keras y tensorflow en R, y fueron gráficadas mediante el uso del paquete @Diagrammer.\n:::\n\n![Dominio de ReLU y Leaky ReLU](../images/a2_dominios.png){#fig-dominios}\n\n::: figure-caption\nElaboración propia: Elaborada a partir de las imagenes que se observan en @Rallabandi23.\n:::\n\n![Diagrama de flujo de la metodología de Walk Forward Validation](../images/a2_wfv.png){#fig-wfv}\n\n::: figure-caption\nElaboración propia\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}