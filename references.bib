@book{kocenda2017,
  title     = "Elements of Time Series Econometrics: An Applied Approach",
  author    = {Kocenda, E. and Cerný, A.},
  year      = 2017,
  publisher = "Karolinum Press",
  address   = "Prague"
}
@book{anderson2017,
  title     = "Statistics for business & economics",
  author    = {Anderson, D. R. and Sweeney, D. J. and Williams, T. A. and Camm, D. J. and Cochran, J. J},
  year      = 2017,
  publisher = "Cengage Learning",
  address   = "Boston"
}
@Inbook{dodge2008,
  title="Time Series",
  bookTitle="The Concise Encyclopedia of Statistics",
  year="2008",
  author = {Dodge, Y.},
  publisher="Springer New York",
  address="New York, NY",
  pages="536--539",
  isbn="978-0-387-32833-1",
  doi="10.1007/978-0-387-32833-1_401",
  url="https://doi.org/10.1007/978-0-387-32833-1_401"
}
@book{espallargas2012,
  title     = "Econometría y series temporales: aplicaciones",
  author    = {Espallargas, S. D. and Solís, M. V.},
  year      = 2012,
  publisher = "Editorial Félix Varela",
  address   = "La Habana"
}
@online{IBM2021,
  title =    "Characteristics of time series",
  author =   {IBM},
  year = 2021,
  howpublished =     {\url{https://www.ibm.com/docs/en/spss-modeler/saas?topic=data-characteristics-time-series}}
}
@online{chirinos2018,
  author={Chirinos, S.},
  title = {Series cronológicas},
  howpublished = {\url{https://www.slideshare.net/SuedimarChirinos/series-cronologicas-119058959}},
  year = {2018}
}
@online{villagarcia2006,
  author={Villagarcía, T. },
  title = {Series Temporales},
  howpublished = {\url{https://halweb.uc3m.es/fjnm/estind/doc_grupo1/archivos/Apuntes%20de%20series.pdf}},
  year = {2006}
}
@online{ruiz2011,
  author={Ruiz, M. C.},
  title = {Tema 5: Procesos Estocásticos},
  howpublished = {\url{http://www.dmae.upct.es/~mcruiz/Telem06/Teoria/apuntes_procesos.pdf}},
  publisher = "Departamento de Matemática y Estadística. Universidad Politécnica de Cartagena",
  year = {2011}
}
@book{castillo_varela2010,
  title     = "ECONOMETRÍA PRÁCTICA: Fundamentos de Series de Tiempo",
  author    = {Castillo, R. A. and Varela, R. },
  year      = 2010,
  publisher = "Universidad Autónoma de Baja California",
  address   = "México"
}
@online{villavicencio2010,
  author = {Villavicencio, J.},
  title = {Introducción a las series de tiempo},
  howpublished = {\url{http://www.estadisticas.gobierno.pr/iepr/LinkClick.aspx}},
  publisher = "Instituto de estadística de Puerto Rico",
  year = {2010}
}
@online{cnmvbv,
  url = {https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&letra=B&idlng=1},
  author = {CNMV},
  title = {Glosario Financiero: Bolsa de valores},
  urldate = {2023-04-24}
}
@online{bmeqe,
  url = {https://www.bolsasymercados.es/esp/Sobre-BME/Que-es},
  author = {BME},
  title = {¿Qué es BME?},
  urldate = {2023-04-24}
}
@online{cnmvsibe,
  url = {https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&letra=S&idlng=1},
  author = {CNMV},
  title = {Glosario Financiero: Servicio de Interconexión Bursátil Español, SIBE},
  urldate = {2023-04-24}
}
@online{cnmvacci,
  url = {https://cnmv.es/Portal/Inversor/Glosario.aspx?id=0&letra=A&idlng=1},
  author = {CNMV},
  title = {Glosario Financiero: Acción},
  urldate = {2023-04-24}
}
@online{cfi23,
  author = {CFI Team},
  title = {What is Stock Price?},
  year = 2023,
  url = {https://corporatefinanceinstitute.com/resources/capital-markets/stock-price/},
  urldate = {2023-04-24}
}
@online{pinset21,
  author = {Pinset, W.},
  title = {Understanding Stock Prices and Values},
  year = 2021,
  url = {https://www.investopedia.com/articles/stocks/08/stock-prices-fool.asp},
  urldate = {2023-04-24}
}
@online{mitchell20,
  author = {Mitchell, C.},
  title = {Market Price: Definition, Meaning, How To Determine, and Example},
  year = 2020,
  url = {https://www.investopedia.com/terms/m/market-price.asp},
  urldate = {2023-04-24}
}
@online{tinvt22,
  author = {The Investopedia Team},
  title = {Intrinsic Value Defined and How It's Determined in Investing and Business},
  year = 2022,
  url = {https://www.investopedia.com/terms/i/intrinsicvalue.asp},
  urldate = {2023-04-24}
}
@online{barone22,
  author = {Barone, A.},
  title = {Opening Price: Definition, Example, Trading Strategies},
  year = 2022,
  url = {https://www.investopedia.com/terms/o/openingprice.asp},
  urldate = {2023-04-24}
}
@online{chen22,
  author = {Chen, J.},
  title = {Today's High},
  year = 2022,
  url = {https://www.investopedia.com/terms/t/todayshigh.asp},
  urldate = {2023-04-24}
}
@online{downey22,
  author = {Downey, L.},
  title = {Today's Low},
  year = 2022,
  url = {https://www.investopedia.com/terms/t/todayslow.asp},
  urldate = {2023-04-24}
}
@online{hayes21,
  author = {Hayes, A.},
  title = {What Is Closing Price? Definition, How It's Used, and Example},
  year = 2021,
  url = {https://www.investopedia.com/terms/c/closingprice.asp},
  urldate = {2023-04-24}
}
@online{ganti20,
  author = {Ganti, A.},
  title = {Adjusted Closing Price},
  year = 2020,
  url = {https://www.investopedia.com/terms/a/adjusted_closing_price.asp},
  urldate = {2023-04-24}
}
@book{chollet2018deep,
  title={Deep Learning with R},
  author={Chollet, F. and Allaire, J.J.},
  isbn={9781617295546},
  lccn={2018285360},
  url={https://books.google.es/books?id=xnIRtAEACAAJ},
  year={2018},
  publisher={Manning Publications}
}
@article{McCarthy_Minsky_Rochester_Shannon_2006,
  title={A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955},
  volume={27},
  url={https://ojs.aaai.org/aimagazine/index.php/aimagazine/article/view/1904},
  DOI={10.1609/aimag.v27i4.1904},
  abstractNote={The 1956 Dartmouth summer research project on artificial intelligence was initiated by this August 31, 1955 proposal, authored by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon. The original typescript consisted of 17 pages plus a title page. Copies of the typescript are housed in the archives at Dartmouth College and Stanford University. The first 5 papers state the proposal, and the remaining pages give qualifications and interests of the four who proposed the study. In the interest of brevity, this article reproduces only the proposal itself, along with the short autobiographical statements of the proposers.}, 
  number={4},
  journal={AI Magazine},
  author={McCarthy, John and Minsky, Marvin L. and Rochester, Nathaniel and Shannon, Claude E.},
  year={2006},
  month={Dec.},
  pages={12}
}
@book{haykin1998neural,
  title={Neural networks: a comprehensive foundation},
  author={Haykin, Simon},
  year={1998},
  publisher={Prentice Hall PTR}
}
@book{banda2014,
  title={Inteligencia Artificial: Principios y Aplicaciones},
  author={Banda, Hugo},
  year={2014},
  publisher={Escuela Politécnica Nacional},
  address = "Quito, Ecuador"
}
@online{Larranaga07,
  author = {Larrañaga, Pedro & Inza, Iñaki & Moujahid, Abdelmalik},
  title = {Tema 14. Redes Neuronales},
  year = 2007,
  publisher = {Departamento de Ciencias de la Computaci´on e Inteligencia Artificial, Universidad del Pa´ıs Vasco–Euskal Herriko Unibertsitatea},
  url = {http://www.sc.ehu.es/ccwbayes/docencia/mmcc/docs/t14-neuronales.pdf}
}
@article{ZHANG199835,
  title = {Forecasting with artificial neural networks:: The state of the art},
  journal = {International Journal of Forecasting},
  volume = {14},
  number = {1},
  pages = {35-62},
  year = {1998},
  issn = {0169-2070},
  doi = {https://doi.org/10.1016/S0169-2070(97)00044-7},
  url = {https://www.sciencedirect.com/science/article/pii/S0169207097000447},
  author = {Guoqiang Zhang & B. {Eddy Patuwo} & Michael {Y. Hu}},
  keywords = {Neural networks, Forecasting},
  abstract = {Interest in using artificial neural networks (ANNs) for forecasting has led to a tremendous surge in research activities in the past decade. While ANNs provide a great deal of promise, they also embody much uncertainty. Researchers to date are still not certain about the effect of key factors on forecasting performance of ANNs. This paper presents a state-of-the-art survey of ANN applications in forecasting. Our purpose is to provide (1) a synthesis of published research in this area, (2) insights on ANN modeling issues, and (3) the future research directions.}
}
@article{TEALAB2018334,
  title = {Time series forecasting using artificial neural networks   methodologies: A systematic review},
  journal = {Future Computing and Informatics Journal},
  volume = {3},
  number = {2},
  pages = {334-340},
  year = {2018},
  issn = {2314-7288},
  doi = {https://doi.org/10.1016/j.fcij.2018.10.003},
  url = {https://www.sciencedirect.com/science/article/pii/S2314728817300715},
  author = {Ahmed Tealab},
  keywords = {Forecasting, Nonlinear time series, Neural networks, Moving averages},
  abstract = {This paper studies the advances in time series forecasting models using artificial neural network methodologies in a systematic literature review. The systematic review has been done using a manual search of the published papers in the last 11 years (2006–2016) for the time series forecasting using new neural network models and the used methods are displayed. In the covered period in the study, the results obtained found 17 studies that meet all the requirements of the search criteria. Only three of the obtained proposals considered a process different to the autoregressive of a neural networks model. These results conclude that, although there are many studies that presented the application of neural network models, but few of them proposed new neural networks models for forecasting that considered theoretical support and a systematic procedure in the construction of model. This leads to the importance of formulating new models of neural networks.}
}
@article{wongguo2010,
  author={Wong, W.K. and Guo, Z.X.},
  title={{A hybrid intelligent model for medium-term sales forecasting in fashion retail supply chains using extreme learning machine and harmony search algorithm}},
  journal={International Journal of Production Economics},
  year=2010,
  volume={128},
  number={2},
  pages={614-624},
  month={December},
  keywords={ Fashion sales forecasting Harmony search Neural network Extreme learning machine},
  doi={},
  abstract={A hybrid intelligent (HI) model, comprising a data preprocessing component and a HI forecaster, is developed to tackle the medium-term fashion sales forecasting problem. The HI forecaster firstly adopts a novel learning algorithm-based neural network to generate initial sales forecasts and then uses a heuristic fine-tuning process to obtain more accurate forecasts based on the initial ones. The learning algorithm integrates an improved harmony search algorithm and an extreme learning machine to improve the network generalization performance. Extensive experiments based on real fashion retail data and public benchmark datasets were conducted to evaluate the performance of the proposed model. The experimental results demonstrate that the performance of the proposed model is much superior to traditional ARIMA models and two recently developed neural network models for fashion sales forecasting.},
  url={https://ideas.repec.org/a/eee/proeco/v128y2010i2p614-624.html}
}
@article{GURESEN201110389,
  title = {Using artificial neural network models in stock market index prediction},
  journal = {Expert Systems with Applications},
  volume = {38},
  number = {8},
  pages = {10389-10397},
  year = {2011},
  issn = {0957-4174},
  doi = {https://doi.org/10.1016/j.eswa.2011.02.068},
  url = {https://www.sciencedirect.com/science/article/pii/S0957417411002740},
  author = {Erkam Guresen and Gulgun Kayakutlu and Tugrul U. Daim},
  keywords = {Financial time series (FTS) prediction, Recurrent neural networks (RNN), Dynamic artificial neural networks (DAN2), Hybrid forecasting models},
  abstract = {Forecasting stock exchange rates is an important financial problem that is receiving increasing attention. During the last few years, a number of neural network models and hybrid models have been proposed for obtaining accurate prediction results, in an attempt to outperform the traditional linear and nonlinear approaches. This paper evaluates the effectiveness of neural network models which are known to be dynamic and effective in stock-market predictions. The models analysed are multi-layer perceptron (MLP), dynamic artificial neural network (DAN2) and the hybrid neural networks which use generalized autoregressive conditional heteroscedasticity (GARCH) to extract new input variables. The comparison for each model is done in two view points: Mean Square Error (MSE) and Mean Absolute Deviate (MAD) using real exchange daily rate values of NASDAQ Stock Exchange index.}
}
@article{SEZER2020106181,
  title = {Financial time series forecasting with deep learning : A systematic literature review: 2005–2019},
  journal = {Applied Soft Computing},
  volume = {90},
  pages = {106181},
  year = {2020},
  issn = {1568-4946},
  doi = {https://doi.org/10.1016/j.asoc.2020.106181},
  url = {https://www.sciencedirect.com/science/article/pii/S1568494620301216},
  author = {Omer Berat Sezer and Mehmet Ugur Gudelek and Ahmet Murat Ozbayoglu},
  keywords = {Deep learning, Finance, Computational intelligence, Machine learning, Time series forecasting, CNN, LSTM, RNN},
  abstract = {Financial time series forecasting is undoubtedly the top choice of computational intelligence for finance researchers in both academia and the finance industry due to its broad implementation areas and substantial impact. Machine Learning (ML) researchers have created various models, and a vast number of studies have been published accordingly. As such, a significant number of surveys exist covering ML studies on financial time series forecasting. Lately, Deep Learning (DL) models have appeared within the field, with results that significantly outperform their traditional ML counterparts. Even though there is a growing interest in developing models for financial time series forecasting, there is a lack of review papers that solely focus on DL for finance. Hence, the motivation of this paper is to provide a comprehensive literature review of DL studies on financial time series forecasting implementation. We not only categorized the studies according to their intended forecasting implementation areas, such as index, forex, and commodity forecasting, but we also grouped them based on their DL model choices, such as Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), and Long-Short Term Memory (LSTM). We also tried to envision the future of the field by highlighting its possible setbacks and opportunities for the benefit of interested researchers.}
}
@online{COlah15,
  author = {Olah, Christopher},
  title = {Understanding LSTM networks},
  year = 2015,
  publisher = {Colah's blog},
  url = {https://colah.github.io/posts/2015-08-Understanding-LSTMs/}
}
@article{SeppJur97,
    author = {Hochreiter, Sepp & Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}